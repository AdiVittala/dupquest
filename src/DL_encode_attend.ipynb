{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification of Quora Duplicates using Embed, Encode, Attend & Predict\n",
    "(based on methods described by Matthew Honnibal at https://explosion.ai/blog/deep-learning-formula-nlp#entailment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install keras\n",
    "#! pip install spacy\n",
    "#! python -m spacy download en_vectors_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import spacy, numpy and other utility libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/media/siri/78C6823EC681FD1E/minio/data/dq-data/dl/'\n",
    "input_folder = '/media/siri/78C6823EC681FD1E/minio/data/dq-data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load previously split data using train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pickle.load(open(input_folder+'X_train.p', 'rb'))\n",
    "X_test = pickle.load(open(input_folder+'X_test.p', 'rb'))\n",
    "y_train = pickle.load(open(input_folder+'y_train.p', 'rb'))\n",
    "y_test = pickle.load(open(input_folder+'y_test.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 270872 entries, 186150 to 121959\n",
      "Data columns (total 4 columns):\n",
      "qid1         270872 non-null int64\n",
      "qid2         270872 non-null int64\n",
      "question1    270872 non-null object\n",
      "question2    270872 non-null object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 10.3+ MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load spacy glove vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_vectors_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function to retrieve the word vectors \n",
    "Used to convert the train and test datasets (text to IDs) based on the glove vectors. \n",
    "Accounts for OOV tokens by adding a set of OOV vectors and assigning them randomly to OOV tokens. \n",
    "This function has been taken from https://github.com/explosion/spaCy/tree/master/examples/keras_parikh_entailment and adapted to this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(nlp, X, num_oov, max_length, norm_vectors = True):\n",
    "    len_q1 = X['question1'].size\n",
    "    sents = pd.concat([X['question1'], X['question2']]).values\n",
    "    \n",
    "    # the extra +1 is for a zero vector represting NULL for padding\n",
    "    num_vectors = max(lex.rank for lex in nlp.vocab) + 2 \n",
    "    \n",
    "    # create random vectors for OOV tokens\n",
    "    oov = np.random.normal(size=(num_oov, nlp.vocab.vectors_length))\n",
    "    oov = oov / oov.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    vectors = np.zeros((num_vectors + num_oov, nlp.vocab.vectors_length), dtype='float32')\n",
    "    vectors[num_vectors:, ] = oov\n",
    "    for lex in nlp.vocab:\n",
    "        if lex.has_vector and lex.vector_norm > 0:\n",
    "            vectors[lex.rank + 1] = lex.vector / lex.vector_norm if norm_vectors == True else lex.vector\n",
    "            \n",
    "    sents_as_ids = []\n",
    "    for sent in sents:\n",
    "        doc = nlp(sent)\n",
    "        word_ids = []\n",
    "        \n",
    "        for i, token in enumerate(doc):\n",
    "            # skip odd spaces from tokenizer\n",
    "            if token.has_vector and token.vector_norm == 0:\n",
    "                continue\n",
    "                \n",
    "            if i > max_length:\n",
    "                break\n",
    "                \n",
    "            if token.has_vector:\n",
    "                word_ids.append(token.rank + 1)\n",
    "            else:\n",
    "                # if we don't have a vector, pick an OOV entry\n",
    "                word_ids.append(token.rank % num_oov + num_vectors) \n",
    "                \n",
    "        # there must be a simpler way of generating padded arrays from lists...\n",
    "        word_id_vec = np.zeros((max_length), dtype='int')\n",
    "        clipped_len = min(max_length, len(word_ids))\n",
    "        word_id_vec[:clipped_len] = word_ids[:clipped_len]\n",
    "        sents_as_ids.append(word_id_vec)        \n",
    "        \n",
    "    return vectors, np.array(sents_as_ids[:len_q1]), np.array(sents_as_ids[len_q1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert train dataset (text to IDs) based on the glove vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v, q1_train_w2v, q2_train_w2v = create_dataset(nlp, X_train, 100, 50, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert test dataset (text to IDs) based on the glove vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, q1_test_w2v, q2_test_w2v = create_dataset(nlp, X_test, 100, 50, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_train_w2v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pickle the word vectors and train and test token IDs for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(w2v, open(data_folder+'w2v.p', 'wb'))\n",
    "pickle.dump(q1_train_w2v, open(data_folder+'q1_train_w2v.p', 'wb'))\n",
    "pickle.dump(q2_train_w2v, open(data_folder+'q2_train_w2v.p', 'wb'))\n",
    "pickle.dump(q1_test_w2v, open(data_folder+'q1_test_w2v.p', 'wb'))\n",
    "pickle.dump(q2_test_w2v, open(data_folder+'q2_test_w2v.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load previously pickled word vectors and train and test token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = pickle.load(open(data_folder+'w2v.p', 'rb'))\n",
    "q1_train_w2v = pickle.load(open(data_folder+'q1_train_w2v.p', 'rb'))\n",
    "q2_train_w2v = pickle.load(open(data_folder+'q2_train_w2v.p', 'rb'))\n",
    "q1_test_w2v = pickle.load(open(data_folder+'q1_test_w2v.p', 'rb'))\n",
    "q2_test_w2v = pickle.load(open(data_folder+'q2_test_w2v.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import keras and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, Model, models, initializers\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam,RMSprop, Nadam, SGD\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### configure tensorflow backend to dynamically grow memory (prevent OOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "                                    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 - Decomposable Attention Model\n",
    "(based on Example 1. A Decomposable Attention Model for Natural Language Inference from https://explosion.ai/blog/deep-learning-formula-nlp. Method 1 functions have been taken from https://github.com/explosion/spaCy/tree/master/examples/keras_parikh_entailment and adapted to this problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function to create embedding (word vector embeddings) for the NN\n",
    "(used by both models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(vectors, max_length, projected_dim):\n",
    "    return models.Sequential([\n",
    "        layers.Embedding(\n",
    "            vectors.shape[0],\n",
    "            vectors.shape[1],\n",
    "            input_length=max_length,\n",
    "            weights=[vectors],\n",
    "            trainable=False),\n",
    "        \n",
    "        layers.TimeDistributed(\n",
    "            layers.Dense(projected_dim,\n",
    "                         activation=None,\n",
    "                         use_bias=False))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function to create feed-forward layer for the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feedforward(num_units=200, activation='relu', dropout_rate=0.2):\n",
    "    return models.Sequential([\n",
    "        layers.Dense(num_units, activation=activation, use_bias=True),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(num_units, activation=activation, use_bias=True),\n",
    "        layers.Dropout(dropout_rate)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function to normalize attention weights\n",
    "(as described in 3.1 of https://arxiv.org/pdf/1606.01933v1.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(axis):\n",
    "    def _normalize(att_weights):\n",
    "        exp_weights = K.exp(att_weights)\n",
    "        sum_weights = K.sum(exp_weights, axis=axis, keepdims=True)\n",
    "        return exp_weights/sum_weights\n",
    "    return _normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function to sum the comparison vectors\n",
    "(as described in 3.3 of https://arxiv.org/pdf/1606.01933v1.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_word(x):\n",
    "    return K.sum(x, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function to build the NN\n",
    "(as described in 3.1 Attend, 3.2 Compare and 3.3 Aggregate of https://arxiv.org/pdf/1606.01933v1.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vectors, max_length, num_hidden, num_classes, projected_dim,  \n",
    "                dropout_rate0=0.2, dropout_rate1=0.2, dropout_rate2=0.2, dropout_rate3=0.2):\n",
    "    input1 = layers.Input(shape=(max_length,), dtype='int32', name='words1')\n",
    "    input2 = layers.Input(shape=(max_length,), dtype='int32', name='words2')\n",
    "    \n",
    "    # embeddings (projected)\n",
    "    embed = create_embedding(vectors, max_length, projected_dim)\n",
    "    a = embed(input1)\n",
    "    b = embed(input2)     \n",
    "    \n",
    "    # step 1: attend\n",
    "    F = create_feedforward(num_hidden, dropout_rate=dropout_rate1)\n",
    "    att_weights = layers.dot([F(a), F(b)], axes=-1, normalize=True)\n",
    "    \n",
    "    G = create_feedforward(num_hidden, dropout_rate=dropout_rate2)    \n",
    "    \n",
    "    norm_weights_a = layers.Lambda(normalizer(1))(att_weights)\n",
    "    norm_weights_b = layers.Lambda(normalizer(2))(att_weights)\n",
    "    alpha = layers.dot([norm_weights_a, a], axes=1)\n",
    "    beta  = layers.dot([norm_weights_b, b], axes=1)\n",
    "\n",
    "    # step 2: compare\n",
    "    comp1 = layers.concatenate([a, beta])\n",
    "    comp2 = layers.concatenate([b, alpha])\n",
    "    v1 = layers.TimeDistributed(G)(comp1)\n",
    "    v2 = layers.TimeDistributed(G)(comp2)\n",
    "\n",
    "    # step 3: aggregate\n",
    "    v1_sum = layers.Lambda(sum_word)(v1)\n",
    "    v2_sum = layers.Lambda(sum_word)(v2)\n",
    "    concat = layers.concatenate([v1_sum, v2_sum])\n",
    "        \n",
    "    H = create_feedforward(num_hidden, dropout_rate=dropout_rate3)\n",
    "    out = H(concat)\n",
    "    out = layers.Dense(num_classes, activation='sigmoid', use_bias=True)(out)\n",
    "    \n",
    "    model = Model([input1, input2], out)\n",
    "    \n",
    "    model.compile(optimizer=Nadam(lr=0.00001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build NN and display model  summary\n",
    "(TODO - dropout rates to be optimized further using hyperparameter optimization methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words1 (InputLayer)             (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "words2 (InputLayer)             (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 50, 200)      321381600   words1[0][0]                     \n",
      "                                                                 words2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 50, 200)      80400       sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 50, 50)       0           sequential_2[1][0]               \n",
      "                                                                 sequential_2[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 50, 50)       0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 50, 50)       0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 50, 200)      0           lambda_2[0][0]                   \n",
      "                                                                 sequential_1[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 50, 200)      0           lambda_1[0][0]                   \n",
      "                                                                 sequential_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 50, 400)      0           sequential_1[1][0]               \n",
      "                                                                 dot_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 50, 400)      0           sequential_1[2][0]               \n",
      "                                                                 dot_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 50, 200)      120400      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 50, 200)      120400      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 200)          0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 200)          0           time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 400)          0           lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 200)          120400      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            201         sequential_4[1][0]               \n",
      "==================================================================================================\n",
      "Total params: 321,703,001\n",
      "Trainable params: 381,401\n",
      "Non-trainable params: 321,321,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "m1 = build_model(w2v, 50, 200, 1, 200, dropout_rate1=0.2, dropout_rate2=0.2, dropout_rate3=0.2)\n",
    "m1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit NN model  with the train data\n",
    "(TODO - batch_size and epochs to be optimized further using hyperparameter optimization methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 270872 samples, validate on 133415 samples\n",
      "Epoch 1/50\n",
      "270872/270872 [==============================] - 39s 143us/step - loss: 0.6207 - acc: 0.6446 - val_loss: 0.5857 - val_acc: 0.6915\n",
      "Epoch 2/50\n",
      "270872/270872 [==============================] - 38s 140us/step - loss: 0.5773 - acc: 0.6947 - val_loss: 0.5668 - val_acc: 0.7039\n",
      "Epoch 3/50\n",
      "270872/270872 [==============================] - 38s 140us/step - loss: 0.5608 - acc: 0.7067 - val_loss: 0.5596 - val_acc: 0.7073\n",
      "Epoch 4/50\n",
      "270872/270872 [==============================] - 38s 140us/step - loss: 0.5513 - acc: 0.7148 - val_loss: 0.5518 - val_acc: 0.7111\n",
      "Epoch 5/50\n",
      "270872/270872 [==============================] - 38s 140us/step - loss: 0.5443 - acc: 0.7203 - val_loss: 0.5479 - val_acc: 0.7156\n",
      "Epoch 6/50\n",
      "270872/270872 [==============================] - 38s 140us/step - loss: 0.5382 - acc: 0.7250 - val_loss: 0.5335 - val_acc: 0.7266\n",
      "Epoch 7/50\n",
      "270872/270872 [==============================] - 38s 140us/step - loss: 0.5325 - acc: 0.7291 - val_loss: 0.5340 - val_acc: 0.7271\n",
      "Epoch 8/50\n",
      "270872/270872 [==============================] - 38s 140us/step - loss: 0.5274 - acc: 0.7317 - val_loss: 0.5293 - val_acc: 0.7298\n",
      "Epoch 9/50\n",
      "270872/270872 [==============================] - 38s 140us/step - loss: 0.5221 - acc: 0.7357 - val_loss: 0.5252 - val_acc: 0.7325\n",
      "Epoch 10/50\n",
      "270872/270872 [==============================] - 38s 140us/step - loss: 0.5174 - acc: 0.7390 - val_loss: 0.5140 - val_acc: 0.7412\n",
      "Epoch 11/50\n",
      "270872/270872 [==============================] - 38s 140us/step - loss: 0.5138 - acc: 0.7412 - val_loss: 0.5182 - val_acc: 0.7362\n",
      "Epoch 12/50\n",
      "270872/270872 [==============================] - 38s 140us/step - loss: 0.5088 - acc: 0.7447 - val_loss: 0.5147 - val_acc: 0.7379\n",
      "Epoch 13/50\n",
      "270872/270872 [==============================] - 38s 140us/step - loss: 0.5050 - acc: 0.7468 - val_loss: 0.5034 - val_acc: 0.7481\n",
      "Epoch 14/50\n",
      "270872/270872 [==============================] - 38s 140us/step - loss: 0.5009 - acc: 0.7493 - val_loss: 0.5311 - val_acc: 0.7265\n",
      "Epoch 15/50\n",
      "270872/270872 [==============================] - 38s 140us/step - loss: 0.4978 - acc: 0.7518 - val_loss: 0.5037 - val_acc: 0.7454\n",
      "Epoch 16/50\n",
      "270872/270872 [==============================] - 38s 140us/step - loss: 0.4939 - acc: 0.7533 - val_loss: 0.4959 - val_acc: 0.7518\n",
      "Epoch 17/50\n",
      "270872/270872 [==============================] - 38s 140us/step - loss: 0.4916 - acc: 0.7552 - val_loss: 0.4975 - val_acc: 0.7493\n",
      "Epoch 18/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4879 - acc: 0.7569 - val_loss: 0.4988 - val_acc: 0.7460\n",
      "Epoch 19/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4844 - acc: 0.7593 - val_loss: 0.4875 - val_acc: 0.7555\n",
      "Epoch 20/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4819 - acc: 0.7613 - val_loss: 0.4906 - val_acc: 0.7524\n",
      "Epoch 21/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4788 - acc: 0.7628 - val_loss: 0.4834 - val_acc: 0.7582\n",
      "Epoch 22/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4764 - acc: 0.7645 - val_loss: 0.4852 - val_acc: 0.7563\n",
      "Epoch 23/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4737 - acc: 0.7660 - val_loss: 0.4768 - val_acc: 0.7615\n",
      "Epoch 24/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4714 - acc: 0.7669 - val_loss: 0.4826 - val_acc: 0.7562\n",
      "Epoch 25/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4688 - acc: 0.7688 - val_loss: 0.4776 - val_acc: 0.7601\n",
      "Epoch 26/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4668 - acc: 0.7699 - val_loss: 0.4773 - val_acc: 0.7603\n",
      "Epoch 27/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4643 - acc: 0.7713 - val_loss: 0.4757 - val_acc: 0.7604\n",
      "Epoch 28/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4620 - acc: 0.7718 - val_loss: 0.4716 - val_acc: 0.7638\n",
      "Epoch 29/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4601 - acc: 0.7737 - val_loss: 0.4752 - val_acc: 0.7591\n",
      "Epoch 30/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4581 - acc: 0.7751 - val_loss: 0.4693 - val_acc: 0.7658\n",
      "Epoch 31/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4555 - acc: 0.7767 - val_loss: 0.4690 - val_acc: 0.7649\n",
      "Epoch 32/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4531 - acc: 0.7786 - val_loss: 0.4641 - val_acc: 0.7689\n",
      "Epoch 33/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4512 - acc: 0.7794 - val_loss: 0.4642 - val_acc: 0.7686\n",
      "Epoch 34/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4494 - acc: 0.7807 - val_loss: 0.4636 - val_acc: 0.7687\n",
      "Epoch 35/50\n",
      "270872/270872 [==============================] - 38s 140us/step - loss: 0.4479 - acc: 0.7811 - val_loss: 0.4641 - val_acc: 0.7686\n",
      "Epoch 36/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4454 - acc: 0.7830 - val_loss: 0.4583 - val_acc: 0.7726\n",
      "Epoch 37/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4438 - acc: 0.7837 - val_loss: 0.4659 - val_acc: 0.7645\n",
      "Epoch 38/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4422 - acc: 0.7846 - val_loss: 0.4570 - val_acc: 0.7728\n",
      "Epoch 39/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4406 - acc: 0.7863 - val_loss: 0.4528 - val_acc: 0.7776\n",
      "Epoch 40/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4381 - acc: 0.7875 - val_loss: 0.4502 - val_acc: 0.7770\n",
      "Epoch 41/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4371 - acc: 0.7882 - val_loss: 0.4558 - val_acc: 0.7738\n",
      "Epoch 42/50\n",
      "270872/270872 [==============================] - 38s 140us/step - loss: 0.4346 - acc: 0.7891 - val_loss: 0.4509 - val_acc: 0.7758\n",
      "Epoch 43/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4337 - acc: 0.7896 - val_loss: 0.4532 - val_acc: 0.7744\n",
      "Epoch 44/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4318 - acc: 0.7911 - val_loss: 0.4441 - val_acc: 0.7823\n",
      "Epoch 45/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4301 - acc: 0.7927 - val_loss: 0.4446 - val_acc: 0.7811\n",
      "Epoch 46/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4282 - acc: 0.7934 - val_loss: 0.4469 - val_acc: 0.7794\n",
      "Epoch 47/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4274 - acc: 0.7936 - val_loss: 0.4483 - val_acc: 0.7796\n",
      "Epoch 48/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4256 - acc: 0.7948 - val_loss: 0.4440 - val_acc: 0.7819\n",
      "Epoch 49/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4246 - acc: 0.7953 - val_loss: 0.4458 - val_acc: 0.7804\n",
      "Epoch 50/50\n",
      "270872/270872 [==============================] - 38s 141us/step - loss: 0.4225 - acc: 0.7976 - val_loss: 0.4449 - val_acc: 0.7805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2b57df89e8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.fit([q1_train_w2v, q2_train_w2v], y_train, batch_size=150, epochs=50,\n",
    "      validation_data=([q1_test_w2v, q2_test_w2v], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['not duplicate', 'duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_binary = lambda x: 1 if x[0] >= .5 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m.evaluate(test_sents,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dl1 = m1.predict([q1_test_w2v, q2_test_w2v], batch_size=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dl1_classes = np.array([convert_binary(y) for y in y_pred_dl1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for DL method 1  0.7804669639845595\n",
      "Recall score for DL method 1   0.6092414747293888\n",
      "Precision score for DL method 1   0.7480700527144177\n"
     ]
    }
   ],
   "source": [
    "score_dl1 = accuracy_score(y_test.values, y_pred_dl1_classes)\n",
    "rscore_dl1 = recall_score(y_test.values, y_pred_dl1_classes)\n",
    "pscore_dl1 = precision_score(y_test.values, y_pred_dl1_classes)\n",
    "print('Accuracy score for DL method 1 ', score_dl1)\n",
    "print('Recall score for DL method 1  ', rscore_dl1)\n",
    "print('Precision score for DL method 1  ', pscore_dl1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "not duplicate       0.79      0.88      0.84     84267\n",
      "    duplicate       0.75      0.61      0.67     49148\n",
      "\n",
      "    micro avg       0.78      0.78      0.78    133415\n",
      "    macro avg       0.77      0.74      0.75    133415\n",
      " weighted avg       0.78      0.78      0.77    133415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test.values, y_pred_dl1_classes, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 - Hierarchical Attention Networks\n",
    "(Code implemented based on algorithm described at Example 2: Hierarchical Attention Networks for Document Classification from https://explosion.ai/blog/deep-learning-formula-nlp). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### custom keras layer to implement the attention mechanism (with trainable weights) for the NN\n",
    "(implementation based on word and sentence attention layers described in https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf and keras custom layer example https://keras.io/layers/writing-your-own-keras-layers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer, RNN\n",
    "\n",
    "class Attention_Layer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim):\n",
    "        self.output_dim = output_dim\n",
    "        super(Attention_Layer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_weight(name='W', \n",
    "                                      shape=(input_shape[-1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.b = self.add_weight(name='b', \n",
    "                                      shape=(self.output_dim,),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.u = self.add_weight(name='u', \n",
    "                                      shape=(self.output_dim,1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(Attention_Layer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, h_it):        \n",
    "        u_it = K.tanh(K.bias_add(K.dot(h_it, self.W), self.b))\n",
    "        att_weights = K.dot(u_it, self.u)\n",
    "        exp_weights = K.exp(att_weights)\n",
    "        sum_weights = K.sum(exp_weights, axis=1, keepdims=True)\n",
    "        alpha_it = exp_weights/sum_weights\n",
    "        return K.sum(h_it*alpha_it, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function to build the hierarchical NN\n",
    "(implementation based on GRU-based word and sentence encoders and word and sentence attention layers described in https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hier_model(vectors=w2v, max_length=50, num_hidden=200, num_classes=1, \n",
    "                projected_dim=200, learn_rate=0.0001, dropout_rate1=0.2, dropout_rate2=0.2):\n",
    "    \n",
    "    K.clear_session()\n",
    "    # input    \n",
    "    model_input = layers.Input(shape=(2, max_length), dtype='int32')\n",
    "    \n",
    "    # embeddings (projected)\n",
    "    embed = create_embedding(vectors, max_length, projected_dim)\n",
    "    \n",
    "    # step 1: word encoder\n",
    "    word_sequence_input = layers.Input(shape=(max_length,), dtype='int32')\n",
    "    h_w = layers.Bidirectional(layers.GRU(num_hidden, dropout=dropout_rate1, return_sequences=True))(embed(word_sequence_input))\n",
    "    \n",
    "    # step 2: word attention\n",
    "    s_w = Attention_Layer(num_hidden, 1)(h_w)\n",
    "    word_encode_attend = Model(word_seq_input, s_w)\n",
    "    \n",
    "    # step 3: sentence encoder\n",
    "    sent_encode_attend = layers.TimeDistributed(word_encode_attend)(model_input)\n",
    "    h = layers.Bidirectional(layers.GRU(num_hidden, dropout=dropout_rate2, return_sequences=True))(sent_encode_attend)\n",
    "    \n",
    "    # step 4: sentence attention\n",
    "    v = Attention_Layer(num_hidden, 1)(h)\n",
    "    \n",
    "    # step 5: final classification\n",
    "    out = layers.Dense(num_classes, activation='sigmoid', use_bias=True)(v)\n",
    "    \n",
    "    model = Model(model_input, out)\n",
    "    \n",
    "    model.compile(optimizer=Nadam(lr=learn_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert the train and test into format suitable for input into the NN \n",
    "(add a new dimension and concatenate along this dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = np.concatenate([np.expand_dims(q1_train_w2v, axis=1),np.expand_dims(q2_train_w2v, axis=1)], axis=1)\n",
    "test_sents = np.concatenate([np.expand_dims(q1_test_w2v, axis=1),np.expand_dims(q2_test_w2v, axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization for the NN using GridSearchCV / RandomizedSearchCV \n",
    "(needs modification or alternate method like hyperas/hyperopt - as the current version takes a very long time to search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=build_hier_model, verbose=3)\n",
    "# define the grid search parameters\n",
    "learn_rate = np.logspace(-6, -1, 4)\n",
    "#momentum = np.linspace(0, 0.9, 4)\n",
    "#optimizer = ['RMSprop', 'Adam', 'Nadam']\n",
    "epochs = [10, 50, 100]\n",
    "batch_size= [50, 150]\n",
    "param_grid = dict(learn_rate=learn_rate, epochs=epochs, batch_size=batch_size)\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_jobs=1, cv=5)\n",
    "grid_result = grid.fit(train_sents, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build NN and display model  summary\n",
    "(TODO - dropout rates to be optimized further using hyperparameter optimization methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 2, 50)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 2, 400)            321943200 \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 2, 400)            721200    \n",
      "_________________________________________________________________\n",
      "attention__layer_2 (Attentio (None, 400)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 322,745,201\n",
      "Trainable params: 1,423,601\n",
      "Non-trainable params: 321,321,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "m2 = build_hier_model(w2v, 50, 200, 1, 200)\n",
    "#m = build_hier_model(learn_rate=grid_result.best_params_['learn_rate'],\n",
    "#                    momentum=grid_result.best_params_['momentum'])\n",
    "m2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit NN model  with the train data\n",
    "(TODO - batch_size and epochs to be optimized further using hyperparameter optimization methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 270872 samples, validate on 133415 samples\n",
      "Epoch 1/50\n",
      "270872/270872 [==============================] - 177s 654us/step - loss: 0.5783 - acc: 0.6886 - val_loss: 0.5547 - val_acc: 0.7088\n",
      "Epoch 2/50\n",
      "270872/270872 [==============================] - 175s 645us/step - loss: 0.5543 - acc: 0.7094 - val_loss: 0.5453 - val_acc: 0.7150\n",
      "Epoch 3/50\n",
      "270872/270872 [==============================] - 175s 645us/step - loss: 0.5457 - acc: 0.7143 - val_loss: 0.5409 - val_acc: 0.7174\n",
      "Epoch 4/50\n",
      "270872/270872 [==============================] - 175s 645us/step - loss: 0.5393 - acc: 0.7182 - val_loss: 0.5336 - val_acc: 0.7229\n",
      "Epoch 5/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.5312 - acc: 0.7246 - val_loss: 0.5228 - val_acc: 0.7297\n",
      "Epoch 6/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.5216 - acc: 0.7311 - val_loss: 0.5130 - val_acc: 0.7398\n",
      "Epoch 7/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.5115 - acc: 0.7394 - val_loss: 0.5025 - val_acc: 0.7471\n",
      "Epoch 8/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.5025 - acc: 0.7460 - val_loss: 0.4949 - val_acc: 0.7509\n",
      "Epoch 9/50\n",
      "270872/270872 [==============================] - 175s 647us/step - loss: 0.4946 - acc: 0.7508 - val_loss: 0.4906 - val_acc: 0.7549\n",
      "Epoch 10/50\n",
      "270872/270872 [==============================] - 170s 629us/step - loss: 0.4876 - acc: 0.7554 - val_loss: 0.4812 - val_acc: 0.7600\n",
      "Epoch 11/50\n",
      "270872/270872 [==============================] - 175s 647us/step - loss: 0.4804 - acc: 0.7598 - val_loss: 0.4749 - val_acc: 0.7641\n",
      "Epoch 12/50\n",
      "270872/270872 [==============================] - 175s 647us/step - loss: 0.4741 - acc: 0.7640 - val_loss: 0.4697 - val_acc: 0.7670\n",
      "Epoch 13/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.4687 - acc: 0.7683 - val_loss: 0.4679 - val_acc: 0.7689\n",
      "Epoch 14/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.4625 - acc: 0.7717 - val_loss: 0.4604 - val_acc: 0.7745\n",
      "Epoch 15/50\n",
      "270872/270872 [==============================] - 175s 647us/step - loss: 0.4573 - acc: 0.7751 - val_loss: 0.4565 - val_acc: 0.7754\n",
      "Epoch 16/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.4525 - acc: 0.7780 - val_loss: 0.4532 - val_acc: 0.7790\n",
      "Epoch 17/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.4470 - acc: 0.7822 - val_loss: 0.4467 - val_acc: 0.7836\n",
      "Epoch 18/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.4422 - acc: 0.7843 - val_loss: 0.4439 - val_acc: 0.7853\n",
      "Epoch 19/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.4379 - acc: 0.7869 - val_loss: 0.4371 - val_acc: 0.7901\n",
      "Epoch 20/50\n",
      "270872/270872 [==============================] - 175s 647us/step - loss: 0.4328 - acc: 0.7907 - val_loss: 0.4351 - val_acc: 0.7911\n",
      "Epoch 21/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.4292 - acc: 0.7934 - val_loss: 0.4321 - val_acc: 0.7935\n",
      "Epoch 22/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.4249 - acc: 0.7960 - val_loss: 0.4286 - val_acc: 0.7956\n",
      "Epoch 23/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.4204 - acc: 0.7988 - val_loss: 0.4245 - val_acc: 0.7980\n",
      "Epoch 24/50\n",
      "270872/270872 [==============================] - 175s 647us/step - loss: 0.4166 - acc: 0.8012 - val_loss: 0.4227 - val_acc: 0.7997\n",
      "Epoch 25/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.4134 - acc: 0.8033 - val_loss: 0.4204 - val_acc: 0.8003\n",
      "Epoch 26/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.4098 - acc: 0.8052 - val_loss: 0.4165 - val_acc: 0.8028\n",
      "Epoch 27/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.4067 - acc: 0.8075 - val_loss: 0.4155 - val_acc: 0.8034\n",
      "Epoch 28/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.4020 - acc: 0.8096 - val_loss: 0.4135 - val_acc: 0.8042\n",
      "Epoch 29/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.3988 - acc: 0.8121 - val_loss: 0.4101 - val_acc: 0.8068\n",
      "Epoch 30/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.3951 - acc: 0.8141 - val_loss: 0.4107 - val_acc: 0.8061\n",
      "Epoch 31/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.3920 - acc: 0.8152 - val_loss: 0.4124 - val_acc: 0.8054\n",
      "Epoch 32/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.3884 - acc: 0.8182 - val_loss: 0.4039 - val_acc: 0.8109\n",
      "Epoch 33/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.3846 - acc: 0.8203 - val_loss: 0.4017 - val_acc: 0.8130\n",
      "Epoch 34/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.3818 - acc: 0.8210 - val_loss: 0.3998 - val_acc: 0.8132\n",
      "Epoch 35/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.3776 - acc: 0.8234 - val_loss: 0.4011 - val_acc: 0.8123\n",
      "Epoch 36/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.3745 - acc: 0.8260 - val_loss: 0.3963 - val_acc: 0.8152\n",
      "Epoch 37/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.3713 - acc: 0.8275 - val_loss: 0.3986 - val_acc: 0.8135\n",
      "Epoch 38/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.3680 - acc: 0.8295 - val_loss: 0.3950 - val_acc: 0.8160\n",
      "Epoch 39/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.3644 - acc: 0.8316 - val_loss: 0.3926 - val_acc: 0.8178\n",
      "Epoch 40/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.3612 - acc: 0.8329 - val_loss: 0.3917 - val_acc: 0.8180\n",
      "Epoch 41/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.3576 - acc: 0.8352 - val_loss: 0.3885 - val_acc: 0.8205\n",
      "Epoch 42/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.3540 - acc: 0.8366 - val_loss: 0.3973 - val_acc: 0.8136\n",
      "Epoch 43/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.3511 - acc: 0.8391 - val_loss: 0.3874 - val_acc: 0.8220\n",
      "Epoch 44/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.3476 - acc: 0.8412 - val_loss: 0.3927 - val_acc: 0.8188\n",
      "Epoch 45/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.3444 - acc: 0.8422 - val_loss: 0.3863 - val_acc: 0.8230\n",
      "Epoch 46/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.3412 - acc: 0.8449 - val_loss: 0.3869 - val_acc: 0.8226\n",
      "Epoch 47/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.3374 - acc: 0.8461 - val_loss: 0.3838 - val_acc: 0.8226\n",
      "Epoch 48/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.3339 - acc: 0.8480 - val_loss: 0.3806 - val_acc: 0.8260\n",
      "Epoch 49/50\n",
      "270872/270872 [==============================] - 175s 646us/step - loss: 0.3310 - acc: 0.8503 - val_loss: 0.3837 - val_acc: 0.8251\n",
      "Epoch 50/50\n",
      "270872/270872 [==============================] - 175s 647us/step - loss: 0.3276 - acc: 0.8523 - val_loss: 0.3835 - val_acc: 0.8237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0c8da70400>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.fit(train_sents, y_train, batch_size=150, epochs=50,\n",
    "      validation_data=(test_sents, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dl2 = m.predict(test_sents, batch_size=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dl2_classes = np.array([convert_binary(y) for y in y_pred_dl2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for DL method 2  0.8237379604991942\n",
      "Recall score for DL method 2  0.803776348986734\n",
      "Precision score for DL method 2  0.7401079136690647\n"
     ]
    }
   ],
   "source": [
    "score_dl2 = accuracy_score(y_test.values, y_pred_dl2_classes)\n",
    "rscore_dl2 = recall_score(y_test.values, y_pred_dl2_classes)\n",
    "pscore_dl2 = precision_score(y_test.values, y_pred_dl2_classes)\n",
    "print('Accuracy score for DL method 2 ', score_dl2)\n",
    "print('Recall score for DL method 2 ', rscore_dl2)\n",
    "print('Precision score for DL method 2 ', pscore_dl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "not duplicate       0.88      0.84      0.86     84267\n",
      "    duplicate       0.74      0.80      0.77     49148\n",
      "\n",
      "    micro avg       0.82      0.82      0.82    133415\n",
      "    macro avg       0.81      0.82      0.81    133415\n",
      " weighted avg       0.83      0.82      0.83    133415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test.values, y_pred_dl2_classes, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
