{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import persistence as ps\n",
    "from urllib3.response import HTTPResponse\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import partition_all\n",
    "#from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_BUCKET: str = 'dq-data'\n",
    "HASH_BUCKET: str = 'dq-hashed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train_set\n",
    "data: str = 'train.csv'\n",
    "filestream: HTTPResponse = ps.get_file_stream(bucket=INPUT_BUCKET, filename=data)\n",
    "#s3_in_prefix: str = 's3://' + INPUT_BUCKET + '/'\n",
    "#s3_in_url: str = s3_in_prefix + train_data\n",
    "#s3_options: Dict = ps.fetch_s3_options()\n",
    "dtypes: Dict[str, str] = {\n",
    "    'id': 'int64',\n",
    "    'qid1': 'int64',\n",
    "    'qid2': 'int64',\n",
    "    'question1': 'object',\n",
    "    'question2': 'object',\n",
    "    'is_duplicate': 'int64'\n",
    "}\n",
    "df: pd.DataFrame = pd.read_csv(#urlpath=s3_in_url, \n",
    "                                     #storage_options=s3_options,\n",
    "                                     filestream,\n",
    "                                     header=0, \n",
    "                                     usecols=dtypes.keys(), \n",
    "                                     names=dtypes.keys(),\n",
    "                                     skipinitialspace=True,\n",
    "                                     skip_blank_lines=True,\n",
    "                                     encoding='utf-8')\n",
    "df = df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    qid1  qid2                                          question1  \\\n",
       "id                                                                  \n",
       "0      1     2  What is the step by step guide to invest in sh...   \n",
       "1      3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2      5     6  How can I increase the speed of my internet co...   \n",
       "3      7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4      9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                            question2  is_duplicate  \n",
       "id                                                                   \n",
       "0   What is the step by step guide to invest in sh...             0  \n",
       "1   What would happen if the Indian government sto...             0  \n",
       "2   How can Internet speed be increased by hacking...             0  \n",
       "3   Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4             Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 404290 entries, 0 to 404289\n",
      "Data columns (total 5 columns):\n",
      "qid1            404290 non-null int64\n",
      "qid2            404290 non-null int64\n",
      "question1       404289 non-null object\n",
      "question2       404288 non-null object\n",
      "is_duplicate    404290 non-null int64\n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 18.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop(columns=['is_duplicate'])\n",
    "y = df['is_duplicate']\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 270874 entries, 316451 to 121958\n",
      "Data columns (total 4 columns):\n",
      "qid1         270874 non-null int64\n",
      "qid2         270874 non-null int64\n",
      "question1    270873 non-null object\n",
      "question2    270873 non-null object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 10.3+ MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from spacy.lemmatizer import Lemmatizer\n",
    "#from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "#lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "#nlp.add_pipe(lemmatizer)\n",
    "#nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tagger = nlp.get_pipe('tagger')\n",
    "#tagger.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser = nlp.get_pipe('parser')\n",
    "#parser.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ner = nlp.get_pipe('ner')\n",
    "#ner.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize, pos-tag, parse dependencies, recognize entities (pipeline)\n",
    "#pipeline = ['tagger', 'parser', 'ner']\n",
    "#for name in pipeline:\n",
    "#    component = nlp.create_pipe(name)   # 3. create the pipeline components\n",
    "#    nlp.add_pipe(component)             # 4. add the component to the pipeline\n",
    "\n",
    "#preprocess_q1 = lambda row: nlp(row['question1'])\n",
    "#x_df1['pr_question1'] = x_df1.apply(preprocess_q1, axis=1)\n",
    "#preprocess_q2 = lambda row: nlp(row['question2'])\n",
    "#x_df1['pr_question2'] = x_df1.apply(preprocess_q2, axis=1)\n",
    "#x_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "import nltk\n",
    "def tokenize(text):\n",
    "        tokens = [word for word in nlp(text) if len(word) > 1] #if len(word) > 1 because I only want to retain words that are at least two characters before stemming, although I can't think of any such words that are not also stopwords\n",
    "        #stems = [stemmer.stem(item) for item in tokens]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "ps.create_bucket(bucket=HASH_BUCKET)\n",
    "import os\n",
    "import shutil\n",
    "tmp_train_path = '/tmp/train'\n",
    "try:\n",
    "    shutil.rmtree(tmp_train_path)\n",
    "except:\n",
    "    pass\n",
    "try:    \n",
    "    os.mkdir(tmp_train_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "def tokenize(pipeline, series, batch_id, output_dir):\n",
    "    #tfidf = TfidfVectorizer(tokenizer=tokenizer, binary=True, stop_words='english', use_idf=True, max_features=max_features)\n",
    "    #series1 = df[col1]\n",
    "    #series2 = df[col2]\n",
    "    #series = pd.concat([series1, series2])\n",
    "    #start = time.time()\n",
    "    print('processing batch {}'.format(batch_id))\n",
    "    #trnsfmd = pipeline(series)\n",
    "    #end =  time.time()\n",
    "    # save transformed batch\n",
    "    out_file = ('%d' % batch_id)\n",
    "    out_path = output_dir+'/'+out_file \n",
    "    with open(out_path, 'wb') as handle:\n",
    "        for doc in pipeline.pipe(series):\n",
    "            #pickle.dump(doc, handle)\n",
    "            handle.write(' '.join(w for w in doc if not w.is_space))\n",
    "            handle.write('\\n')\n",
    "            \n",
    "    ps.copy_file(dest_bucket=HASH_BUCKET, file='train/'+out_file, source=out_path)\n",
    "    os.remove(out_path)\n",
    "    #print('created TF-IDF vectors in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all files in bucket dq-hashed at path train/ are ['0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '5', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '6', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '7', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '8', '80', '9']\n",
      "processing batch 0\n",
      "started process Process-166\n",
      "started process 1\n",
      "processing batch 1\n",
      "processing batch 2\n",
      "started process 2\n",
      "processing batch 3\n",
      "started process 3\n",
      "processing batch 4\n",
      "started process 4\n",
      "processing batch 5\n",
      "started process 5\n",
      "started process 6\n",
      "processing batch 6\n",
      "started process 7\n",
      "processing batch 7\n",
      "started process 8\n",
      "processing batch 8\n",
      "started process 9\n",
      "processing batch 9\n",
      "processing batch 10\n",
      "started process 10\n",
      "processing batch 11\n",
      "started process 11\n",
      "processing batch 12\n",
      "started process 12\n",
      "processing batch 13\n",
      "started process 13\n",
      "started process 14\n",
      "processing batch 14\n",
      "started process 15processing batch 15\n",
      "\n",
      "processing batch 16\n",
      "started process 16\n",
      "started process 17\n",
      "processing batch 17\n",
      "processing batch 18\n",
      "started process 18\n",
      "processing batch 19\n",
      "started process 19\n",
      "processing batch 20\n",
      "started process 20\n",
      "processing batch 21\n",
      "started process 21\n",
      "processing batch 22\n",
      "started process 22\n",
      "processing batch 23\n",
      "started process 23\n",
      "processing batch 24\n",
      "started process 24\n",
      "processing batch 25\n",
      "started process 25\n",
      "processing batch 26\n",
      "started process 26\n",
      "processing batch 27\n",
      "started process 27\n",
      "processing batch 28\n",
      "started process 28\n",
      "processing batch 29\n",
      "started process 29\n",
      "processing batch 30\n",
      "started process 30\n",
      "processing batch 31\n",
      "started process 31\n",
      "processing batch 32\n",
      "started process 32\n",
      "processing batch 33\n",
      "started process 33\n",
      "processing batch 34\n",
      "started process 34\n",
      "processing batch 35\n",
      "started process 35\n",
      "processing batch 36\n",
      "started process 36\n",
      "processing batch 37\n",
      "started process 37\n",
      "processing batch 38\n",
      "started process 38\n",
      "processing batch 39\n",
      "started process 39\n",
      "processing batch 40\n",
      "started process 40\n",
      "processing batch 41\n",
      "started process 41\n",
      "processing batch 42\n",
      "started process 42\n",
      "processing batch 43\n",
      "started process 43\n",
      "processing batch 44\n",
      "started process 44\n",
      "processing batch 45\n",
      "started process 45\n",
      "processing batch 46\n",
      "started process 46\n",
      "processing batch 47\n",
      "started process 47\n",
      "processing batch 48\n",
      "started process 48\n",
      "processing batch 49\n",
      "started process 49\n",
      "processing batch 50\n",
      "started process 50\n",
      "processing batch 51\n",
      "started process 51\n",
      "processing batch 52\n",
      "started process 52\n",
      "processing batch 53\n",
      "started process 53\n",
      "started process 54\n",
      "processing batch 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-166:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "Process 2:\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished process Process-166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process 12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Process 1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "Process 3:\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Process 5:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished process 1\n",
      "finished process 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished process 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Process 4:\n",
      "Process 9:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Process 6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished process 4\n",
      "finished process 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process 8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished process 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "Process 7:\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Process 11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished process 7\n",
      "finished process 8\n",
      "finished process 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process 14:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Process 10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "Process 17:\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished process 10\n",
      "finished process 11\n",
      "finished process 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process 19:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Process 16:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Process 13:\n",
      "Process 18:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Process 15:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished process 13\n",
      "finished process 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process 21:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished process 15\n",
      "finished process 16\n",
      "finished process 17\n",
      "finished process 18\n",
      "finished process 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Process 20:\n",
      "Process 27:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process 22:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "Traceback (most recent call last):\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Process 24:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished process 20\n",
      "finished process 21\n",
      "finished process 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process 23:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished process 23\n",
      "finished process 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process 26:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Process 31:\n",
      "Process 25:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "Process 29:\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Process 32:\n",
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished process 25\n",
      "finished process 26\n",
      "finished process 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Process 28:\n",
      "Process 34:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "Process 36:\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "Process 39:\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished process 28\n",
      "finished process 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process 30:\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Process 35:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process 33:\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished process 30\n",
      "finished process 31\n",
      "finished process 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished process 33\n",
      "finished process 34\n",
      "finished process 35\n",
      "finished process 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process 41:\n",
      "Traceback (most recent call last):\n",
      "Process 40:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Process 37:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Process 42:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished process 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process 45:\n",
      "Traceback (most recent call last):\n",
      "Process 38:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished process 38\n",
      "finished process 39\n",
      "finished process 40\n",
      "finished process 41\n",
      "finished process 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process 49:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "Process 44:\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Process 47:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Process 54:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Process 46:\n",
      "Traceback (most recent call last):\n",
      "Process 53:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process 51:\n",
      "Process 43:\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process 52:\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "Process 48:\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "Traceback (most recent call last):\n",
      "Process 50:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-35-c79d5582a068>\", line 30, in tokenize\n",
      "    handle.write(' '.join(w for w in doc if not w.is_space))\n",
      "TypeError: sequence item 0: expected str instance, spacy.tokens.token.Token found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished process 43\n",
      "finished process 44\n",
      "finished process 45\n",
      "finished process 46\n",
      "finished process 47\n",
      "finished process 48\n",
      "finished process 49\n",
      "finished process 50\n",
      "finished process 51\n",
      "finished process 52\n",
      "finished process 53\n",
      "finished process 54\n"
     ]
    }
   ],
   "source": [
    "from toolz import partition_all\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Process, cpu_count\n",
    "# empty HASH_BUCKET\n",
    "ps.remove_all_files(bucket=HASH_BUCKET, path='train/')\n",
    "series = pd.Series(pd.concat([X_train['question1'], X_train['question2']]),dtype=str)\n",
    "series.dropna()\n",
    "partitions = partition_all(10000, series.tolist())\n",
    "#hashvect = HashingVectorizer(tokenizer=nlp, binary=True, stop_words='english')\n",
    "#Parallel(n_jobs=8)(delayed(tokenize)(nlp, batch, i, tmp_train_path)\n",
    "#         for i, batch in enumerate(partitions))\n",
    "#executor(tasks)\n",
    "\n",
    "# parallel joblib and spacy dont work together, trying serial\n",
    "#for i, batch in enumerate(partitions):\n",
    "#    tokenize(nlp, batch, i, tmp_train_path)\n",
    "\n",
    "#trying multiprocessing\n",
    "processes = []\n",
    "for i, batch in enumerate(partitions):\n",
    "    p = Process(target=tokenize, name=i, kwargs={'pipeline':nlp, \n",
    "                                                 'series':batch, \n",
    "                                                 'batch_id':i, \n",
    "                                                 'output_dir':tmp_train_path})\n",
    "    p.start()\n",
    "    print('started process {}'.format(p.name))\n",
    "    processes.append(p)\n",
    "for p in processes:\n",
    "    p.join()\n",
    "    print('finished process {}'.format(p.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import vstack\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "data = []\n",
    "#files = ps.get_all_filenames(bucket=HASH_BUCKET, path='train/')\n",
    "files = os.listdir(tmp_train_path)\n",
    "for file in files:\n",
    "    #ps.get_file(bucket=HASH_BUCKET, filename='train/'+file, filepath=tmp_train_path+file)\n",
    "    with open(tmp_train_path+file, 'rb') as handle:\n",
    "        data.append(pickle.load(handle))\n",
    "\n",
    "#tfidf = TfidfTransformer()\n",
    "pass_through = lambda x:x\n",
    "tfidf = TfidfVectorizer(analyzer=pass_through, binary=True)\n",
    "X_trfmd = tfidf.fit_transform(vstack(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vstack(data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trfmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension reduction using SVD\n",
    "start = time.time()\n",
    "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42)\n",
    "X_svd = svd.fit_transform(X_trfmd)\n",
    "end =  time.time()\n",
    "print('created SVD transform in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_svd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split back into two\n",
    "X1 = X_svd[:len(X_train), :]\n",
    "X2 = X_svd[len(X_train):, :]\n",
    "## find pair-wise cosine similarity\n",
    "#start = time.time()\n",
    "#X_sim = cosine_similarity(X1, X2)\n",
    "#end =  time.time()\n",
    "#print('computed cosine similarity in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svd_feature_length = X_sim.shape[1]\n",
    "#start = time.time()\n",
    "#temp_df = pd.DataFrame(X_sim)\n",
    "#x_df1 = pd.concat([x_df1,temp_df], axis=1)\n",
    "#end =  time.time()\n",
    "#print('rebuilt dataframe with new tf_svd feature columns in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_df1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_df1 = tfidf_svd_vectorize(x_df1, 'question1', 'question2', 10000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference in text size\n",
    "compute_size_diff = lambda row: abs(len(str(row['question1'])) - len(str(row['question2'])))\n",
    "X_train['size_diff'] = X_train.apply(compute_size_diff, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vector norm diff (distance)\n",
    "#compute_spacy_distance = lambda row: abs(row['question1'].vector_norm - row['question2'].vector_norm)\n",
    "#x_df1['spacy_distance'] = x_df1.apply(compute_spacy_distance, axis=1)\n",
    "#x_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return mean distance between tokens and document centroid\n",
    "def compute_mean_distance(doc):\n",
    "    mean_distance = 0.0\n",
    "    centroid = doc.vector\n",
    "    count = 0\n",
    "    for token in doc:\n",
    "        if not token.is_stop:\n",
    "            mean_distance += np.inner(token.vector,centroid)\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        count = 1\n",
    "    return mean_distance / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mean distance from centroid for question1\n",
    "#compute_q1_mean_dist = lambda row: compute_mean_distance(row['question1'])\n",
    "#x_df1['q1_mean_dist'] = x_df1.apply(compute_q1_mean_dist, axis=1)\n",
    "#x_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mean distance from centroid for question1\n",
    "#compute_q2_mean_dist = lambda row: compute_mean_distance(row['question2'])\n",
    "#x_df1['q2_mean_dist'] = x_df1.apply(compute_q2_mean_dist, axis=1)\n",
    "#x_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## difference in mean distance\n",
    "#compute_mean_dist_diff = lambda row: abs(row['q1_mean_dist'] - row['q2_mean_dist'])\n",
    "#x_df1['mean_dist_diff'] = x_df1.apply(compute_mean_dist_diff, axis=1)\n",
    "#x_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## centroid similarity\n",
    "#compute_centroid_similarity = lambda row: np.inner(row['question1'].vector, row['question2'].vector)\n",
    "#x_df1['centroid_similarity'] = x_df1.apply(compute_centroid_similarity, axis=1)\n",
    "#x_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio\n",
    "compute_ratio = lambda row: fuzz.ratio(str(row['question1']), str(row['question2']))\n",
    "X_train['ratio'] = X_train.apply(compute_ratio, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial ratio\n",
    "compute_partial_ratio = lambda row: fuzz.partial_ratio(str(row['question1']), str(row['question2']))\n",
    "X_train['partial_ratio'] = X_train.apply(compute_partial_ratio, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_sort_ratio\n",
    "compute_token_sort_ratio = lambda row: fuzz.token_sort_ratio(str(row['question1']), str(row['question2']))\n",
    "X_train['token_sort_ratio'] = X_train.apply(compute_token_sort_ratio, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_set_ratio\n",
    "compute_token_set_ratio = lambda row: fuzz.token_set_ratio(str(row['question1']), str(row['question2']))\n",
    "X_train['token_set_ratio'] = X_train.apply(compute_token_set_ratio, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build complete feature dataframe\n",
    "X_train_temp = pd.concat([pd.DataFrame(X1, columns=['q1_'+str(i) for i in range(X1.shape[1])], index=X_train.index), \n",
    "                     pd.DataFrame(X2, columns=['q2_'+str(i) for i in range(X2.shape[1])], index=X_train.index)], axis=1)\n",
    "X_train_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train_temp, X_train], axis=1)\n",
    "del X_train_temp\n",
    "X_train = X_train.drop(columns=['qid1', 'qid2','question1','question2'])\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load test_set\n",
    "#test_data: str = 'test.csv'\n",
    "#filestream: HTTPResponse = ps.get_file_stream(bucket=INPUT_BUCKET, filename=test_data)\n",
    "#dtypes: Dict[str, str] = {\n",
    "#    'id': 'int64',\n",
    "#    'question1': 'object',\n",
    "#    'question2': 'object'\n",
    "#}\n",
    "#test_df: pd.DataFrame = pd.read_csv(#urlpath=s3_in_url, \n",
    "#                                     #storage_options=s3_options,\n",
    "#                                     filestream,\n",
    "#                                     header=0, \n",
    "#                                     usecols=dtypes.keys(), \n",
    "#                                     names=dtypes.keys(),\n",
    "#                                     skipinitialspace=True,\n",
    "#                                     skip_blank_lines=True,\n",
    "#                                     encoding='utf-8')\n",
    "#test_df = test_df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ps.create_bucket(bucket=HASH_BUCKET)\n",
    "tmp_test_path = '/tmp/test/'\n",
    "if os.path.isdir(tmp_test_path):\n",
    "    shutil.rmtree(tmp_test_path)\n",
    "os.mkdir(tmp_test_path)\n",
    "def transform(transformer, series, batch_id, output_dir, max_features=10000):\n",
    "    #tfidf = TfidfVectorizer(tokenizer=tokenizer, binary=True, stop_words='english', use_idf=True, max_features=max_features)\n",
    "    #series1 = df[col1]\n",
    "    #series2 = df[col2]\n",
    "    #series = pd.concat([series1, series2])\n",
    "    #start = time.time()\n",
    "    print('processing batch {}'.format(batch_id))\n",
    "    X = transformer.transform(series)\n",
    "    #end =  time.time()\n",
    "    # save transformed batch\n",
    "    out_file = ('%d' % batch_id)\n",
    "    out_path = output_dir+'/'+out_file \n",
    "    with open(out_path, 'wb') as handle:\n",
    "        pickle.dump(X, handle)\n",
    "    #ps.copy_file(dest_bucket=HASH_BUCKET, file='test/'+out_file, source=out_path)\n",
    "    #print('created TF-IDF vectors in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty HASH_BUCKET\n",
    "#ps.remove_all_files(bucket=HASH_BUCKET, path='test/')\n",
    "series = pd.Series(pd.concat([X_test['question1'], X_test['question2']]),dtype=str)\n",
    "partitions = partition_all(10000, series.tolist())\n",
    "Parallel(n_jobs=8)(delayed(transform)(hashvect, batch, i, tmp_test_path, 10000)\n",
    "         for i, batch in enumerate(partitions))\n",
    "#executor(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "files = ps.get_all_filenames(bucket=HASH_BUCKET, path='test/')\n",
    "files = os.listdir(tmp_test_path)\n",
    "for file in files:\n",
    "    #ps.get_file(bucket=HASH_BUCKET, filename='test/'+file, filepath=tmp_test_path+file)\n",
    "    with open(tmp_test_path+file, 'rb') as handle:\n",
    "        data.append(pickle.load(handle))\n",
    "X_test_trfmd = tfidf.transform(vstack(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_trfmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension reduction using SVD\n",
    "start = time.time()\n",
    "X_test_svd = svd.transform(X_test_trfmd)\n",
    "end =  time.time()\n",
    "print('created SVD transform in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split back into two\n",
    "X1_test = X_test_svd[:len(X_test), :]\n",
    "X2_test = X_test_svd[len(X_test):, :]\n",
    "## find pair-wise cosine similarity\n",
    "#start = time.time()\n",
    "#X_sim = cosine_similarity(X1, X2)\n",
    "#end =  time.time()\n",
    "#print('computed cosine similarity in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference in text size\n",
    "X_test['size_diff'] = X_test.apply(compute_size_diff, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio\n",
    "X_test['ratio'] = X_test.apply(compute_ratio, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial ratio\n",
    "X_test['partial_ratio'] = X_test.apply(compute_partial_ratio, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_sort_ratio\n",
    "X_test['token_sort_ratio'] = X_test.apply(compute_token_sort_ratio, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_set_ratio\n",
    "X_test['token_set_ratio'] = X_test.apply(compute_token_set_ratio, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build complete feature dataframe\n",
    "X_test_temp = pd.concat([pd.DataFrame(X1_test, columns=['q1_'+str(i) for i in range(X1_test.shape[1])], index=X_test.index), \n",
    "                    pd.DataFrame(X2_test, columns=['q2_'+str(i) for i in range(X2_test.shape[1])], index=X_test.index)], axis=1)\n",
    "X_test_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.concat([X_test_temp, X_test], axis=1)\n",
    "del X_test_temp\n",
    "X_test = X_test.drop(columns=['question1','question2', 'qid1', 'qid2'])\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "logr_model = LogisticRegression(random_state=42)\n",
    "param_grid = {'C': np.logspace(-2, 7, 10),\n",
    "             #'penalty': ['l1','l2'],\n",
    "             'tol': np.logspace(-5, -1, 5),\n",
    "             'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "             #'max_iter': np.linspace(10, 1000, 10)\n",
    "             }\n",
    "logr_cv = RandomizedSearchCV(logr_model, param_distributions=param_grid, cv=5, n_jobs=-1)\n",
    "#y_train = X['is_duplicate']\n",
    "#X_train = X.drop(columns=['is_duplicate'])\n",
    "logr_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_model = LogisticRegression(solver=logr_cv.best_params_['solver'], \n",
    "                                random_state=42, \n",
    "                                C=logr_cv.best_params_['C'], \n",
    "                                tol=logr_cv.best_params_['tol'], \n",
    "                                #max_iter=logr_cv.best_params_['max_iter'], \n",
    "                                n_jobs=-1)\n",
    "logr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_pred = logr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "logr_acc_score = accuracy_score(y_test, logr_pred)\n",
    "logr_prec_score = precision_score(y_test, logr_pred)\n",
    "logr_rec_score = recall_score(y_test, logr_pred)\n",
    "print('Logistic Regression')\n",
    "print('accuracy score : {}'.format(logr_acc_score))\n",
    "print('precision score : {}'.format(logr_prec_score))\n",
    "print('recall score : {}'.format(logr_rec_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
