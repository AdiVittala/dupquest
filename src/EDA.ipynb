{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import persistence as ps\n",
    "from urllib3.response import HTTPResponse\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import partition_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_BUCKET = 'dq-data'\n",
    "HASH_BUCKET = 'dq-hashed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train_set\n",
    "data = 'train.csv'\n",
    "filestream = ps.get_file_stream(bucket=INPUT_BUCKET, filename=data)\n",
    "dtypes = {\n",
    "    'id': 'int64',\n",
    "    'qid1': 'int64',\n",
    "    'qid2': 'int64',\n",
    "    'question1': 'object',\n",
    "    'question2': 'object',\n",
    "    'is_duplicate': 'int64'\n",
    "}\n",
    "df = pd.read_csv(#urlpath=s3_in_url, \n",
    "                                     #storage_options=s3_options,\n",
    "                                     filestream,\n",
    "                                     header=0, \n",
    "                                     usecols=dtypes.keys(), \n",
    "                                     #names=dtypes.keys(),\n",
    "                                     skipinitialspace=True,\n",
    "                                     skip_blank_lines=True,\n",
    "                                     encoding='utf-8')\n",
    "df = df.set_index('id')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 404287 entries, 0 to 404289\n",
      "Data columns (total 5 columns):\n",
      "qid1            404287 non-null int64\n",
      "qid2            404287 non-null int64\n",
      "question1       404287 non-null object\n",
      "question2       404287 non-null object\n",
      "is_duplicate    404287 non-null int64\n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 18.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#shrink df to 150,000 records\n",
    "df = df.iloc[:75000]\n",
    "\n",
    "X = df.drop(columns=['is_duplicate'])\n",
    "\n",
    "y = df['is_duplicate']\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 50250 entries, 71916 to 15795\n",
      "Data columns (total 4 columns):\n",
      "qid1         50250 non-null int64\n",
      "qid2         50250 non-null int64\n",
      "question1    50250 non-null object\n",
      "question2    50250 non-null object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del X,y,df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "def get_tokens(process='train'):\n",
    "    if process=='test':\n",
    "        X = X_test\n",
    "    else:\n",
    "        X = X_train\n",
    "    series = pd.Series(pd.concat([X['question1'], X['question2']]),dtype=str)\n",
    "    series.dropna()\n",
    "    for question in series:\n",
    "        yield preprocess_string(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec (fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minio.definitions.Object at 0x7fc99b639978>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.get_file(bucket=INPUT_BUCKET, filename='cc.en.300.bin.gz', filepath='/tmp/cc.en.300.bin.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "with gzip.open('/tmp/cc.en.300.bin.gz', 'rb') as f_in:\n",
    "    with open('/tmp/cc.en.300.bin', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove('/tmp/cc.en.300.bin.gz')\n",
    "from gensim.models import FastText\n",
    "model = FastText.load_fasttext_format('/tmp/cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ft_vectors(model, process):\n",
    "    for tokens in get_tokens(process):\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                vector = model.wv[token]\n",
    "            except:\n",
    "                continue\n",
    "            vectors.append(vector)\n",
    "        yield np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100500,)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ft = np.array([vectors for vectors in get_ft_vectors(model,'train')])\n",
    "X_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split back into two\n",
    "X1_ft = X_ft[:len(X_train)]\n",
    "X2_ft = X_ft[len(X_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del X_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ft_test = np.array([vectors for vectors in get_ft_vectors(model,'test')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49500,)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ft_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split back into two\n",
    "X1_ft_test = X_ft_test[:len(X_test)]\n",
    "X2_ft_test = X_ft_test[len(X_test):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del X_ft_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_lengths(X):\n",
    "    #q_meta = []\n",
    "    for q in X:\n",
    "        #q_meta.append(len(q))\n",
    "        yield len(q)\n",
    "    #return q_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50250,)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_meta_train = [(q1_len, q2_len) for q1_len, q2_len in zip(get_q_lengths(X1_ft), get_q_lengths(X2_ft))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_300 = np.concatenate( \n",
    "    np.vstack( [np.array(np.vsplit(y, y.shape[0])) for y in (x for x in X_ft if x.size>0)] )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(482518, 300)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_300.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del X1_ft, X2_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "## These are the usual ipython objects, including this one you are creating\n",
    "#ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "# Get a sorted list of the objects and their sizes\n",
    "#sorted([(x, sys.getsizeof(globals().get(x))) \n",
    "#        for x in dir() if not x.startswith('_') \n",
    "#        and x not in sys.modules and x not in ipython_vars], \n",
    "#       key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import mmwrite, mmread\n",
    "mmwrite( 'wor2vec_300_train.mtx', X_train_300 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.copy_file(dest_bucket=INPUT_BUCKET, file='wor2vec_300_train.mtx', source='wor2vec_300_train.mtx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del X_train_300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minio.definitions.Object at 0x7fca4a3a7470>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.get_file(bucket=INPUT_BUCKET, filename='embed_train.mtx', filepath='embed_train.mtx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import mmread\n",
    "X_rd = mmread('embed_train.mtx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(482518, 3)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_rd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50250"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(q_meta_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild X1_rd and X2_rd\n",
    "X1_list = []\n",
    "X2_list = []\n",
    "q1_ptr = 0\n",
    "for len_q1, _ in q_meta_train:\n",
    "    q1 = np.array(X_rd[q1_ptr:q1_ptr+len_q1])\n",
    "    X1_list.append(q1)\n",
    "    q1_ptr = q1_ptr+len_q1\n",
    "q2_ptr = q1_ptr\n",
    "for _, len_q2 in q_meta_train:\n",
    "    q2 = np.array(X_rd[q2_ptr:q2_ptr+len_q2])\n",
    "    X2_list.append(q2)\n",
    "    q2_ptr = q2_ptr+len_q2\n",
    "X1_rd = np.array(X1_list)\n",
    "X2_rd = np.array(X2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del X1_list, X2_list, X_rd, X1_rd_tmp, X2_rd_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask import delayed, compute\n",
    "from dask.distributed import Client\n",
    "from utils import dask\n",
    "client = dask.create_dask_client(num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel, polynomial_kernel, sigmoid_kernel, laplacian_kernel, rbf_kernel\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cdist, directed_hausdorff, euclidean\n",
    "from fastdtw import fastdtw\n",
    "import similaritymeasures\n",
    "def compute_pairwise_kernel(pc1, pc2, method='linear'):\n",
    "    if method=='polynomial':\n",
    "        return polynomial_kernel(pc1, pc2, 2)\n",
    "    elif method=='rbf':\n",
    "        return rbf_kernel(pc1, pc2)\n",
    "    elif method=='sigmoid':\n",
    "        return sigmoid_kernel(pc1, pc2)\n",
    "    elif method=='laplacian':\n",
    "        return laplacian_kernel(pc1, pc2)\n",
    "    else:\n",
    "        return linear_kernel(pc1, pc2)\n",
    "    \n",
    "def compute_pairwise_dist(pc1, pc2, method='euclidean'):\n",
    "    if pc1.size == 0 or pc2.size == 0:\n",
    "        return (np.nan,np.nan,np.nan)\n",
    "    if method=='hausdorff':\n",
    "        dist = directed_hausdorff(pc1, pc2)\n",
    "        return dist\n",
    "    if method=='fdtw':\n",
    "        dist, _ = fastdtw(pc1, pc2, dist=euclidean)\n",
    "        return (dist, np.nan, np.nan)\n",
    "    if method=='pcm':\n",
    "        dist = similaritymeasures.pcm(pc1[:,:2], pc2[:,:2])\n",
    "        return (dist, np.nan, np.nan)\n",
    "    if method=='discrete_frechet':\n",
    "        dist = similaritymeasures.frechet_dist(pc1[:,:2], pc2[:,:2])\n",
    "        return (dist, np.nan, np.nan)\n",
    "    if method=='area':\n",
    "        dist = similaritymeasures.area_between_two_curves(pc1[:,:2], pc2[:,:2])\n",
    "        return (dist, np.nan, np.nan)\n",
    "    if method=='curve_length':\n",
    "        dist = similaritymeasures.curve_length_measure(pc1[:,:2], pc2[:,:2])\n",
    "        return (dist, np.nan, np.nan)\n",
    "    if method=='dtw':\n",
    "        dist, _ = similaritymeasures.dtw(pc1[:,:2], pc2[:,:2])\n",
    "        return (dist, np.nan, np.nan)\n",
    "    else:\n",
    "        dist_mat = pairwise_distances(pc1, pc2, metric=method) \n",
    "    #dist_mat = cdist(pc1, pc2, metric=method)\n",
    "    return (np.mean(dist_mat), np.min(dist_mat), np.max(dist_mat))\n",
    "\n",
    "def compute_pairwise_metric(pc1, pc2, method='hausdorff'):\n",
    "    if pc1.size == 0 or pc2.size == 0:\n",
    "        return np.nan\n",
    "    if method == 'mda_hausdorff':\n",
    "        return hausdorff(pc1, pc2)\n",
    "    if method == 'mda_hausdorff_wavg':\n",
    "        return hausdorff_wavg(pc1, pc2)\n",
    "    if method == 'mda_hausdorff_avg':\n",
    "        return hausdorff_avg(pc1, pc2)\n",
    "    if method == 'discrete_frechet':\n",
    "        return discrete_frechet(pc1, pc2)\n",
    "    #return directed_hausdorff(pc1, pc2)[0]\n",
    "        \n",
    "def assign_pwmetric(df, method='euclidean'):\n",
    "    #return compute_pairwise_kernel(pc1_embd, pc2_embd, method=method)\n",
    "    return df.apply(compute_pairwise_dist, method, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic Time Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastdtw\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/ef/a4cf2d72518b5fccd30284ba4d431fdb6266f72e75702ff05fd7c00b0b63/fastdtw-0.3.2.tar.gz (118kB)\n",
      "\u001b[K    100% |████████████████████████████████| 122kB 1.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from fastdtw) (1.15.0)\n",
      "Building wheels for collected packages: fastdtw\n",
      "  Running setup.py bdist_wheel for fastdtw ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/61/62/46/5a9cc316aa50cc4fb36fd4bf14e7dd206642db3247d7746c23\n",
      "Successfully built fastdtw\n",
      "Installing collected packages: fastdtw\n",
      "Successfully installed fastdtw-0.3.2\n"
     ]
    }
   ],
   "source": [
    "#! pip install fastdtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting similaritymeasures\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/43/6a2490996bdb7425c03a37ff2543ead8b8a899c08c54b49cfacf9d8b5acd/similaritymeasures-0.2.3.tar.gz (395kB)\n",
      "\u001b[K    100% |████████████████████████████████| 399kB 2.7MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.6/site-packages (from similaritymeasures) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /opt/conda/lib/python3.6/site-packages (from similaritymeasures) (1.1.0)\n",
      "Building wheels for collected packages: similaritymeasures\n",
      "  Running setup.py bdist_wheel for similaritymeasures ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/bd/f3/6e/232b21a68a527c5b5b3c566b9179b73f7262570989decb536a\n",
      "Successfully built similaritymeasures\n",
      "Installing collected packages: similaritymeasures\n",
      "Successfully installed similaritymeasures-0.2.3\n"
     ]
    }
   ],
   "source": [
    "#! pip install similaritymeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Worker process 176 was killed by unknown signal\n",
      "distributed.nanny - WARNING - Worker process 513 was killed by unknown signal\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "jaccard = []\n",
    "chebyshev = []\n",
    "braycurtis = []\n",
    "cosine = []\n",
    "correlation = []\n",
    "hamming = []\n",
    "canberra = []\n",
    "hausdorff = []\n",
    "cityblock = []\n",
    "euclidean = []\n",
    "l1 = []\n",
    "l2 = []\n",
    "manhattan = []\n",
    "dice = []\n",
    "kulsinski = []\n",
    "rogerstanimoto = []\n",
    "russellrao = []\n",
    "sokalmichener = []\n",
    "minkowski = []\n",
    "seuclidean = []\n",
    "sokalsneath = []\n",
    "sqeuclidean = []\n",
    "fdtw = []\n",
    "dtw = []\n",
    "pcm = []\n",
    "area = []\n",
    "curve_length = []\n",
    "discrete_frechet = []\n",
    "for q_tuple in zip(X1_rd, X2_rd):\n",
    "    if q_tuple:\n",
    "        q1_rd, q2_rd = q_tuple\n",
    "        jaccard.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'jaccard'))\n",
    "        chebyshev.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'chebyshev'))\n",
    "        braycurtis.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'braycurtis'))\n",
    "        cosine.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'cosine'))\n",
    "        correlation.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'correlation'))\n",
    "        hamming.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'hamming'))\n",
    "        canberra.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'canberra'))\n",
    "        hausdorff.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'hausdorff'))\n",
    "        cityblock.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'cityblock'))\n",
    "        euclidean.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'euclidean'))\n",
    "        l1.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'l1'))\n",
    "        l2.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'l2'))\n",
    "        manhattan.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'manhattan'))\n",
    "        dice.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'dice'))\n",
    "        kulsinski.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'kulsinski'))\n",
    "        rogerstanimoto.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'rogerstanimoto'))\n",
    "        russellrao.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'russellrao'))\n",
    "        sokalmichener.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'sokalmichener'))\n",
    "        minkowski.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'minkowski'))\n",
    "        seuclidean.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'seuclidean'))\n",
    "        sokalsneath.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'sokalsneath'))\n",
    "        sqeuclidean.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'sqeuclidean'))\n",
    "        fdtw.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'fdtw'))\n",
    "        dtw.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'dtw'))\n",
    "        pcm.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'pcm'))\n",
    "        area.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'area'))\n",
    "        curve_length.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'curve_length'))\n",
    "        discrete_frechet.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'discrete_frechet'))\n",
    "    else:\n",
    "        jaccard.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        chebyshev.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        braycurtis.append(delayed((np.nan,np.nan,np.nan))) \n",
    "        cosine.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        correlation.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        hamming.append(delayed((np.nan,np.nan,np.nan))) \n",
    "        canberra.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        hausdorff.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        cityblock.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        euclidean.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        l1.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        l2.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        manhattan.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        dice.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        kulsinski.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        rogerstanimoto.append(delayed((np.nan,np.nan,np.nan))) \n",
    "        russellrao.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        sokalmichener.append(delayed((np.nan,np.nan,np.nan))) \n",
    "        minkowski.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        seuclidean.append(delayed((np.nan,np.nan,np.nan))) \n",
    "        sokalsneath.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        sqeuclidean.append(delayed((np.nan,np.nan,np.nan))) \n",
    "        fdtw.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        dtw.append(delayed((np.nan,np.nan,np.nan))) \n",
    "        pcm.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        area.append(delayed((np.nan,np.nan,np.nan))) \n",
    "        curve_length.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        discrete_frechet.append(delayed((np.nan,np.nan,np.nan))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Worker process 264 was killed by unknown signal\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.comm.tcp - WARNING - Closing dangling stream in <TCP local=tcp://172.18.0.4:36226 remote=tcp://172.18.0.4:36289>\n"
     ]
    }
   ],
   "source": [
    "jaccard = compute(*jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chebyshev = compute(*chebyshev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "braycurtis = compute(*braycurtis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine = compute(*cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = compute(*correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamming = compute(*hamming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canberra = compute(*canberra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hausdorff = compute(*hausdorff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityblock = compute(*cityblock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean = compute(*euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = compute(*l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = compute(*l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan = compute(*manhattan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice = compute(*dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kulsinski = compute(*kulsinski)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rogerstanimoto = compute(*rogerstanimoto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "russellrao = compute(*russellrao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sokalmichener = compute(*sokalmichener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minkowski = compute(*minkowski)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seuclidean = compute(*seuclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sokalsneath = compute(*sokalsneath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqeuclidean = compute(*sqeuclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdtw = compute(*fdtw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw = compute(*dtw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcm = compute(*pcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = compute(*area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_length = compute(*curve_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_frechet = compute(*discrete_frechet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(braycurtis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add above metrics to X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train,\n",
    "                     pd.Series((x for x,_,_ in jaccard), name='jaccard_mean',index=X_train.index), \n",
    "                     pd.Series((x for _,x,_ in jaccard), name='jaccard_min',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in jaccard), name='jaccard_max',index=X_train.index), \n",
    "                     pd.Series((x for x,_,_ in chebyshev), name='chebyshev_mean',index=X_train.index), \n",
    "                     pd.Series((x for _,x,_ in chebyshev), name='chebyshev_min',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in chebyshev), name='chebyshev_max',index=X_train.index), \n",
    "                     pd.Series((x for x,_,_ in braycurtis), name='braycurtis_mean',index=X_train.index),\n",
    "                     pd.Series((x for _,x,_ in braycurtis), name='braycurtis_min',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in braycurtis), name='braycurtis_max',index=X_train.index),  \n",
    "                     pd.Series((x for x,_,_ in cosine), name='cosine_mean',index=X_train.index), \n",
    "                     pd.Series((x for _,x,_ in cosine), name='cosine_min',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in cosine), name='cosine_max',index=X_train.index),  \n",
    "                     pd.Series((x for x,_,_ in correlation), name='correlation_mean',index=X_train.index), \n",
    "                     pd.Series((x for _,x,_ in correlation), name='correlation_min',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in correlation), name='correlation_max',index=X_train.index),  \n",
    "                     pd.Series((x for x,_,_ in hamming), name='hamming_mean',index=X_train.index), \n",
    "                     pd.Series((x for _,x,_ in hamming), name='hamming_min',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in hamming), name='hamming_max',index=X_train.index),  \n",
    "                     pd.Series((x for x,_,_ in canberra), name='canberra_mean',index=X_train.index), \n",
    "                     pd.Series((x for _,x,_ in canberra), name='canberra_min',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in canberra), name='canberra_max',index=X_train.index),  \n",
    "                     pd.Series((x for _,_,x in hausdorff), name='hausdorff',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in fdtw), name='fdtw',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in dtw), name='dtw',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in pcm), name='pcm',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in area), name='area',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in curve_length), name='curve_length',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in discrete_frechet), name='discrete_frechet',index=X_train.index),\n",
    "                     pd.Series((x for x,_,_ in cityblock), name='cityblock_mean',index=X_train.index), \n",
    "                     pd.Series((x for _,x,_ in cityblock), name='cityblock_min',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in cityblock), name='cityblock_max',index=X_train.index),  \n",
    "                     pd.Series((x for x,_,_ in euclidean), name='euclidean_mean',index=X_train.index), \n",
    "                     pd.Series((x for _,x,_ in euclidean), name='euclidean_min',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in euclidean), name='euclidean_max',index=X_train.index),    \n",
    "                     pd.Series((x for x,_,_ in l1), name='l1_mean',index=X_train.index), \n",
    "                     pd.Series((x for _,x,_ in l1), name='l1_min',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in l1), name='l1_max',index=X_train.index),\n",
    "                     pd.Series((x for x,_,_ in l2), name='l2_mean',index=X_train.index), \n",
    "                     pd.Series((x for _,x,_ in l2), name='l2_min',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in l2), name='l2_max',index=X_train.index),\n",
    "                     pd.Series((x for x,_,_ in manhattan), name='manhattan_mean',index=X_train.index), \n",
    "                     pd.Series((x for _,x,_ in manhattan), name='manhattan_min',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in manhattan), name='manhattan_max',index=X_train.index),\n",
    "                     pd.Series((x for x,_,_ in dice), name='dice_mean',index=X_train.index), \n",
    "                     pd.Series((x for _,x,_ in dice), name='dice_min',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in dice), name='dice_max',index=X_train.index),\n",
    "                     pd.Series((x for x,_,_ in kulsinski), name='kulsinski_mean',index=X_train.index), \n",
    "                     pd.Series((x for _,x,_ in kulsinski), name='kulsinski_min',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in kulsinski), name='kulsinski_max',index=X_train.index),\n",
    "                     pd.Series((x for x,_,_ in rogerstanimoto), name='rogerstanimoto_mean',index=X_train.index), \n",
    "                     pd.Series((x for _,x,_ in rogerstanimoto), name='rogerstanimoto_min',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in rogerstanimoto), name='rogerstanimoto_max',index=X_train.index),\n",
    "                     pd.Series((x for x,_,_ in russellrao), name='russellrao_mean',index=X_train.index), \n",
    "                     pd.Series((x for _,x,_ in russellrao), name='russellrao_min',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in russellrao), name='russellrao_max',index=X_train.index),\n",
    "                     pd.Series((x for x,_,_ in sokalmichener), name='sokalmichener_mean',index=X_train.index), \n",
    "                     pd.Series((x for _,x,_ in sokalmichener), name='sokalmichener_min',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in sokalmichener), name='sokalmichener_max',index=X_train.index),\n",
    "                     pd.Series((x for x,_,_ in minkowski), name='minkowski_mean',index=X_train.index), \n",
    "                     pd.Series((x for _,x,_ in minkowski), name='minkowski_min',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in minkowski), name='minkowski_max',index=X_train.index),\n",
    "                     pd.Series((x for x,_,_ in seuclidean), name='seuclidean_mean',index=X_train.index), \n",
    "                     pd.Series((x for _,x,_ in seuclidean), name='seuclidean_min',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in seuclidean), name='seuclidean_max',index=X_train.index),\n",
    "                     pd.Series((x for x,_,_ in sokalsneath), name='sokalsneath_mean',index=X_train.index), \n",
    "                     pd.Series((x for _,x,_ in sokalsneath), name='sokalsneath_min',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in sokalsneath), name='sokalsneath_max',index=X_train.index),\n",
    "                     pd.Series((x for x,_,_ in sqeuclidean), name='sqeuclidean_mean',index=X_train.index), \n",
    "                     pd.Series((x for _,x,_ in sqeuclidean), name='sqeuclidean_min',index=X_train.index), \n",
    "                     pd.Series((x for _,_,x in sqeuclidean), name='sqeuclidean_max',index=X_train.index)\n",
    "                    ], axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[X_train.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_meta_test = [(q1_len, q2_len) for q1_len, q2_len in zip(get_q_lengths(X1_ft_test), get_q_lengths(X2_ft_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_300 = np.concatenate( \n",
    "    np.vstack( [np.array(np.vsplit(y, y.shape[0])) for y in (x for x in X_ft_test if x.size>0)] )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_300.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import mmwrite, mmread\n",
    "mmwrite( 'wor2vec_300_test.mtx', X_test_300 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.copy_file(dest_bucket=INPUT_BUCKET, file='wor2vec_300_test.mtx', source='wor2vec_300_test.mtx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del X_test_300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minio.definitions.Object at 0x7fc89bfc9e80>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.get_file(bucket=INPUT_BUCKET, filename='embed_test.mtx', filepath='embed_test.mtx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import mmread\n",
    "X_rd_test = mmread('embed_test.mtx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rd_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild X1_rd_test and X2_rd_test\n",
    "X1_list = []\n",
    "X2_list = []\n",
    "q1_ptr = 0\n",
    "for len_q1, _ in q_meta_test:\n",
    "    q1 = np.array(X_rd_test[q1_ptr:q1_ptr+len_q1])\n",
    "    X1_list.append(q1)\n",
    "    q1_ptr = q1_ptr+len_q1\n",
    "q2_ptr = q1_ptr\n",
    "for _, len_q2 in q_meta_test:\n",
    "    q2 = np.array(X_rd_test[q2_ptr:q2_ptr+len_q2])\n",
    "    X2_list.append(q2)\n",
    "    q2_ptr = q2_ptr+len_q2\n",
    "X1_rd_test = np.array(X1_list)\n",
    "X2_rd_test = np.array(X2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_rd_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del X1_list, X2_list, q1_meta, q2_meta, X_rd_test, X1_rd_tmp, X2_rd_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard = []\n",
    "chebyshev = []\n",
    "braycurtis = []\n",
    "cosine = []\n",
    "correlation = []\n",
    "hamming = []\n",
    "canberra = []\n",
    "hausdorff = []\n",
    "cityblock = []\n",
    "euclidean = []\n",
    "l1 = []\n",
    "l2 = []\n",
    "manhattan = []\n",
    "dice = []\n",
    "kulsinski = []\n",
    "rogerstanimoto = []\n",
    "russellrao = []\n",
    "sokalmichener = []\n",
    "minkowski = []\n",
    "seuclidean = []\n",
    "sokalsneath = []\n",
    "sqeuclidean = []\n",
    "fdtw = []\n",
    "dtw = []\n",
    "pcm = []\n",
    "area = []\n",
    "curve_length = []\n",
    "discrete_frechet = []\n",
    "for q_tuple in zip(X1_rd_test, X2_rd_test):\n",
    "    if q_tuple:\n",
    "        q1_rd, q2_rd = q_tuple\n",
    "        jaccard.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'jaccard'))\n",
    "        chebyshev.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'chebyshev'))\n",
    "        braycurtis.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'braycurtis'))\n",
    "        cosine.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'cosine'))\n",
    "        correlation.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'correlation'))\n",
    "        hamming.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'hamming'))\n",
    "        canberra.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'canberra'))\n",
    "        hausdorff.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'hausdorff'))\n",
    "        cityblock.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'cityblock'))\n",
    "        euclidean.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'euclidean'))\n",
    "        l1.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'l1'))\n",
    "        l2.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'l2'))\n",
    "        manhattan.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'manhattan'))\n",
    "        dice.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'dice'))\n",
    "        kulsinski.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'kulsinski'))\n",
    "        rogerstanimoto.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'rogerstanimoto'))\n",
    "        russellrao.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'russellrao'))\n",
    "        sokalmichener.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'sokalmichener'))\n",
    "        minkowski.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'minkowski'))\n",
    "        seuclidean.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'seuclidean'))\n",
    "        sokalsneath.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'sokalsneath'))\n",
    "        sqeuclidean.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'sqeuclidean'))\n",
    "        fdtw.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'fdtw'))\n",
    "        dtw.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'dtw'))\n",
    "        pcm.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'pcm'))\n",
    "        area.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'area'))\n",
    "        curve_length.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'curve_length'))\n",
    "        discrete_frechet.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'discrete_frechet'))\n",
    "    else:\n",
    "        jaccard.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        chebyshev.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        braycurtis.append(delayed((np.nan,np.nan,np.nan))) \n",
    "        cosine.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        correlation.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        hamming.append(delayed((np.nan,np.nan,np.nan))) \n",
    "        canberra.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        hausdorff.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        cityblock.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        euclidean.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        l1.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        l2.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        manhattan.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        dice.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        kulsinski.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        rogerstanimoto.append(delayed((np.nan,np.nan,np.nan))) \n",
    "        russellrao.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        sokalmichener.append(delayed((np.nan,np.nan,np.nan))) \n",
    "        minkowski.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        seuclidean.append(delayed((np.nan,np.nan,np.nan))) \n",
    "        sokalsneath.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        sqeuclidean.append(delayed((np.nan,np.nan,np.nan))) \n",
    "        fdtw.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        dtw.append(delayed((np.nan,np.nan,np.nan))) \n",
    "        pcm.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        area.append(delayed((np.nan,np.nan,np.nan))) \n",
    "        curve_length.append(delayed((np.nan,np.nan,np.nan)))\n",
    "        discrete_frechet.append(delayed((np.nan,np.nan,np.nan))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard = compute(*jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chebyshev = compute(*chebyshev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "braycurtis = compute(*braycurtis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine = compute(*cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = compute(*correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamming = compute(*hamming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canberra = compute(*canberra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hausdorff = compute(*hausdorff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityblock = compute(*cityblock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean = compute(*euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = compute(*l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = compute(*l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan = compute(*manhattan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice = compute(*dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kulsinski = compute(*kulsinski)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rogerstanimoto = compute(*rogerstanimoto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "russellrao = compute(*russellrao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sokalmichener = compute(*sokalmichener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minkowski = compute(*minkowski)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seuclidean = compute(*seuclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sokalsneath = compute(*sokalsneath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqeuclidean = compute(*sqeuclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdtw = compute(*fdtw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw = compute(*dtw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcm = compute(*pcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = compute(*area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_length = compute(*curve_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_frechet = compute(*discrete_frechet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.concat([X_test,\n",
    "                     pd.Series((x for x,_,_ in jaccard), name='jaccard_mean',index=X_test.index), \n",
    "                     pd.Series((x for _,x,_ in jaccard), name='jaccard_min',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in jaccard), name='jaccard_max',index=X_test.index), \n",
    "                     pd.Series((x for x,_,_ in chebyshev), name='chebyshev_mean',index=X_test.index), \n",
    "                     pd.Series((x for _,x,_ in chebyshev), name='chebyshev_min',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in chebyshev), name='chebyshev_max',index=X_test.index), \n",
    "                     pd.Series((x for x,_,_ in braycurtis), name='braycurtis_mean',index=X_test.index),\n",
    "                     pd.Series((x for _,x,_ in braycurtis), name='braycurtis_min',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in braycurtis), name='braycurtis_max',index=X_test.index),  \n",
    "                     pd.Series((x for x,_,_ in cosine), name='cosine_mean',index=X_test.index), \n",
    "                     pd.Series((x for _,x,_ in cosine), name='cosine_min',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in cosine), name='cosine_max',index=X_test.index),  \n",
    "                     pd.Series((x for x,_,_ in correlation), name='correlation_mean',index=X_test.index), \n",
    "                     pd.Series((x for _,x,_ in correlation), name='correlation_min',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in correlation), name='correlation_max',index=X_test.index),  \n",
    "                     pd.Series((x for x,_,_ in hamming), name='hamming_mean',index=X_test.index), \n",
    "                     pd.Series((x for _,x,_ in hamming), name='hamming_min',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in hamming), name='hamming_max',index=X_test.index),  \n",
    "                     pd.Series((x for x,_,_ in canberra), name='canberra_mean',index=X_test.index), \n",
    "                     pd.Series((x for _,x,_ in canberra), name='canberra_min',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in canberra), name='canberra_max',index=X_test.index),  \n",
    "                     pd.Series((x for _,_,x in hausdorff), name='hausdorff',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in fdtw), name='fdtw',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in dtw), name='dtw',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in pcm), name='pcm',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in area), name='area',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in curve_length), name='curve_length',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in discrete_frechet), name='discrete_frechet',index=X_test.index),\n",
    "                     pd.Series((x for x,_,_ in cityblock), name='cityblock_mean',index=X_test.index), \n",
    "                     pd.Series((x for _,x,_ in cityblock), name='cityblock_min',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in cityblock), name='cityblock_max',index=X_test.index),  \n",
    "                     pd.Series((x for x,_,_ in euclidean), name='euclidean_mean',index=X_test.index), \n",
    "                     pd.Series((x for _,x,_ in euclidean), name='euclidean_min',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in euclidean), name='euclidean_max',index=X_test.index),    \n",
    "                     pd.Series((x for x,_,_ in l1), name='l1_mean',index=X_test.index), \n",
    "                     pd.Series((x for _,x,_ in l1), name='l1_min',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in l1), name='l1_max',index=X_test.index),\n",
    "                     pd.Series((x for x,_,_ in l2), name='l2_mean',index=X_test.index), \n",
    "                     pd.Series((x for _,x,_ in l2), name='l2_min',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in l2), name='l2_max',index=X_test.index),\n",
    "                     pd.Series((x for x,_,_ in manhattan), name='manhattan_mean',index=X_test.index), \n",
    "                     pd.Series((x for _,x,_ in manhattan), name='manhattan_min',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in manhattan), name='manhattan_max',index=X_test.index),\n",
    "                     pd.Series((x for x,_,_ in dice), name='dice_mean',index=X_test.index), \n",
    "                     pd.Series((x for _,x,_ in dice), name='dice_min',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in dice), name='dice_max',index=X_test.index),\n",
    "                     pd.Series((x for x,_,_ in kulsinski), name='kulsinski_mean',index=X_test.index), \n",
    "                     pd.Series((x for _,x,_ in kulsinski), name='kulsinski_min',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in kulsinski), name='kulsinski_max',index=X_test.index),\n",
    "                     pd.Series((x for x,_,_ in rogerstanimoto), name='rogerstanimoto_mean',index=X_test.index), \n",
    "                     pd.Series((x for _,x,_ in rogerstanimoto), name='rogerstanimoto_min',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in rogerstanimoto), name='rogerstanimoto_max',index=X_test.index),\n",
    "                     pd.Series((x for x,_,_ in russellrao), name='russellrao_mean',index=X_test.index), \n",
    "                     pd.Series((x for _,x,_ in russellrao), name='russellrao_min',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in russellrao), name='russellrao_max',index=X_test.index),\n",
    "                     pd.Series((x for x,_,_ in sokalmichener), name='sokalmichener_mean',index=X_test.index), \n",
    "                     pd.Series((x for _,x,_ in sokalmichener), name='sokalmichener_min',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in sokalmichener), name='sokalmichener_max',index=X_test.index),\n",
    "                     pd.Series((x for x,_,_ in minkowski), name='minkowski_mean',index=X_test.index), \n",
    "                     pd.Series((x for _,x,_ in minkowski), name='minkowski_min',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in minkowski), name='minkowski_max',index=X_test.index),\n",
    "                     pd.Series((x for x,_,_ in seuclidean), name='seuclidean_mean',index=X_test.index), \n",
    "                     pd.Series((x for _,x,_ in seuclidean), name='seuclidean_min',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in seuclidean), name='seuclidean_max',index=X_test.index),\n",
    "                     pd.Series((x for x,_,_ in sokalsneath), name='sokalsneath_mean',index=X_test.index), \n",
    "                     pd.Series((x for _,x,_ in sokalsneath), name='sokalsneath_min',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in sokalsneath), name='sokalsneath_max',index=X_test.index),\n",
    "                     pd.Series((x for x,_,_ in sqeuclidean), name='sqeuclidean_mean',index=X_test.index), \n",
    "                     pd.Series((x for _,x,_ in sqeuclidean), name='sqeuclidean_min',index=X_test.index), \n",
    "                     pd.Series((x for _,_,x in sqeuclidean), name='sqeuclidean_max',index=X_test.index)\n",
    "                    ], axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[X_test.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "pass_through = lambda x:x\n",
    "tfidf = TfidfVectorizer(analyzer=pass_through)\n",
    "X_trfmd = tfidf.fit_transform(get_tokens('train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trfmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension reduction using SVD\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import time\n",
    "start = time.time()\n",
    "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42)\n",
    "X_svd = svd.fit_transform(X_trfmd)\n",
    "end =  time.time()\n",
    "print('created SVD transform in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_svd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split back into two\n",
    "X1 = X_svd[:len(X_train), :]\n",
    "X2 = X_svd[len(X_train):, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_trfmd = tfidf.transform(get_tokens('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_trfmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension reduction using SVD\n",
    "start = time.time()\n",
    "X_test_svd = svd.transform(X_test_trfmd)\n",
    "end =  time.time()\n",
    "print('created SVD transform in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split back into two\n",
    "X1_test = X_test_svd[:len(X_test), :]\n",
    "X2_test = X_test_svd[len(X_test):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build complete feature dataframe\n",
    "X_test_temp = pd.concat([pd.DataFrame(X1_test, columns=['q1_'+str(i) for i in range(X1_test.shape[1])], index=X_test.index), \n",
    "                    pd.DataFrame(X2_test, columns=['q2_'+str(i) for i in range(X2_test.shape[1])], index=X_test.index)], axis=1)\n",
    "X_test_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy-wuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference in text size\n",
    "compute_size_diff = lambda row: abs(len(str(row['question1'])) - len(str(row['question2'])))\n",
    "X_train['size_diff'] = X_train.apply(compute_size_diff, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio\n",
    "compute_ratio = lambda row: fuzz.ratio(str(row['question1']), str(row['question2']))\n",
    "X_train['ratio'] = X_train.apply(compute_ratio, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial ratio\n",
    "compute_partial_ratio = lambda row: fuzz.partial_ratio(str(row['question1']), str(row['question2']))\n",
    "X_train['partial_ratio'] = X_train.apply(compute_partial_ratio, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_sort_ratio\n",
    "compute_token_sort_ratio = lambda row: fuzz.token_sort_ratio(str(row['question1']), str(row['question2']))\n",
    "X_train['token_sort_ratio'] = X_train.apply(compute_token_sort_ratio, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_set_ratio\n",
    "compute_token_set_ratio = lambda row: fuzz.token_set_ratio(str(row['question1']), str(row['question2']))\n",
    "X_train['token_set_ratio'] = X_train.apply(compute_token_set_ratio, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build complete feature dataframe\n",
    "#X_train_temp = pd.concat([pd.DataFrame(X1, columns=['q1_'+str(i) for i in range(X1.shape[1])], index=X_train.index), \n",
    "#                     pd.DataFrame(X2, columns=['q2_'+str(i) for i in range(X2.shape[1])], index=X_train.index)], axis=1)\n",
    "#X_train_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = pd.concat([X_train_temp, X_train], axis=1)\n",
    "#del X_train_temp\n",
    "X_train_final = X_train.drop(columns=['qid1', 'qid2','question1','question2']).dropna()\n",
    "X_train_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference in text size\n",
    "X_test['size_diff'] = X_test.apply(compute_size_diff, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio\n",
    "X_test['ratio'] = X_test.apply(compute_ratio, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial ratio\n",
    "X_test['partial_ratio'] = X_test.apply(compute_partial_ratio, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_sort_ratio\n",
    "X_test['token_sort_ratio'] = X_test.apply(compute_token_sort_ratio, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_set_ratio\n",
    "X_test['token_set_ratio'] = X_test.apply(compute_token_set_ratio, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final = X_test.drop(columns=['question1','question2', 'qid1', 'qid2']).dropna()\n",
    "X_test_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "logr_model = LogisticRegression(random_state=42)\n",
    "param_grid = {'C': np.logspace(-2, 7, 10),\n",
    "             #'penalty': ['l1','l2'],\n",
    "             'tol': np.logspace(-5, -1, 5),\n",
    "             #'solver': ['lbfgs']\n",
    "             #'max_iter': np.linspace(10, 1000, 10)\n",
    "             }\n",
    "logr_cv = RandomizedSearchCV(logr_model, param_distributions=param_grid, cv=5, n_jobs=-1)\n",
    "y_train_final = y_train.loc[X_train_final.index]\n",
    "logr_cv.fit(X_train_final, y_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_model = LogisticRegression(#solver=logr_cv.best_params_['solver'], \n",
    "                                random_state=42, \n",
    "                                C=logr_cv.best_params_['C'], \n",
    "                                tol=logr_cv.best_params_['tol'], \n",
    "                                #max_iter=logr_cv.best_params_['max_iter'], \n",
    "                                n_jobs=-1)\n",
    "logr_model.fit(X_train_final, y_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_pred = logr_model.predict(X_test_final)\n",
    "y_test_final = y_test.loc[X_test_final.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "logr_acc_score = accuracy_score(y_test_final, logr_pred)\n",
    "logr_prec_score = precision_score(y_test_final, logr_pred)\n",
    "logr_rec_score = recall_score(y_test_final, logr_pred)\n",
    "print('Logistic Regression')\n",
    "print('accuracy score : {}'.format(logr_acc_score))\n",
    "print('precision score : {}'.format(logr_prec_score))\n",
    "print('recall score : {}'.format(logr_rec_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XQBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! conda install -c conda-forge py-xgboost -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# Model selection\n",
    "params_xgb = {'n_estimators' : [1, 2, 4, 8, 16, 32, 64, 100, 200],\n",
    "               'gamma':np.linspace(.01, 1, 10, endpoint=True), \n",
    "               'learning_rate' : np.linspace(.01, 1, 10, endpoint=True),\n",
    "               'reg_lambda': np.linspace(0.01, 10, 20, endpoint=True),\n",
    "               'max_depth' : np.linspace(1, 32, 32, endpoint=True, dtype=int)\n",
    "                 }\n",
    "cv_xgb = RandomizedSearchCV(xgb.XGBClassifier(objective='binary:logistic', random_state=42), param_distributions=params_xgb, cv=5, n_jobs=3, random_state=42)\n",
    "cv_xgb.fit(X_train_final, y_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_xgb_model = xgb.XGBClassifier(random_state=42,\n",
    "                                  n_estimators=cv_xgb.best_params_['n_estimators'],\n",
    "                                  gamma=cv_xgb.best_params_['gamma'],\n",
    "                                  learning_rate=cv_xgb.best_params_['learning_rate'],\n",
    "                                  reg_lambda=cv_xgb.best_params_['reg_lambda'],\n",
    "                                  max_depth=cv_xgb.best_params_['max_depth'])\n",
    "clf_xgb_model.fit(X_train_final, y_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb = clf_xgb_model.predict(X_test_final)\n",
    "score_xgb = accuracy_score(y_test_final, y_pred_xgb)\n",
    "rscore_xgb = recall_score(y_test_final, y_pred_xgb)\n",
    "pscore_xgb = precision_score(y_test_final, y_pred_xgb)\n",
    "print('Accuracy score for XGBoost ', score_xgb)\n",
    "print('Recall score for XGBoost ', rscore_xgb)\n",
    "print('Precision score for XGBoost ', pscore_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
