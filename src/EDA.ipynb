{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import persistence as ps\n",
    "from urllib3.response import HTTPResponse\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import partition_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_BUCKET = 'dq-data'\n",
    "HASH_BUCKET = 'dq-hashed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train_set\n",
    "data = 'train.csv'\n",
    "filestream = ps.get_file_stream(bucket=INPUT_BUCKET, filename=data)\n",
    "dtypes = {\n",
    "    'id': 'int64',\n",
    "    'qid1': 'int64',\n",
    "    'qid2': 'int64',\n",
    "    'question1': 'object',\n",
    "    'question2': 'object',\n",
    "    'is_duplicate': 'int64'\n",
    "}\n",
    "df = pd.read_csv(#urlpath=s3_in_url, \n",
    "                                     #storage_options=s3_options,\n",
    "                                     filestream,\n",
    "                                     header=0, \n",
    "                                     usecols=dtypes.keys(), \n",
    "                                     #names=dtypes.keys(),\n",
    "                                     skipinitialspace=True,\n",
    "                                     skip_blank_lines=True,\n",
    "                                     encoding='utf-8')\n",
    "df = df.set_index('id')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 404287 entries, 0 to 404289\n",
      "Data columns (total 5 columns):\n",
      "qid1            404287 non-null int64\n",
      "qid2            404287 non-null int64\n",
      "question1       404287 non-null object\n",
      "question2       404287 non-null object\n",
      "is_duplicate    404287 non-null int64\n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 18.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#shrink df to 150,000 records\n",
    "df = df.iloc[:75000]\n",
    "\n",
    "X = df.drop(columns=['is_duplicate'])\n",
    "\n",
    "y = df['is_duplicate']\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 50250 entries, 71916 to 15795\n",
      "Data columns (total 4 columns):\n",
      "qid1         50250 non-null int64\n",
      "qid2         50250 non-null int64\n",
      "question1    50250 non-null object\n",
      "question2    50250 non-null object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del X,y,df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "def get_tokens(process='train'):\n",
    "    if process=='test':\n",
    "        X = X_test\n",
    "    else:\n",
    "        X = X_train\n",
    "    series = pd.Series(pd.concat([X['question1'], X['question2']]),dtype=str)\n",
    "    series.dropna()\n",
    "    for question in series:\n",
    "        yield preprocess_string(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec (fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.get_file(bucket=INPUT_BUCKET, filename='cc.en.300.bin.gz', filepath='/tmp/cc.en.300.bin.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "with gzip.open('/tmp/cc.en.300.bin.gz', 'rb') as f_in:\n",
    "    with open('/tmp/cc.en.300.bin', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.remove('/tmp/cc.en.300.bin.gz')\n",
    "from gensim.models import FastText\n",
    "model = FastText.load_fasttext_format('/tmp/cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ft_vectors(model, process):\n",
    "    for tokens in get_tokens(process):\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                vector = model.wv[token]\n",
    "            except:\n",
    "                continue\n",
    "            vectors.append(vector)\n",
    "        yield np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100500,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ft = np.array([vectors for vectors in get_ft_vectors(model,'train')])\n",
    "X_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split back into two\n",
    "X1_ft = X_ft[:len(X_train)]\n",
    "X2_ft = X_ft[len(X_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del X_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ft_test = np.array([vectors for vectors in get_ft_vectors(model,'test')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49500,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ft_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split back into two\n",
    "X1_ft_test = X_ft_test[:len(X_test)]\n",
    "X2_ft_test = X_ft_test[len(X_test):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del X_ft_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_lengths(X):\n",
    "    #q_meta = []\n",
    "    for q in X:\n",
    "        #q_meta.append(len(q))\n",
    "        yield len(q)\n",
    "    #return q_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50250,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_meta_train = [(q1_len, q2_len) for q1_len, q2_len in zip(get_q_lengths(X1_ft), get_q_lengths(X2_ft))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_300 = np.concatenate( \n",
    "    np.vstack( [np.array(np.vsplit(y, y.shape[0])) for y in (x for x in X_ft if x.size>0)] )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(482518, 300)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_300.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del X1_ft, X2_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "## These are the usual ipython objects, including this one you are creating\n",
    "#ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "# Get a sorted list of the objects and their sizes\n",
    "#sorted([(x, sys.getsizeof(globals().get(x))) \n",
    "#        for x in dir() if not x.startswith('_') \n",
    "#        and x not in sys.modules and x not in ipython_vars], \n",
    "#       key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import mmwrite, mmread\n",
    "mmwrite( 'wor2vec_300_train.mtx', X_train_300 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.copy_file(dest_bucket=INPUT_BUCKET, file='wor2vec_300_train.mtx', source='wor2vec_300_train.mtx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del X_train_300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minio.definitions.Object at 0x7f1b83097668>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.get_file(bucket=INPUT_BUCKET, filename='embed_train.mtx', filepath='embed_train.mtx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import mmread\n",
    "X_rd = mmread('embed_train.mtx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(482518, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_rd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50250"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(q_meta_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild X1_rd and X2_rd\n",
    "X1_list = []\n",
    "X2_list = []\n",
    "q1_ptr = 0\n",
    "for len_q1, _ in q_meta_train:\n",
    "    q1 = np.array(X_rd[q1_ptr:q1_ptr+len_q1])\n",
    "    X1_list.append(q1)\n",
    "    q1_ptr = q1_ptr+len_q1\n",
    "q2_ptr = q1_ptr\n",
    "for _, len_q2 in q_meta_train:\n",
    "    q2 = np.array(X_rd[q2_ptr:q2_ptr+len_q2])\n",
    "    X2_list.append(q2)\n",
    "    q2_ptr = q2_ptr+len_q2\n",
    "X1_rd = np.array(X1_list)\n",
    "X2_rd = np.array(X2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del X1_list, X2_list, X_rd, X1_rd_tmp, X2_rd_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask import delayed, compute\n",
    "from dask.distributed import Client\n",
    "from utils import dask\n",
    "client = dask.create_dask_client(num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import ConvexHull\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "from math import atan2, cos, sin, pi\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "def unit_vector(pt0, pt1):\n",
    "    # returns an unit vector that points in the direction of pt0 to pt1\n",
    "    dis_0_to_1 = sqrt((pt0[0] - pt1[0])**2 + (pt0[1] - pt1[1])**2)\n",
    "    return (pt1[0] - pt0[0]) / dis_0_to_1, \\\n",
    "           (pt1[1] - pt0[1]) / dis_0_to_1\n",
    "\n",
    "\n",
    "def orthogonal_vector(vector):\n",
    "    # from vector returns a orthogonal/perpendicular vector of equal length\n",
    "    return -1 * vector[1], vector[0]\n",
    "\n",
    "\n",
    "def bounding_area(index, hull):\n",
    "    unit_vector_p = unit_vector(hull[index], hull[index+1])\n",
    "    unit_vector_o = orthogonal_vector(unit_vector_p)\n",
    "\n",
    "    dis_p = tuple(np.dot(unit_vector_p, pt) for pt in hull)\n",
    "    dis_o = tuple(np.dot(unit_vector_o, pt) for pt in hull)\n",
    "\n",
    "    min_p = min(dis_p)\n",
    "    min_o = min(dis_o)\n",
    "    len_p = max(dis_p) - min_p\n",
    "    len_o = max(dis_o) - min_o\n",
    "\n",
    "    return {'area': len_p * len_o,\n",
    "            'length_parallel': len_p,\n",
    "            'length_orthogonal': len_o,\n",
    "            'rectangle_center': (min_p + len_p / 2, min_o + len_o / 2),\n",
    "            'unit_vector': unit_vector_p,\n",
    "            }\n",
    "\n",
    "\n",
    "def to_xy_coordinates(unit_vector_angle, point):\n",
    "    # returns converted unit vector coordinates in x, y coordinates\n",
    "    angle_orthogonal = unit_vector_angle + pi / 2\n",
    "    return point[0] * cos(unit_vector_angle) + point[1] * cos(angle_orthogonal), \\\n",
    "           point[0] * sin(unit_vector_angle) + point[1] * sin(angle_orthogonal)\n",
    "\n",
    "\n",
    "def rotate_points(center_of_rotation, angle, points):\n",
    "    # Requires: center_of_rotation to be a 2d vector. ex: (1.56, -23.4)\n",
    "    #           angle to be in radians\n",
    "    #           points to be a list or tuple of points. ex: ((1.56, -23.4), (1.56, -23.4))\n",
    "    # Effects: rotates a point cloud around the center_of_rotation point by angle\n",
    "    rot_points = []\n",
    "    ang = []\n",
    "    for pt in points:\n",
    "        diff = tuple([pt[d] - center_of_rotation[d] for d in range(2)])\n",
    "        diff_angle = atan2(diff[1], diff[0]) + angle\n",
    "        ang.append(diff_angle)\n",
    "        diff_length = sqrt(sum([d**2 for d in diff]))\n",
    "        rot_points.append((center_of_rotation[0] + diff_length * cos(diff_angle),\n",
    "                           center_of_rotation[1] + diff_length * sin(diff_angle)))\n",
    "\n",
    "    return rot_points\n",
    "\n",
    "\n",
    "def rectangle_corners(rectangle):\n",
    "    # Requires: the output of mon_bounding_rectangle\n",
    "    # Effects: returns the corner locations of the bounding rectangle\n",
    "    corner_points = []\n",
    "    for i1 in (.5, -.5):\n",
    "        for i2 in (i1, -1 * i1):\n",
    "            corner_points.append((rectangle['rectangle_center'][0] + i1 * rectangle['length_parallel'],\n",
    "                            rectangle['rectangle_center'][1] + i2 * rectangle['length_orthogonal']))\n",
    "\n",
    "    return rotate_points(rectangle['rectangle_center'], rectangle['unit_vector_angle'], corner_points)\n",
    "\n",
    "\n",
    "BoundingBox = namedtuple('BoundingBox', ('area',\n",
    "                                         'length_parallel',\n",
    "                                         'length_orthogonal',\n",
    "                                         'rectangle_center',\n",
    "                                         'unit_vector',\n",
    "                                         'unit_vector_angle',\n",
    "                                         'corner_points'\n",
    "                                        )\n",
    ")\n",
    "\n",
    "\n",
    "# use this function to find the listed properties of the minimum bounding box of a point cloud\n",
    "def MinimumBoundingBox(points):\n",
    "    # Requires: points to be a list or tuple of 2D points. ex: ((5, 2), (3, 4), (6, 8))\n",
    "    #           needs to be more than 2 points\n",
    "    # Effects:  returns a namedtuple that contains:\n",
    "    #               area: area of the rectangle\n",
    "    #               length_parallel: length of the side that is parallel to unit_vector\n",
    "    #               length_orthogonal: length of the side that is orthogonal to unit_vector\n",
    "    #               rectangle_center: coordinates of the rectangle center\n",
    "    #                   (use rectangle_corners to get the corner points of the rectangle)\n",
    "    #               unit_vector: direction of the length_parallel side. RADIANS\n",
    "    #                   (it's orthogonal vector can be found with the orthogonal_vector function\n",
    "    #               unit_vector_angle: angle of the unit vector\n",
    "    #               corner_points: set that contains the corners of the rectangle\n",
    "\n",
    "    if len(points) <= 2: raise ValueError('More than two points required.')\n",
    "\n",
    "    hull_ordered = [points[index] for index in ConvexHull(points).vertices]\n",
    "    hull_ordered.append(hull_ordered[0])\n",
    "    hull_ordered = tuple(hull_ordered)\n",
    "\n",
    "    min_rectangle = bounding_area(0, hull_ordered)\n",
    "    for i in range(1, len(hull_ordered)-1):\n",
    "        rectangle = bounding_area(i, hull_ordered)\n",
    "        if rectangle['area'] < min_rectangle['area']:\n",
    "            min_rectangle = rectangle\n",
    "\n",
    "    min_rectangle['unit_vector_angle'] = atan2(min_rectangle['unit_vector'][1], min_rectangle['unit_vector'][0])\n",
    "    min_rectangle['rectangle_center'] = to_xy_coordinates(min_rectangle['unit_vector_angle'], min_rectangle['rectangle_center'])\n",
    "\n",
    "    # this is ugly but a quick hack and is being changed in the speedup branch\n",
    "    return BoundingBox(\n",
    "        area = min_rectangle['area'],\n",
    "        length_parallel = min_rectangle['length_parallel'],\n",
    "        length_orthogonal = min_rectangle['length_orthogonal'],\n",
    "        rectangle_center = min_rectangle['rectangle_center'],\n",
    "        unit_vector = min_rectangle['unit_vector'],\n",
    "        unit_vector_angle = min_rectangle['unit_vector_angle'],\n",
    "        corner_points = set(rectangle_corners(min_rectangle))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_array(points_set):\n",
    "    points = []\n",
    "    for point in points_set:\n",
    "        points.append(list(point))\n",
    "    return np.array(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel, polynomial_kernel, sigmoid_kernel, laplacian_kernel, rbf_kernel\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cdist, directed_hausdorff\n",
    "from fastdtw import fastdtw\n",
    "import similaritymeasures\n",
    "from scipy.spatial import procrustes\n",
    "def compute_pairwise_kernel(pc1, pc2, method='linear'):\n",
    "    if method=='polynomial':\n",
    "        return polynomial_kernel(pc1, pc2, 2)\n",
    "    elif method=='rbf':\n",
    "        return rbf_kernel(pc1, pc2)\n",
    "    elif method=='sigmoid':\n",
    "        return sigmoid_kernel(pc1, pc2)\n",
    "    elif method=='laplacian':\n",
    "        return laplacian_kernel(pc1, pc2)\n",
    "    else:\n",
    "        return linear_kernel(pc1, pc2)\n",
    "    \n",
    "def compute_pairwise_dist(pc1, pc2, method='euclidean'):\n",
    "    if pc1.size == 0 or pc2.size == 0:\n",
    "        return np.nan\n",
    "    if method=='hausdorff':\n",
    "        dist = directed_hausdorff(pc1, pc2)\n",
    "        return dist[0]\n",
    "    else:\n",
    "        dist_mat = pairwise_distances(pc1, pc2, metric=method) \n",
    "    #dist_mat = cdist(pc1, pc2, metric=method)\n",
    "    return np.linalg.norm(dist_mat, ord='fro')\n",
    "\n",
    "def compute_pairwise_metric(pc1, pc2, method='dtw'):\n",
    "    if pc1.size == 0 or pc2.size == 0:\n",
    "        return np.nan\n",
    "    #if method=='fdtw':\n",
    "    #    dist, _ = fastdtw(pc1, pc2, dist=euclidean)\n",
    "    if method=='pcm':\n",
    "        dist = similaritymeasures.pcm(pc1[:,:2], pc2[:,:2])\n",
    "    if method=='discrete_frechet':\n",
    "        dist = similaritymeasures.frechet_dist(pc1[:,:2], pc2[:,:2])\n",
    "    if method=='area':\n",
    "        dist = similaritymeasures.area_between_two_curves(pc1[:,:2], pc2[:,:2])\n",
    "    if method=='curve_length':\n",
    "        dist = similaritymeasures.curve_length_measure(pc1[:,:2], pc2[:,:2])\n",
    "    if method=='dtw':\n",
    "        dist, _ = similaritymeasures.dtw(pc1[:,:2], pc2[:,:2])\n",
    "    if method=='procrustes': \n",
    "        mbox1 = MinimumBoundingBox([x[:2] for x in pc1.tolist()])\n",
    "        mbox2 = MinimumBoundingBox([x[:2] for x in pc2.tolist()])\n",
    "        _,_,dist = procrustes(make_array(mbox1.corner_points), make_array(mbox2.corner_points))\n",
    "    return dist\n",
    "\n",
    "        \n",
    "def assign_pwmetric(df, method='euclidean'):\n",
    "    #return compute_pairwise_kernel(pc1_embd, pc2_embd, method=method)\n",
    "    return df.apply(compute_pairwise_dist, method, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mbox2 = MinimumBoundingBox([x[:2] for x in q2_temp.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mbox1 = MinimumBoundingBox([x[:2] for x in q1_temp.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_array(mbox2.corner_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic Time Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install fastdtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install similaritymeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard = []\n",
    "chebyshev = []\n",
    "braycurtis = []\n",
    "cosine = []\n",
    "correlation = []\n",
    "hamming = []\n",
    "canberra = []\n",
    "hausdorff = []\n",
    "cityblock = []\n",
    "euclidean = []\n",
    "l1 = []\n",
    "l2 = []\n",
    "manhattan = []\n",
    "dice = []\n",
    "kulsinski = []\n",
    "rogerstanimoto = []\n",
    "russellrao = []\n",
    "sokalmichener = []\n",
    "minkowski = []\n",
    "seuclidean = []\n",
    "sokalsneath = []\n",
    "sqeuclidean = []\n",
    "#fdtw = []\n",
    "dtw = []\n",
    "#pcm = []\n",
    "#area = []\n",
    "curve_length = []\n",
    "discrete_frechet = []\n",
    "procrustes = []\n",
    "for q_tuple in zip(X1_rd, X2_rd):\n",
    "    if q_tuple:\n",
    "        q1_rd, q2_rd = q_tuple\n",
    "        jaccard.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'jaccard'))\n",
    "        chebyshev.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'chebyshev'))\n",
    "        braycurtis.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'braycurtis'))\n",
    "        cosine.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'cosine'))\n",
    "        correlation.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'correlation'))\n",
    "        hamming.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'hamming'))\n",
    "        canberra.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'canberra'))\n",
    "        hausdorff.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'hausdorff'))\n",
    "        cityblock.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'cityblock'))\n",
    "        euclidean.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'euclidean'))\n",
    "        l1.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'l1'))\n",
    "        l2.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'l2'))\n",
    "        manhattan.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'manhattan'))\n",
    "        dice.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'dice'))\n",
    "        kulsinski.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'kulsinski'))\n",
    "        rogerstanimoto.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'rogerstanimoto'))\n",
    "        russellrao.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'russellrao'))\n",
    "        sokalmichener.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'sokalmichener'))\n",
    "        minkowski.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'minkowski'))\n",
    "        seuclidean.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'seuclidean'))\n",
    "        sokalsneath.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'sokalsneath'))\n",
    "        sqeuclidean.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'sqeuclidean'))\n",
    "        #fdtw.append(delayed(compute_pairwise_metric)(q1_rd, q2_rd, 'fdtw'))\n",
    "        dtw.append(delayed(compute_pairwise_metric)(q1_rd, q2_rd, 'dtw'))\n",
    "        #pcm.append(delayed(compute_pairwise_metric)(q1_rd, q2_rd, 'pcm'))\n",
    "        #area.append(delayed(compute_pairwise_metric)(q1_rd, q2_rd, 'area'))\n",
    "        curve_length.append(delayed(compute_pairwise_metric)(q1_rd, q2_rd, 'curve_length'))\n",
    "        discrete_frechet.append(delayed(compute_pairwise_metric)(q1_rd, q2_rd, 'discrete_frechet'))\n",
    "        procrustes.append(delayed(compute_pairwise_metric)(q1_rd, q2_rd, 'procrustes'))\n",
    "    else:\n",
    "        jaccard.append(delayed(np.nan))\n",
    "        chebyshev.append(delayed(np.nan))\n",
    "        braycurtis.append(delayed(np.nan))\n",
    "        cosine.append(delayed(np.nan))\n",
    "        correlation.append(delayed(np.nan))\n",
    "        hamming.append(delayed(np.nan))\n",
    "        canberra.append(delayed(np.nan))\n",
    "        hausdorff.append(delayed(np.nan))\n",
    "        cityblock.append(delayed(np.nan))\n",
    "        euclidean.append(delayed(np.nan))\n",
    "        l1.append(delayed(np.nan))\n",
    "        l2.append(delayed(np.nan))\n",
    "        manhattan.append(delayed(np.nan))\n",
    "        dice.append(delayed(np.nan))\n",
    "        kulsinski.append(delayed(np.nan))\n",
    "        rogerstanimoto.append(delayed(np.nan))\n",
    "        russellrao.append(delayed(np.nan))\n",
    "        sokalmichener.append(delayed(np.nan))\n",
    "        minkowski.append(delayed(np.nan))\n",
    "        seuclidean.append(delayed(np.nan))\n",
    "        sokalsneath.append(delayed(np.nan))\n",
    "        sqeuclidean.append(delayed(np.nan)) \n",
    "        #fdtw.append(delayed(np.nan)\n",
    "        dtw.append(delayed(np.nan))\n",
    "        #pcm.append(delayed(np.nan)\n",
    "        #area.append(delayed(np.nan) \n",
    "        curve_length.append(delayed(np.nan))\n",
    "        discrete_frechet.append(delayed(np.nan))\n",
    "        procrustes.append(delayed(np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard = compute(*jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "chebyshev = compute(*chebyshev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "braycurtis = compute(*braycurtis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine = compute(*cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = compute(*correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamming = compute(*hamming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "canberra = compute(*canberra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hausdorff = compute(*hausdorff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityblock = compute(*cityblock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Worker process 623 was killed by unknown signal\n"
     ]
    }
   ],
   "source": [
    "euclidean = compute(*euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Worker process 780 was killed by unknown signal\n",
      "distributed.comm.tcp - WARNING - Closing dangling stream in <TCP local=tcp://172.18.0.4:59224 remote=tcp://172.18.0.4:44667>\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "l1 = compute(*l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Worker process 625 was killed by unknown signal\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\x16'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-33d4b29fe8eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_keys__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0mpostcomputes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, dsk, keys, restrictions, loose_restrictions, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[1;32m   2336\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2337\u001b[0m                 results = self.gather(packed, asynchronous=asynchronous,\n\u001b[0;32m-> 2338\u001b[0;31m                                       direct=direct)\n\u001b[0m\u001b[1;32m   2339\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(self, futures, errors, maxsize, direct, asynchronous)\u001b[0m\n\u001b[1;32m   1660\u001b[0m             return self.sync(self._gather, futures, errors=errors,\n\u001b[1;32m   1661\u001b[0m                              \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1662\u001b[0;31m                              asynchronous=asynchronous)\n\u001b[0m\u001b[1;32m   1663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoroutine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tornado/gen.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m                         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhad_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tornado/gen.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1139\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mexc_info\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m                             \u001b[0myielded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m                         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m                             \u001b[0;31m# Break up a reference to itself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36m_gather\u001b[0;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[1;32m   1501\u001b[0m                             six.reraise(type(exception),\n\u001b[1;32m   1502\u001b[0m                                         \u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1503\u001b[0;31m                                         traceback)\n\u001b[0m\u001b[1;32m   1504\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'skip'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m                         \u001b[0mbad_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    690\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/distributed/protocol/pickle.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Failed to deserialize %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\x16'."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.comm.tcp - WARNING - Closing dangling stream in <TCP local=tcp://172.18.0.4:59250 remote=tcp://172.18.0.4:44667>\n"
     ]
    }
   ],
   "source": [
    "l2 = compute(*l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan = compute(*manhattan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice = compute(*dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kulsinski = compute(*kulsinski)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rogerstanimoto = compute(*rogerstanimoto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "russellrao = compute(*russellrao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sokalmichener = compute(*sokalmichener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minkowski = compute(*minkowski)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seuclidean = compute(*seuclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sokalsneath = compute(*sokalsneath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqeuclidean = compute(*sqeuclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fdtw = compute(*fdtw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw = compute(*dtw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pcm = compute(*pcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#area = compute(*area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#curve_length = compute(*curve_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_frechet = compute(*discrete_frechet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procrustes = compute(*procrustes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(braycurtis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.isnan(procrustes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add above metrics to X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train,\n",
    "                     pd.Series(jaccard, name='jaccard',index=X_train.index), \n",
    "                     pd.Series(chebyshev, name='chebyshev',index=X_train.index), \n",
    "                     pd.Series(braycurtis, name='braycurtis',index=X_train.index),\n",
    "                     pd.Series(cosine, name='cosine',index=X_train.index),\n",
    "                     pd.Series(correlation, name='correlation',index=X_train.index),  \n",
    "                     pd.Series(hamming, name='hamming',index=X_train.index), \n",
    "                     pd.Series(canberra, name='canberra',index=X_train.index),  \n",
    "                     pd.Series(hausdorff, name='hausdorff',index=X_train.index), \n",
    "                     #pd.Series((x for x,_,_ in fdtw), name='fdtw',index=X_train.index), \n",
    "                     pd.Series(dtw, name='dtw',index=X_train.index), \n",
    "                     #pd.Series((x for x,_,_ in pcm), name='pcm',index=X_train.index), \n",
    "                     #pd.Series((x for x,_,_ in area), name='area',index=X_train.index), \n",
    "                     #pd.Series(curve_length, name='curve_length',index=X_train.index), \n",
    "                     pd.Series(discrete_frechet, name='discrete_frechet',index=X_train.index),\n",
    "                     pd.Series(procrustes, name='procrustes',index=X_train.index),\n",
    "                     pd.Series(cityblock, name='cityblock',index=X_train.index),   \n",
    "                     pd.Series(euclidean, name='euclidean',index=X_train.index),  \n",
    "                     pd.Series(l1, name='l1',index=X_train.index), \n",
    "                     pd.Series(l2, name='l2',index=X_train.index), \n",
    "                     pd.Series(manhattan, name='manhattan',index=X_train.index), \n",
    "                     pd.Series(dice, name='dice',index=X_train.index), \n",
    "                     pd.Series(kulsinski, name='kulsinski',index=X_train.index), \n",
    "                     pd.Series(rogerstanimoto, name='rogerstanimoto',index=X_train.index), \n",
    "                     pd.Series(russellrao, name='russellrao',index=X_train.index), \n",
    "                     pd.Series(sokalmichener, name='sokalmichener',index=X_train.index),\n",
    "                     pd.Series(minkowski, name='minkowski',index=X_train.index),\n",
    "                     pd.Series(seuclidean, name='seuclidean',index=X_train.index), \n",
    "                     pd.Series(sokalsneath, name='sokalsneath',index=X_train.index),\n",
    "                     pd.Series(sqeuclidean, name='sqeuclidean',index=X_train.index)\n",
    "                    ], axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train.drop(columns=['curve_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[X_train.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_meta_test = [(q1_len, q2_len) for q1_len, q2_len in zip(get_q_lengths(X1_ft_test), get_q_lengths(X2_ft_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_300 = np.concatenate( \n",
    "    np.vstack( [np.array(np.vsplit(y, y.shape[0])) for y in (x for x in X_ft_test if x.size>0)] )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_300.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import mmwrite, mmread\n",
    "mmwrite( 'wor2vec_300_test.mtx', X_test_300 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.copy_file(dest_bucket=INPUT_BUCKET, file='wor2vec_300_test.mtx', source='wor2vec_300_test.mtx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del X_test_300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.get_file(bucket=INPUT_BUCKET, filename='embed_test.mtx', filepath='embed_test.mtx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import mmread\n",
    "X_rd_test = mmread('embed_test.mtx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rd_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild X1_rd_test and X2_rd_test\n",
    "X1_list = []\n",
    "X2_list = []\n",
    "q1_ptr = 0\n",
    "for len_q1, _ in q_meta_test:\n",
    "    q1 = np.array(X_rd_test[q1_ptr:q1_ptr+len_q1])\n",
    "    X1_list.append(q1)\n",
    "    q1_ptr = q1_ptr+len_q1\n",
    "q2_ptr = q1_ptr\n",
    "for _, len_q2 in q_meta_test:\n",
    "    q2 = np.array(X_rd_test[q2_ptr:q2_ptr+len_q2])\n",
    "    X2_list.append(q2)\n",
    "    q2_ptr = q2_ptr+len_q2\n",
    "X1_rd_test = np.array(X1_list)\n",
    "X2_rd_test = np.array(X2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_rd_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del X1_list, X2_list, q1_meta, q2_meta, X_rd_test, X1_rd_tmp, X2_rd_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard = []\n",
    "chebyshev = []\n",
    "braycurtis = []\n",
    "cosine = []\n",
    "correlation = []\n",
    "hamming = []\n",
    "canberra = []\n",
    "hausdorff = []\n",
    "cityblock = []\n",
    "euclidean = []\n",
    "l1 = []\n",
    "l2 = []\n",
    "manhattan = []\n",
    "dice = []\n",
    "kulsinski = []\n",
    "rogerstanimoto = []\n",
    "russellrao = []\n",
    "sokalmichener = []\n",
    "minkowski = []\n",
    "seuclidean = []\n",
    "sokalsneath = []\n",
    "sqeuclidean = []\n",
    "#fdtw = []\n",
    "dtw = []\n",
    "#pcm = []\n",
    "#area = []\n",
    "#curve_length = []\n",
    "discrete_frechet = []\n",
    "for q_tuple in zip(X1_rd_test, X2_rd_test):\n",
    "    if q_tuple:\n",
    "        q1_rd, q2_rd = q_tuple\n",
    "        jaccard.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'jaccard'))\n",
    "        chebyshev.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'chebyshev'))\n",
    "        braycurtis.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'braycurtis'))\n",
    "        cosine.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'cosine'))\n",
    "        correlation.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'correlation'))\n",
    "        hamming.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'hamming'))\n",
    "        canberra.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'canberra'))\n",
    "        hausdorff.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'hausdorff'))\n",
    "        cityblock.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'cityblock'))\n",
    "        euclidean.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'euclidean'))\n",
    "        l1.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'l1'))\n",
    "        l2.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'l2'))\n",
    "        manhattan.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'manhattan'))\n",
    "        dice.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'dice'))\n",
    "        kulsinski.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'kulsinski'))\n",
    "        rogerstanimoto.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'rogerstanimoto'))\n",
    "        russellrao.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'russellrao'))\n",
    "        sokalmichener.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'sokalmichener'))\n",
    "        minkowski.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'minkowski'))\n",
    "        seuclidean.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'seuclidean'))\n",
    "        sokalsneath.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'sokalsneath'))\n",
    "        sqeuclidean.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'sqeuclidean'))\n",
    "        #fdtw.append(delayed(compute_pairwise_metric)(q1_rd, q2_rd, 'fdtw'))\n",
    "        dtw.append(delayed(compute_pairwise_metric)(q1_rd, q2_rd, 'dtw'))\n",
    "        #pcm.append(delayed(compute_pairwise_metric)(q1_rd, q2_rd, 'pcm'))\n",
    "        #area.append(delayed(compute_pairwise_metric)(q1_rd, q2_rd, 'area'))\n",
    "        #curve_length.append(delayed(compute_pairwise_metric)(q1_rd, q2_rd, 'curve_length'))\n",
    "        discrete_frechet.append(delayed(compute_pairwise_metric)(q1_rd, q2_rd, 'discrete_frechet'))\n",
    "    else:\n",
    "        jaccard.append(delayed(np.nan))\n",
    "        chebyshev.append(delayed(np.nan))\n",
    "        braycurtis.append(delayed(np.nan))\n",
    "        cosine.append(delayed(np.nan))\n",
    "        correlation.append(delayed(np.nan))\n",
    "        hamming.append(delayed(np.nan))\n",
    "        canberra.append(delayed(np.nan))\n",
    "        hausdorff.append(delayed(np.nan))\n",
    "        cityblock.append(delayed(np.nan))\n",
    "        euclidean.append(delayed(np.nan))\n",
    "        l1.append(delayed(np.nan))\n",
    "        l2.append(delayed(np.nan))\n",
    "        manhattan.append(delayed(np.nan))\n",
    "        dice.append(delayed(np.nan))\n",
    "        kulsinski.append(delayed(np.nan))\n",
    "        rogerstanimoto.append(delayed(np.nan))\n",
    "        russellrao.append(delayed(np.nan))\n",
    "        sokalmichener.append(delayed(np.nan))\n",
    "        minkowski.append(delayed(np.nan))\n",
    "        seuclidean.append(delayed(np.nan))\n",
    "        sokalsneath.append(delayed(np.nan))\n",
    "        sqeuclidean.append(delayed(np.nan)) \n",
    "        #fdtw.append(delayed(np.nan)\n",
    "        dtw.append(delayed(np.nan))\n",
    "        #pcm.append(delayed(np.nan)\n",
    "        #area.append(delayed(np.nan) \n",
    "        #curve_length.append(delayed(np.nan))\n",
    "        discrete_frechet.append(delayed(np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard = compute(*jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chebyshev = compute(*chebyshev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "braycurtis = compute(*braycurtis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine = compute(*cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = compute(*correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamming = compute(*hamming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canberra = compute(*canberra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hausdorff = compute(*hausdorff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityblock = compute(*cityblock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean = compute(*euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = compute(*l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = compute(*l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan = compute(*manhattan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice = compute(*dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kulsinski = compute(*kulsinski)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rogerstanimoto = compute(*rogerstanimoto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "russellrao = compute(*russellrao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sokalmichener = compute(*sokalmichener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minkowski = compute(*minkowski)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seuclidean = compute(*seuclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sokalsneath = compute(*sokalsneath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqeuclidean = compute(*sqeuclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fdtw = compute(*fdtw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw = compute(*dtw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pcm = compute(*pcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#area = compute(*area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#curve_length = compute(*curve_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_frechet = compute(*discrete_frechet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.concat([X_test,\n",
    "                     pd.Series(jaccard, name='jaccard',index=X_test.index), \n",
    "                     pd.Series(chebyshev, name='chebyshev',index=X_test.index), \n",
    "                     pd.Series(braycurtis, name='braycurtis',index=X_test.index),\n",
    "                     pd.Series(cosine, name='cosine',index=X_test.index),\n",
    "                     pd.Series(correlation, name='correlation',index=X_test.index),  \n",
    "                     pd.Series(hamming, name='hamming',index=X_test.index), \n",
    "                     pd.Series(canberra, name='canberra',index=X_test.index),  \n",
    "                     pd.Series(hausdorff, name='hausdorff',index=X_test.index), \n",
    "                     #pd.Series((x for x,_,_ in fdtw), name='fdtw',index=X_test.index), \n",
    "                     pd.Series(dtw, name='dtw',index=X_test.index), \n",
    "                     #pd.Series((x for x,_,_ in pcm), name='pcm',index=X_test.index), \n",
    "                     #pd.Series((x for x,_,_ in area), name='area',index=X_test.index), \n",
    "                     #pd.Series(curve_length, name='curve_length',index=X_test.index), \n",
    "                     pd.Series(discrete_frechet, name='discrete_frechet',index=X_test.index),\n",
    "                     pd.Series(cityblock, name='cityblock',index=X_test.index),   \n",
    "                     pd.Series(euclidean, name='euclidean',index=X_test.index),  \n",
    "                     pd.Series(l1, name='l1',index=X_test.index), \n",
    "                     pd.Series(l2, name='l2',index=X_test.index), \n",
    "                     pd.Series(manhattan, name='manhattan',index=X_test.index), \n",
    "                     pd.Series(dice, name='dice',index=X_test.index), \n",
    "                     pd.Series(kulsinski, name='kulsinski',index=X_test.index), \n",
    "                     pd.Series(rogerstanimoto, name='rogerstanimoto',index=X_test.index), \n",
    "                     pd.Series(russellrao, name='russellrao',index=X_test.index), \n",
    "                     pd.Series(sokalmichener, name='sokalmichener',index=X_test.index),\n",
    "                     pd.Series(minkowski, name='minkowski',index=X_test.index),\n",
    "                     pd.Series(seuclidean, name='seuclidean',index=X_test.index), \n",
    "                     pd.Series(sokalsneath, name='sokalsneath',index=X_test.index),\n",
    "                     pd.Series(sqeuclidean, name='sqeuclidean',index=X_test.index)\n",
    "                    ], axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[X_test.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "pass_through = lambda x:x\n",
    "tfidf = TfidfVectorizer(analyzer=pass_through)\n",
    "X_trfmd = tfidf.fit_transform(get_tokens('train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trfmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension reduction using SVD\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import time\n",
    "start = time.time()\n",
    "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42)\n",
    "X_svd = svd.fit_transform(X_trfmd)\n",
    "end =  time.time()\n",
    "print('created SVD transform in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_svd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split back into two\n",
    "X1 = X_svd[:len(X_train), :]\n",
    "X2 = X_svd[len(X_train):, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_trfmd = tfidf.transform(get_tokens('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_trfmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension reduction using SVD\n",
    "start = time.time()\n",
    "X_test_svd = svd.transform(X_test_trfmd)\n",
    "end =  time.time()\n",
    "print('created SVD transform in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split back into two\n",
    "X1_test = X_test_svd[:len(X_test), :]\n",
    "X2_test = X_test_svd[len(X_test):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build complete feature dataframe\n",
    "X_test_temp = pd.concat([pd.DataFrame(X1_test, columns=['q1_'+str(i) for i in range(X1_test.shape[1])], index=X_test.index), \n",
    "                    pd.DataFrame(X2_test, columns=['q2_'+str(i) for i in range(X2_test.shape[1])], index=X_test.index)], axis=1)\n",
    "X_test_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy-wuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference in text size\n",
    "compute_size_diff = lambda row: abs(len(str(row['question1'])) - len(str(row['question2'])))\n",
    "X_train['size_diff'] = X_train.apply(compute_size_diff, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio\n",
    "compute_ratio = lambda row: fuzz.ratio(str(row['question1']), str(row['question2']))\n",
    "X_train['ratio'] = X_train.apply(compute_ratio, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial ratio\n",
    "compute_partial_ratio = lambda row: fuzz.partial_ratio(str(row['question1']), str(row['question2']))\n",
    "X_train['partial_ratio'] = X_train.apply(compute_partial_ratio, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_sort_ratio\n",
    "compute_token_sort_ratio = lambda row: fuzz.token_sort_ratio(str(row['question1']), str(row['question2']))\n",
    "X_train['token_sort_ratio'] = X_train.apply(compute_token_sort_ratio, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_set_ratio\n",
    "compute_token_set_ratio = lambda row: fuzz.token_set_ratio(str(row['question1']), str(row['question2']))\n",
    "X_train['token_set_ratio'] = X_train.apply(compute_token_set_ratio, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build complete feature dataframe\n",
    "#X_train_temp = pd.concat([pd.DataFrame(X1, columns=['q1_'+str(i) for i in range(X1.shape[1])], index=X_train.index), \n",
    "#                     pd.DataFrame(X2, columns=['q2_'+str(i) for i in range(X2.shape[1])], index=X_train.index)], axis=1)\n",
    "#X_train_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = pd.concat([X_train_temp, X_train], axis=1)\n",
    "#del X_train_temp\n",
    "X_train_final = X_train.drop(columns=['qid1', 'qid2','question1','question2']).dropna()\n",
    "X_train_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference in text size\n",
    "X_test['size_diff'] = X_test.apply(compute_size_diff, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio\n",
    "X_test['ratio'] = X_test.apply(compute_ratio, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial ratio\n",
    "X_test['partial_ratio'] = X_test.apply(compute_partial_ratio, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_sort_ratio\n",
    "X_test['token_sort_ratio'] = X_test.apply(compute_token_sort_ratio, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_set_ratio\n",
    "X_test['token_set_ratio'] = X_test.apply(compute_token_set_ratio, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final = X_test.drop(columns=['question1','question2', 'qid1', 'qid2']).dropna()\n",
    "X_test_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "logr_model = LogisticRegression(random_state=42)\n",
    "param_grid = {'C': np.logspace(-2, 7, 10),\n",
    "             #'penalty': ['l1','l2'],\n",
    "             'tol': np.logspace(-5, -1, 5),\n",
    "             #'solver': ['lbfgs']\n",
    "             #'max_iter': np.linspace(10, 1000, 10)\n",
    "             }\n",
    "logr_cv = RandomizedSearchCV(logr_model, param_distributions=param_grid, cv=5, n_jobs=-1)\n",
    "y_train_final = y_train.loc[X_train_final.index]\n",
    "logr_cv.fit(X_train_final, y_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_model = LogisticRegression(#solver=logr_cv.best_params_['solver'], \n",
    "                                random_state=42, \n",
    "                                C=logr_cv.best_params_['C'], \n",
    "                                tol=logr_cv.best_params_['tol'], \n",
    "                                #max_iter=logr_cv.best_params_['max_iter'], \n",
    "                                n_jobs=-1)\n",
    "logr_model.fit(X_train_final, y_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_pred = logr_model.predict(X_test_final)\n",
    "y_test_final = y_test.loc[X_test_final.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "logr_acc_score = accuracy_score(y_test_final, logr_pred)\n",
    "logr_prec_score = precision_score(y_test_final, logr_pred)\n",
    "logr_rec_score = recall_score(y_test_final, logr_pred)\n",
    "print('Logistic Regression')\n",
    "print('accuracy score : {}'.format(logr_acc_score))\n",
    "print('precision score : {}'.format(logr_prec_score))\n",
    "print('recall score : {}'.format(logr_rec_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XQBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# Model selection\n",
    "params_xgb = {'n_estimators' : [1, 2, 4, 8, 16, 32, 64, 100, 200],\n",
    "               'gamma':np.linspace(.01, 1, 10, endpoint=True), \n",
    "               'learning_rate' : np.linspace(.01, 1, 10, endpoint=True),\n",
    "               'reg_lambda': np.linspace(0.01, 10, 20, endpoint=True),\n",
    "               'max_depth' : np.linspace(1, 32, 32, endpoint=True, dtype=int)\n",
    "                 }\n",
    "cv_xgb = RandomizedSearchCV(xgb.XGBClassifier(objective='binary:logistic', random_state=42), param_distributions=params_xgb, cv=5, n_jobs=3, random_state=42)\n",
    "cv_xgb.fit(X_train_final, y_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_xgb_model = xgb.XGBClassifier(random_state=42,\n",
    "                                  n_estimators=cv_xgb.best_params_['n_estimators'],\n",
    "                                  gamma=cv_xgb.best_params_['gamma'],\n",
    "                                  learning_rate=cv_xgb.best_params_['learning_rate'],\n",
    "                                  reg_lambda=cv_xgb.best_params_['reg_lambda'],\n",
    "                                  max_depth=cv_xgb.best_params_['max_depth'])\n",
    "clf_xgb_model.fit(X_train_final, y_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb = clf_xgb_model.predict(X_test_final)\n",
    "score_xgb = accuracy_score(y_test_final, y_pred_xgb)\n",
    "rscore_xgb = recall_score(y_test_final, y_pred_xgb)\n",
    "pscore_xgb = precision_score(y_test_final, y_pred_xgb)\n",
    "print('Accuracy score for XGBoost ', score_xgb)\n",
    "print('Recall score for XGBoost ', rscore_xgb)\n",
    "print('Precision score for XGBoost ', pscore_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
