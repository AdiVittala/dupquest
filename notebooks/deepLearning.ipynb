{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification of Quora Duplicates using Embed, Encode, Attend & Predict\n",
    "(based on methods described by <a href='https://explosion.ai/blog/deep-learning-formula-nlp#entailment'>Matthew Honnibal</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc'></a>\n",
    "## Table of Contents\n",
    "1. [Imports & Intialization](#init)\n",
    "2. [Load Data (previously split into train & test)](#data_load)\n",
    "3. [Embedding - GloVe](#embed)\n",
    "    1. [Load pretrained GloVe model](#load_glove)\n",
    "    2. [Function - Embed with GloVe](#f_glove)\n",
    "    3. [Function - Embedding Layer for NN](#f_emb)\n",
    "    4. [Convert train data (text to GloVe IDs)](#c_gl_train)\n",
    "    5. [Convert test data (text to GloVe IDs)](#c_gl_test) \n",
    "4. [Modeling](#model)\n",
    "    1. [Decomposable Attention Model](#da)\n",
    "        1. [Function - Create feed-forward layer](#f_ff)\n",
    "        2. [Function - Normalize attention weights](#f_norm_att)\n",
    "        3. [Function - Sum a vector](#f_sum)\n",
    "        4. [Function - Slice input (return one slice at a time)](#f_slice)\n",
    "        5. [Function - Build decomposable attention model](#f_bld_da)\n",
    "        6. [Hyperparameter optimization](#da_hyp)\n",
    "            1. [Shallow optimization run](#da_hyp_run1)\n",
    "            2. [Deep optimization run](#da_hyp_run2)\n",
    "        7. [Train optimized model](#da_train)\n",
    "        8. [Predict test data](#da_pred)\n",
    "        9. [Evaluate Model](#da_eval)\n",
    "    2. [Hierarchical Attention Model](#ha)\n",
    "        1. [Class - Attention_Layer](#c_att_ha)\n",
    "        5. [Function - Build decomposable attention model](#f_bld_ha)\n",
    "        6. [Hyperparameter optimization](#ha_hyp)\n",
    "            1. [Shallow optimization run](#ha_hyp_run1)\n",
    "            2. [Deep optimization run](#ha_hyp_run2)\n",
    "        7. [Train optimized model](#ha_train)\n",
    "        8. [Predict test data](#ha_pred)\n",
    "        9. [Evaluate Model](#ha_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='init'></a>\n",
    "## Imports & Intialization\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "\n",
    "from keras import layers, Model, models, initializers\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam,RMSprop, Nadam, SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.utils.dl_utils import init_session, data, plot_optimization_history\n",
    "from src.utils.plotting import plot_roc\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, loguniform, lognormal, normal\n",
    "import gc\n",
    "\n",
    "# change these to the appropriate data folder\n",
    "f_path = '/media/siri/78C6823EC681FD1E/minio/data/'\n",
    "data_folder = f_path+'dq-data/dl/'\n",
    "input_folder = f_path+'dq-data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_load'></a>\n",
    "## Load Data (previously split into train & test)\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run the following cell if you have already split the data into test and train sets and saved the split data (i.e. you have already run the notebook logR_GBM.ipynb in the same location as this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pickle.load(open(input_folder+'X_train.p', 'rb'))\n",
    "X_test = pickle.load(open(input_folder+'X_test.p', 'rb'))\n",
    "y_train = pickle.load(open(input_folder+'y_train.p', 'rb'))\n",
    "y_test = pickle.load(open(input_folder+'y_test.p', 'rb'))\n",
    "\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_load'></a>\n",
    "## Load Data (if previously not split into train & test)\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### uncomment and run the following cell to load data if you have not split the data into test and train sets and saved it already (see notebook logR_GBM.ipynb in the same location as this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data = 'train.csv'\n",
    "# filestream = input_folder+data\n",
    "# dtypes = {\n",
    "#     'id': 'int64',\n",
    "#     'qid1': 'int64',\n",
    "#     'qid2': 'int64',\n",
    "#     'question1': 'object',\n",
    "#     'question2': 'object',\n",
    "#     'is_duplicate': 'int64'\n",
    "# }\n",
    "# df = pd.read_csv(\n",
    "#                                      filestream,\n",
    "#                                      header=0, \n",
    "#                                      usecols=dtypes.keys(), \n",
    "#                                      #names=dtypes.keys(),\n",
    "#                                      skipinitialspace=True,\n",
    "#                                      skip_blank_lines=True,\n",
    "#                                      encoding='utf-8')\n",
    "# df = df.set_index('id')\n",
    "# df = df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "# X = df.drop(columns=['is_duplicate'])\n",
    "\n",
    "# y = df['is_duplicate']\n",
    "# X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='embed'></a>\n",
    "## Embedding\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load_glove'></a>\n",
    "#### Load pretrained GloVe model\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_vectors_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='f_glove'></a>\n",
    "#### Function - Embed with GloVe \n",
    "[back to table of contents](#toc)\n",
    "\n",
    "Used to convert the train and test datasets (text to IDs) based on the glove vectors. \n",
    "Accounts for OOV tokens by adding a set of OOV vectors and assigning them randomly to OOV tokens. \n",
    "This function has been taken from <a href='https://github.com/explosion/spaCy/tree/master/examples/keras_parikh_entailment'>A decomposable attention model for Natural Language Inference</a> and adapted to this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(nlp, X, num_oov, max_length, norm_vectors = True):\n",
    "    len_q1 = X['question1'].size\n",
    "    sents = pd.concat([X['question1'], X['question2']]).values\n",
    "    \n",
    "    # the extra +1 is for a zero vector represting NULL for padding\n",
    "    num_vectors = max(lex.rank for lex in nlp.vocab) + 2 \n",
    "    \n",
    "    # create random vectors for OOV tokens\n",
    "    oov = np.random.normal(size=(num_oov, nlp.vocab.vectors_length))\n",
    "    oov = oov / oov.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    vectors = np.zeros((num_vectors + num_oov, nlp.vocab.vectors_length), dtype='float32')\n",
    "    vectors[num_vectors:, ] = oov\n",
    "    for lex in nlp.vocab:\n",
    "        if lex.has_vector and lex.vector_norm > 0:\n",
    "            vectors[lex.rank + 1] = lex.vector / lex.vector_norm if norm_vectors == True else lex.vector\n",
    "            \n",
    "    sents_as_ids = []\n",
    "    for sent in sents:\n",
    "        doc = nlp(sent)\n",
    "        word_ids = []\n",
    "        \n",
    "        for i, token in enumerate(doc):\n",
    "            # skip odd spaces from tokenizer\n",
    "            if token.has_vector and token.vector_norm == 0:\n",
    "                continue\n",
    "                \n",
    "            if i > max_length:\n",
    "                break\n",
    "                \n",
    "            if token.has_vector:\n",
    "                word_ids.append(token.rank + 1)\n",
    "            else:\n",
    "                # if we don't have a vector, pick an OOV entry\n",
    "                word_ids.append(token.rank % num_oov + num_vectors) \n",
    "                \n",
    "        # there must be a simpler way of generating padded arrays from lists...\n",
    "        word_id_vec = np.zeros((max_length), dtype='int')\n",
    "        clipped_len = min(max_length, len(word_ids))\n",
    "        word_id_vec[:clipped_len] = word_ids[:clipped_len]\n",
    "        sents_as_ids.append(word_id_vec)        \n",
    "        \n",
    "    return vectors, np.array(sents_as_ids[:len_q1]), np.array(sents_as_ids[len_q1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='f_emb'></a>\n",
    "#### Function - Embedding Layer for NN\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(vectors, max_length, projected_dim):\n",
    "    return models.Sequential([\n",
    "        layers.Embedding(\n",
    "            vectors.shape[0],\n",
    "            vectors.shape[1],\n",
    "            input_length=max_length,\n",
    "            weights=[vectors],\n",
    "            trainable=False),\n",
    "        \n",
    "        layers.TimeDistributed(\n",
    "            layers.Dense(projected_dim,\n",
    "                         activation=None,\n",
    "                         use_bias=False))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='c_gl_train'></a>\n",
    "#### Convert train data (text to GloVe IDs)\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v, q1_train_w2v, q2_train_w2v = create_dataset(nlp, X_train, 100, 50, True)\n",
    "\n",
    "pickle.dump(w2v, open(data_folder+'w2v.p', 'wb'))\n",
    "pickle.dump(q1_train_w2v, open(data_folder+'q1_train_w2v.p', 'wb'))\n",
    "pickle.dump(q2_train_w2v, open(data_folder+'q2_train_w2v.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='c_gl_test'></a>\n",
    "#### Convert test data (text to GloVe IDs)\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, q1_test_w2v, q2_test_w2v = create_dataset(nlp, X_test, 100, 50, True)\n",
    "\n",
    "\n",
    "pickle.dump(q1_test_w2v, open(data_folder+'q1_test_w2v.p', 'wb'))\n",
    "pickle.dump(q2_test_w2v, open(data_folder+'q2_test_w2v.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model'></a>\n",
    "## Modeling\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load previously pickled word vectors and train and test token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = pickle.load(open(data_folder+'w2v.p', 'rb'))\n",
    "q1_train_w2v = pickle.load(open(data_folder+'q1_train_w2v.p', 'rb'))\n",
    "q2_train_w2v = pickle.load(open(data_folder+'q2_train_w2v.p', 'rb'))\n",
    "q1_test_w2v = pickle.load(open(data_folder+'q1_test_w2v.p', 'rb'))\n",
    "q2_test_w2v = pickle.load(open(data_folder+'q2_test_w2v.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='da'></a>\n",
    "### Decomposable Attention Model\n",
    "(based on Example 1. <a href='https://explosion.ai/blog/deep-learning-formula-nlp'>A Decomposable Attention Model for Natural Language Inference</a>)\n",
    "\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='f_ff'></a>\n",
    "#### Function - Create feed-forward layer\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feedforward(num_units, dropout_rate, activation='relu'):\n",
    "    return models.Sequential([\n",
    "        layers.Dense(num_units, activation=activation, use_bias=True),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(num_units, activation=activation, use_bias=True),\n",
    "        layers.Dropout(dropout_rate)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='f_norm_att'></a>\n",
    "#### Function - Normalize attention weights\n",
    "(as described in 3.1 of <a href='https://arxiv.org/pdf/1606.01933v1.pdf'>Parikh et al.</a>)\n",
    "\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(axis):\n",
    "    def _normalize(att_weights):\n",
    "        exp_weights = K.exp(att_weights)\n",
    "        sum_weights = K.sum(exp_weights, axis=axis, keepdims=True)\n",
    "        return exp_weights/sum_weights\n",
    "    return _normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='f_sum'></a>\n",
    "#### Function - Sum a vector\n",
    "(as described in 3.3 of <a href='https://arxiv.org/pdf/1606.01933v1.pdf'>Parikh et al.</a>)\n",
    "\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_word(x):\n",
    "    return K.sum(x, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='f_slice'></a>\n",
    "#### Function - Slice input (return one slice at a time)\n",
    "to take single input instead of two in order to work with hyperas specs\n",
    "\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_slice(i):\n",
    "    def _slice(inputs):\n",
    "        slice_i = K.squeeze(K.slice(inputs, [0,i-1,0], [-1,1,-1]),1)\n",
    "        return slice_i\n",
    "    return _slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='f_bld_da'></a>\n",
    "#### Function - Build decomposable attention model\n",
    "(as described in 3.1 Attend, 3.2 Compare and 3.3 Aggregate of <a href='https://arxiv.org/pdf/1606.01933v1.pdf'>Parikh et al.</a>)\n",
    "\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vectors, max_length, num_classes, projected_dim,  \n",
    "                num_hidden=200, dropout_rate1=0.2, dropout_rate2=0.2, \n",
    "                dropout_rate3=0.2, learn_rate=0.0001, optimizer='nadam'):\n",
    "    K.clear_session()\n",
    "    #input1 = layers.Input(shape=(max_length,), dtype='int32', name='words1')\n",
    "    #input2 = layers.Input(shape=(max_length,), dtype='int32', name='words2')\n",
    "    \n",
    "    # modified model to take single input instead of two in order to work with hyperas specs.\n",
    "    model_input = layers.Input(shape=(2, max_length), dtype='int32')\n",
    "    input1 = layers.Lambda(get_input_slice(1))(model_input)\n",
    "    input2 = layers.Lambda(get_input_slice(2))(model_input)\n",
    "    \n",
    "    # embeddings (projected)\n",
    "    embed = create_embedding(vectors, max_length, projected_dim)\n",
    "    a = embed(input1)\n",
    "    b = embed(input2)     \n",
    "    \n",
    "    # step 1: attend\n",
    "    F = create_feedforward(num_hidden, dropout_rate=dropout_rate1)\n",
    "    att_weights = layers.dot([F(a), F(b)], axes=-1, normalize=True)\n",
    "    \n",
    "    G = create_feedforward(num_hidden, dropout_rate=dropout_rate2)    \n",
    "    \n",
    "    norm_weights_a = layers.Lambda(normalizer(1))(att_weights)\n",
    "    norm_weights_b = layers.Lambda(normalizer(2))(att_weights)\n",
    "    alpha = layers.dot([norm_weights_a, a], axes=1)\n",
    "    beta  = layers.dot([norm_weights_b, b], axes=1)\n",
    "\n",
    "    # step 2: compare\n",
    "    comp1 = layers.concatenate([a, beta])\n",
    "    comp2 = layers.concatenate([b, alpha])\n",
    "    v1 = layers.TimeDistributed(G)(comp1)\n",
    "    v2 = layers.TimeDistributed(G)(comp2)\n",
    "\n",
    "    # step 3: aggregate\n",
    "    v1_sum = layers.Lambda(sum_word)(v1)\n",
    "    v2_sum = layers.Lambda(sum_word)(v2)\n",
    "    concat = layers.concatenate([v1_sum, v2_sum])\n",
    "        \n",
    "    H = create_feedforward(num_hidden, dropout_rate=dropout_rate3)\n",
    "    out = H(concat)\n",
    "    out = layers.Dense(num_classes, activation='sigmoid', use_bias=True)(out)\n",
    "    \n",
    "    model = Model(model_input, out)\n",
    "    if optimizer == 'sgd':\n",
    "        opt = SGD(lr=learn_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        opt = Adam(lr=learn_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(lr=learn_rate)\n",
    "    else:\n",
    "        opt = Nadam(lr=learn_rate)\n",
    "    \n",
    "    model.compile(optimizer=opt,\n",
    "    #model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='da_hyp'></a>\n",
    "#### Hyperparameter optimization\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions=[build_model, get_input_slice, create_embedding, create_feedforward, normalizer, sum_word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='da_hyp_run1'></a>\n",
    "##### Shallow optimization run\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function called by hyperas to build model for every evaluation (with different parameter combinations)\n",
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    K.clear_session()\n",
    "    # this is needed to free up gpu memory after every evaluation\n",
    "    gc.collect()\n",
    "    w2v = pickle.load(open(data_folder+'w2v.p', 'rb'))\n",
    "    model = build_model(vectors=w2v, max_length=50, projected_dim=200, num_classes=1, \n",
    "                        num_hidden=200, dropout_rate1={{uniform(0, 1)}}, \n",
    "                        dropout_rate2={{uniform(0, 1)}}, dropout_rate3={{uniform(0, 1)}},\n",
    "                        optimizer={{choice(['sgd', 'rmsprop','nadam', 'adam'])}},\n",
    "                        learn_rate={{loguniform(-6,-3)}})\n",
    "    result = model.fit(x_train, y_train, batch_size= {{choice([32, 64, 128])}}, epochs=2,\n",
    "      validation_split=0.2)\n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation accuracy of eval run:', validation_acc)\n",
    "    print('history of eval run:', result.history)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK,\n",
    "            'train_acc': np.amax(result.history['acc'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import spacy\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pickle\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import spacy\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pickle\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import layers, Model, models, initializers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.engine.topology import Layer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam, RMSprop, Nadam, SGD\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.wrappers.scikit_learn import KerasClassifier\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import accuracy_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import precision_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import recall_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import classification_report\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.backend.tensorflow_backend import set_session\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, loguniform, lognormal, normal\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import gc\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Layer, RNN\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'dropout_rate1': hp.uniform('dropout_rate1', 0, 1),\n",
      "        'dropout_rate1_1': hp.uniform('dropout_rate1_1', 0, 1),\n",
      "        'dropout_rate1_2': hp.uniform('dropout_rate1_2', 0, 1),\n",
      "        'optimizer': hp.choice('optimizer', ['sgd', 'rmsprop','nadam', 'adam']),\n",
      "        'learn_rate': hp.loguniform('learn_rate', -6,-3),\n",
      "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
      "    }\n",
      "\n",
      ">>> Functions\n",
      "   1: def build_model(vectors, max_length, num_classes, projected_dim,  \n",
      "   2:                 num_hidden=200, dropout_rate1=0.2, dropout_rate2=0.2, \n",
      "   3:                 dropout_rate3=0.2, learn_rate=0.0001, optimizer='nadam'):\n",
      "   4:     K.clear_session()\n",
      "   5:     #input1 = layers.Input(shape=(max_length,), dtype='int32', name='words1')\n",
      "   6:     #input2 = layers.Input(shape=(max_length,), dtype='int32', name='words2')\n",
      "   7:     \n",
      "   8:     # modified model to take single input instead of two in order to work with hyperas specs.\n",
      "   9:     model_input = layers.Input(shape=(2, max_length), dtype='int32')\n",
      "  10:     input1 = layers.Lambda(get_input_slice(1))(model_input)\n",
      "  11:     input2 = layers.Lambda(get_input_slice(2))(model_input)\n",
      "  12:     \n",
      "  13:     # embeddings (projected)\n",
      "  14:     embed = create_embedding(vectors, max_length, projected_dim)\n",
      "  15:     a = embed(input1)\n",
      "  16:     b = embed(input2)     \n",
      "  17:     \n",
      "  18:     # step 1: attend\n",
      "  19:     F = create_feedforward(num_hidden, dropout_rate=dropout_rate1)\n",
      "  20:     att_weights = layers.dot([F(a), F(b)], axes=-1, normalize=True)\n",
      "  21:     \n",
      "  22:     G = create_feedforward(num_hidden, dropout_rate=dropout_rate2)    \n",
      "  23:     \n",
      "  24:     norm_weights_a = layers.Lambda(normalizer(1))(att_weights)\n",
      "  25:     norm_weights_b = layers.Lambda(normalizer(2))(att_weights)\n",
      "  26:     alpha = layers.dot([norm_weights_a, a], axes=1)\n",
      "  27:     beta  = layers.dot([norm_weights_b, b], axes=1)\n",
      "  28: \n",
      "  29:     # step 2: compare\n",
      "  30:     comp1 = layers.concatenate([a, beta])\n",
      "  31:     comp2 = layers.concatenate([b, alpha])\n",
      "  32:     v1 = layers.TimeDistributed(G)(comp1)\n",
      "  33:     v2 = layers.TimeDistributed(G)(comp2)\n",
      "  34: \n",
      "  35:     # step 3: aggregate\n",
      "  36:     v1_sum = layers.Lambda(sum_word)(v1)\n",
      "  37:     v2_sum = layers.Lambda(sum_word)(v2)\n",
      "  38:     concat = layers.concatenate([v1_sum, v2_sum])\n",
      "  39:         \n",
      "  40:     H = create_feedforward(num_hidden, dropout_rate=dropout_rate3)\n",
      "  41:     out = H(concat)\n",
      "  42:     out = layers.Dense(num_classes, activation='sigmoid', use_bias=True)(out)\n",
      "  43:     \n",
      "  44:     model = Model(model_input, out)\n",
      "  45:     if optimizer == 'sgd':\n",
      "  46:         opt = SGD(lr=learn_rate)\n",
      "  47:     elif optimizer == 'adam':\n",
      "  48:         opt = Adam(lr=learn_rate)\n",
      "  49:     elif optimizer == 'rmsprop':\n",
      "  50:         opt = RMSprop(lr=learn_rate)\n",
      "  51:     else:\n",
      "  52:         opt = Nadam(lr=learn_rate)\n",
      "  53:     \n",
      "  54:     model.compile(optimizer=opt,\n",
      "  55:     #model.compile(optimizer=optimizer,\n",
      "  56:                   loss='binary_crossentropy',\n",
      "  57:                   metrics=['accuracy'])\n",
      "  58:     return model\n",
      "  59: \n",
      "  60: def get_input_slice(i):\n",
      "  61:     def _slice(inputs):\n",
      "  62:         slice_i = K.squeeze(K.slice(inputs, [0,i-1,0], [-1,1,-1]),1)\n",
      "  63:         return slice_i\n",
      "  64:     return _slice\n",
      "  65: \n",
      "  66: def create_embedding(vectors, max_length, projected_dim):\n",
      "  67:     return models.Sequential([\n",
      "  68:         layers.Embedding(\n",
      "  69:             vectors.shape[0],\n",
      "  70:             vectors.shape[1],\n",
      "  71:             input_length=max_length,\n",
      "  72:             weights=[vectors],\n",
      "  73:             trainable=False),\n",
      "  74:         \n",
      "  75:         layers.TimeDistributed(\n",
      "  76:             layers.Dense(projected_dim,\n",
      "  77:                          activation=None,\n",
      "  78:                          use_bias=False))\n",
      "  79:     ])\n",
      "  80: \n",
      "  81: def create_feedforward(num_units, dropout_rate, activation='relu'):\n",
      "  82:     return models.Sequential([\n",
      "  83:         layers.Dense(num_units, activation=activation, use_bias=True),\n",
      "  84:         layers.Dropout(dropout_rate),\n",
      "  85:         layers.Dense(num_units, activation=activation, use_bias=True),\n",
      "  86:         layers.Dropout(dropout_rate)\n",
      "  87:     ])\n",
      "  88: \n",
      "  89: def normalizer(axis):\n",
      "  90:     def _normalize(att_weights):\n",
      "  91:         exp_weights = K.exp(att_weights)\n",
      "  92:         sum_weights = K.sum(exp_weights, axis=axis, keepdims=True)\n",
      "  93:         return exp_weights/sum_weights\n",
      "  94:     return _normalize\n",
      "  95: \n",
      "  96: def sum_word(x):\n",
      "  97:     return K.sum(x, axis=1)\n",
      "  98: \n",
      "  99: \n",
      ">>> Data\n",
      "  1: \n",
      "  2: data_folder = '/media/siri/78C6823EC681FD1E/minio/data/dq-data/dl/'\n",
      "  3: input_folder = '/media/siri/78C6823EC681FD1E/minio/data/dq-data/'\n",
      "  4: q1_train_w2v = pickle.load(open(data_folder+'q1_train_w2v.p', 'rb'))\n",
      "  5: q2_train_w2v = pickle.load(open(data_folder+'q2_train_w2v.p', 'rb'))\n",
      "  6: q1_test_w2v = pickle.load(open(data_folder+'q1_test_w2v.p', 'rb'))\n",
      "  7: q2_test_w2v = pickle.load(open(data_folder+'q2_test_w2v.p', 'rb'))\n",
      "  8: x_train = np.concatenate([np.expand_dims(q1_train_w2v, axis=1),np.expand_dims(q2_train_w2v, axis=1)], axis=1)\n",
      "  9: x_test = np.concatenate([np.expand_dims(q1_test_w2v, axis=1),np.expand_dims(q2_test_w2v, axis=1)], axis=1)\n",
      " 10: y_train = pickle.load(open(input_folder+'y_train.p', 'rb'))\n",
      " 11: y_test = pickle.load(open(input_folder+'y_test.p', 'rb'))\n",
      " 12: \n",
      " 13: \n",
      " 14: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "  1: def keras_fmin_fnct(space):\n",
      "  2: \n",
      "  3:     K.clear_session()\n",
      "  4:     # this is needed to free up gpu memory after every evaluation\n",
      "  5:     gc.collect()\n",
      "  6:     w2v = pickle.load(open(data_folder+'w2v.p', 'rb'))\n",
      "  7:     model = build_model(vectors=w2v, max_length=50, projected_dim=200, num_classes=1, \n",
      "  8:                         num_hidden=200, dropout_rate1=space['dropout_rate1'], \n",
      "  9:                         dropout_rate2=space['dropout_rate1_1'], dropout_rate3=space['dropout_rate1_2'],\n",
      " 10:                         optimizer=space['optimizer'],\n",
      " 11:                         learn_rate=space['learn_rate'])\n",
      " 12:     result = model.fit(x_train, y_train, batch_size= space['batch_size'], epochs=2,\n",
      " 13:       validation_split=0.2)\n",
      " 14:     validation_acc = np.amax(result.history['val_acc']) \n",
      " 15:     print('Best validation accuracy of eval run:', validation_acc)\n",
      " 16:     print('history of eval run:', result.history)\n",
      " 17:     return {'loss': -validation_acc, 'status': STATUS_OK,\n",
      " 18:             'train_acc': np.amax(result.history['acc'])}\n",
      " 19: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 32s 149us/step - loss: 0.6281 - acc: 0.6552 - val_loss: 0.6440 - val_acc: 0.6361\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 31s 145us/step - loss: 0.6194 - acc: 0.6573 - val_loss: 0.6194 - val_acc: 0.6377\n",
      "Best validation accuracy of eval run: 0.6376742039565177\n",
      "history of eval run: {'val_loss': [0.6440065463820415, 0.6194075072620341], 'val_acc': [0.6360682971916498, 0.6376742039565177], 'loss': [0.6280545650397935, 0.619427458560793], 'acc': [0.6551775059294046, 0.6573095151329961]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 29s 136us/step - loss: 0.6351 - acc: 0.6280 - val_loss: 0.6542 - val_acc: 0.5889\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 29s 134us/step - loss: 0.6219 - acc: 0.6381 - val_loss: 0.6497 - val_acc: 0.5857\n",
      "Best validation accuracy of eval run: 0.5889063220932894\n",
      "history of eval run: {'val_loss': [0.65418195384298, 0.6497355529576554], 'val_acc': [0.5889063220932894, 0.5857498846232313], 'loss': [0.6350946084134628, 0.6218953041122482], 'acc': [0.6279736221494857, 0.6381398911902412]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 38s 176us/step - loss: 5.9530 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 37s 173us/step - loss: 5.9543 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597], 'loss': [5.9530348429171145, 5.954312881339687], 'acc': [0.6305717199663573, 0.6305809494415359]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 55s 252us/step - loss: 5.9527 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 54s 250us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290320195303, 5.973290320195303], 'val_acc': [0.6294047069725597, 0.6294047069725597], 'loss': [5.952682972079773, 5.953290202740575], 'acc': [0.6306040231278318, 0.6306455557768627]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 32s 146us/step - loss: 5.9501 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 31s 142us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290325124307, 5.973290325124307], 'val_acc': [0.6294047069725597, 0.6294047069725597], 'loss': [5.95014492382799, 5.953290207134931], 'acc': [0.6305532610074733, 0.6306455557925411]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 58s 268us/step - loss: 0.6501 - acc: 0.6384 - val_loss: 0.6233 - val_acc: 0.6761\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 57s 265us/step - loss: 0.6473 - acc: 0.6452 - val_loss: 0.6265 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6761236732843343\n",
      "history of eval run: {'val_loss': [0.6232665803281734, 0.6265085749170667], 'val_acc': [0.6761236732843343, 0.6294047069725597], 'loss': [0.6501060398119957, 0.6473216407858414], 'acc': [0.6384029312802778, 0.6451542937862363]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 40s 184us/step - loss: 5.9515 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 39s 180us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597], 'loss': [5.951508370581336, 5.9532901992770215], 'acc': [0.6305994083918929, 0.6306455557771378]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 58s 269us/step - loss: 5.9527 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 57s 265us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290320195303, 5.973290320195303], 'val_acc': [0.6294047069725597, 0.6294047069725597], 'loss': [5.95272109290414, 5.953290202324684], 'acc': [0.6306224820816272, 0.6306455557763125]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 30s 136us/step - loss: 0.6549 - acc: 0.6149 - val_loss: 0.6469 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 29s 134us/step - loss: 0.6364 - acc: 0.6297 - val_loss: 0.6417 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [0.6468740139130793, 0.641729888579347], 'val_acc': [0.6294047069725597, 0.6294047069725597], 'loss': [0.6548722050600109, 0.6363744216044445], 'acc': [0.6148954531124078, 0.6297087638343056]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 31s 141us/step - loss: 5.9505 - acc: 0.6305 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 30s 139us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290325124307, 5.973290325124307], 'val_acc': [0.6294047069725597, 0.6294047069725597], 'loss': [5.950464069155368, 5.953290213320477], 'acc': [0.6305024988663477, 0.6306455557796132]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 32s 146us/step - loss: 5.9495 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 31s 142us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290325124307, 5.973290325124307], 'val_acc': [0.6294047069725597, 0.6294047069725597], 'loss': [5.949468942084778, 5.953290218091115], 'acc': [0.6305994083984944, 0.6306455557914409]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 38s 176us/step - loss: 0.5573 - acc: 0.7166 - val_loss: 0.5240 - val_acc: 0.7390\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 38s 173us/step - loss: 0.5547 - acc: 0.7251 - val_loss: 0.5526 - val_acc: 0.6894\n",
      "Best validation accuracy of eval run: 0.7389755422088701\n",
      "history of eval run: {'val_loss': [0.524004360563919, 0.5526221143511731], 'val_acc': [0.7389755422088701, 0.6894139363207903], 'loss': [0.5573434329664105, 0.5547093071375867], 'acc': [0.7165904465635736, 0.7251277128850253]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 55s 253us/step - loss: 0.7654 - acc: 0.6273 - val_loss: 0.9451 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 54s 249us/step - loss: 5.9767 - acc: 0.6023 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [0.9450919113293407, 5.973290320195303], 'val_acc': [0.6293677895752362, 0.6294047069725597], 'loss': [0.7654151831238103, 5.976718660172142], 'acc': [0.6272814113704009, 0.6023479789785519]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 47s 218us/step - loss: 0.6342 - acc: 0.6296 - val_loss: 0.6121 - val_acc: 0.6382\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 47s 215us/step - loss: 0.6127 - acc: 0.6500 - val_loss: 0.6091 - val_acc: 0.6675\n",
      "Best validation accuracy of eval run: 0.66754037839122\n",
      "history of eval run: {'val_loss': [0.6120584409359033, 0.6090741649122903], 'val_acc': [0.6381725888368882, 0.66754037839122], 'loss': [0.6342182504626693, 0.6126922972921957], 'acc': [0.6295564774766682, 0.6499720808306603]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 31s 141us/step - loss: 5.9504 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 30s 139us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290325124307, 5.973290325124307], 'val_acc': [0.6294047069725597, 0.6294047069725597], 'loss': [5.950401906918269, 5.953290201411486], 'acc': [0.6306132525904952, 0.6306455557688859]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 31s 142us/step - loss: 5.9501 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 30s 139us/step - loss: 5.9534 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290325124307, 5.973290325124307], 'val_acc': [0.6294047069725597, 0.6294047069725597], 'loss': [5.950079590960573, 5.953363763240099], 'acc': [0.6305578757578528, 0.6306409410281335]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 51s 237us/step - loss: 0.6620 - acc: 0.6555 - val_loss: 0.6310 - val_acc: 0.6785\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 51s 234us/step - loss: 0.6861 - acc: 0.6516 - val_loss: 0.6787 - val_acc: 0.6617\n",
      "Best validation accuracy of eval run: 0.6784863867130376\n",
      "history of eval run: {'val_loss': [0.6309708462313787, 0.6786530601906787], 'val_acc': [0.6784863867130376, 0.6617443470114324], 'loss': [0.6619587704402663, 0.6861143484800698], 'acc': [0.6555005376140156, 0.6516103130155155]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 32s 146us/step - loss: 0.6517 - acc: 0.6275 - val_loss: 0.6539 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 31s 142us/step - loss: 0.6614 - acc: 0.6299 - val_loss: 0.6594 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [0.6538863394268108, 0.659369426128993], 'val_acc': [0.6294047069725597, 0.6294047069725597], 'loss': [0.6517033765947954, 0.6614189392012576], 'acc': [0.6274752304073258, 0.6298610502065213]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 30s 137us/step - loss: 0.6846 - acc: 0.6200 - val_loss: 0.6603 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 29s 134us/step - loss: 0.6494 - acc: 0.6295 - val_loss: 0.6592 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [0.6602836197534904, 0.6592392290931283], 'val_acc': [0.6294047069725597, 0.6294047069725597], 'loss': [0.6846222679335571, 0.6493993953451501], 'acc': [0.6199762802388917, 0.6295380185282365]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 31s 142us/step - loss: 5.9499 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 30s 139us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290325124307, 5.973290325124307], 'val_acc': [0.6294047069725597, 0.6294047069725597], 'loss': [5.949894940355206, 5.953290198984358], 'acc': [0.6306363263154371, 0.6306455557807136]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 38s 176us/step - loss: 0.5549 - acc: 0.7196 - val_loss: 0.5173 - val_acc: 0.7443\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 37s 173us/step - loss: 0.5378 - acc: 0.7393 - val_loss: 0.5380 - val_acc: 0.7159\n",
      "Best validation accuracy of eval run: 0.7442916474245527\n",
      "history of eval run: {'val_loss': [0.5172572896108685, 0.5380122806077516], 'val_acc': [0.7442916474245527, 0.7159390862966167], 'loss': [0.554944083882437, 0.5377519875937447], 'acc': [0.7196177150539524, 0.7392811160236954]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 38s 176us/step - loss: 0.5538 - acc: 0.7181 - val_loss: 0.5451 - val_acc: 0.7392\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 38s 173us/step - loss: 0.5418 - acc: 0.7325 - val_loss: 0.5423 - val_acc: 0.7297\n",
      "Best validation accuracy of eval run: 0.7392155053079761\n",
      "history of eval run: {'val_loss': [0.5451220424592247, 0.5422769810441567], 'val_acc': [0.7392155053079761, 0.7296538994022927], 'loss': [0.5537783737781632, 0.541841101943653], 'acc': [0.7181271545035859, 0.7325482124766209]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 38s 176us/step - loss: 5.9524 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 37s 173us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597], 'loss': [5.952428096256029, 5.953290202665759], 'acc': [0.6305809494393353, 0.6306455557752123]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 38s 176us/step - loss: 3.9191 - acc: 0.5908 - val_loss: 0.5175 - val_acc: 0.7389\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 38s 173us/step - loss: 0.5352 - acc: 0.7341 - val_loss: 0.5148 - val_acc: 0.7498\n",
      "Best validation accuracy of eval run: 0.7497554222438321\n",
      "history of eval run: {'val_loss': [0.5174844107514546, 0.5147848809403541], 'val_acc': [0.7389017074142231, 0.7497554222438321], 'loss': [3.91911789811794, 0.5351998198409046], 'acc': [0.5908249768084632, 0.7341172235903479]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 38s 176us/step - loss: 0.5803 - acc: 0.7051 - val_loss: 0.5629 - val_acc: 0.7257\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 38s 173us/step - loss: 0.5781 - acc: 0.7088 - val_loss: 0.5413 - val_acc: 0.7272\n",
      "Best validation accuracy of eval run: 0.7271804337827192\n",
      "history of eval run: {'val_loss': [0.5629471735944382, 0.541267948353076], 'val_acc': [0.7257221965708378, 0.7271804337827192], 'loss': [0.5803450168920512, 0.578085828647517], 'acc': [0.7050859033595206, 0.7087869236842657]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 38s 176us/step - loss: 5.9513 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 37s 173us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597], 'loss': [5.95126275522624, 5.953290202943019], 'acc': [0.6306455557812637, 0.630645555769436]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 41s 190us/step - loss: 0.5587 - acc: 0.7119 - val_loss: 0.5403 - val_acc: 0.7223\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 40s 187us/step - loss: 0.5280 - acc: 0.7342 - val_loss: 0.5297 - val_acc: 0.7404\n",
      "Best validation accuracy of eval run: 0.7403968620234279\n",
      "history of eval run: {'val_loss': [0.5402961194735273, 0.5297365855527023], 'val_acc': [0.7223073373360187, 0.7403968620234279], 'loss': [0.558719151120898, 0.5280069685259781], 'acc': [0.7119157164145364, 0.7342464362736544]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 38s 175us/step - loss: 5.9517 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 37s 173us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597], 'loss': [5.951668246387388, 5.953290203704385], 'acc': [0.630576334699133, 0.6306455557812637]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 41s 190us/step - loss: 5.9518 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 40s 187us/step - loss: 5.9534 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597], 'loss': [5.951780057383294, 5.9533637704136595], 'acc': [0.6305855641798127, 0.6306409410328095]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 38s 175us/step - loss: 0.5972 - acc: 0.7113 - val_loss: 0.5264 - val_acc: 0.7350\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 38s 173us/step - loss: 0.5646 - acc: 0.7195 - val_loss: 0.5577 - val_acc: 0.6684\n",
      "Best validation accuracy of eval run: 0.7349700046168751\n",
      "history of eval run: {'val_loss': [0.5263951990397558, 0.5577441811187508], 'val_acc': [0.7349700046168751, 0.6684079372272218], 'loss': [0.5971531609210149, 0.5646450731498134], 'acc': [0.71132041514133, 0.7195069613325555]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 36s 167us/step - loss: 0.6337 - acc: 0.6328 - val_loss: 0.7017 - val_acc: 0.4229\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 36s 164us/step - loss: 0.6118 - acc: 0.6586 - val_loss: 0.7244 - val_acc: 0.3866\n",
      "Best validation accuracy of eval run: 0.42288878633341154\n",
      "history of eval run: {'val_loss': [0.7016577207574549, 0.7243579499975679], 'val_acc': [0.42288878633341154, 0.3866174434652843], 'loss': [0.6336900258780306, 0.6118009946088023], 'acc': [0.6328421713262333, 0.6585508798070565]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 38s 176us/step - loss: 5.9518 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 37s 173us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597], 'loss': [5.951817027570685, 5.9532902008723685], 'acc': [0.6305717199545297, 0.6306455557812637]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 38s 175us/step - loss: 5.9515 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 37s 173us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597], 'loss': [5.9514829331474335, 5.9532901925259525], 'acc': [0.6306409410427116, 0.6306455557752123]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 41s 190us/step - loss: 5.9514 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 41s 187us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597], 'loss': [5.951438810477486, 5.953290202524928], 'acc': [0.6306040231323703, 0.6306455557752123]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 38s 175us/step - loss: 5.9522 - acc: 0.6305 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 37s 173us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597], 'loss': [5.952155958085255, 5.953290200020783], 'acc': [0.6305255725712103, 0.6306455557793382]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 36s 167us/step - loss: 0.6383 - acc: 0.6260 - val_loss: 0.6694 - val_acc: 0.5651\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 36s 165us/step - loss: 0.6213 - acc: 0.6394 - val_loss: 0.6721 - val_acc: 0.5393\n",
      "Best validation accuracy of eval run: 0.5650946008383431\n",
      "history of eval run: {'val_loss': [0.6693873950221542, 0.6721180446894971], 'val_acc': [0.5650946008383431, 0.5392893400916208], 'loss': [0.6383352174463695, 0.6213408680197213], 'acc': [0.6259523666738925, 0.6394043295510102]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 38s 176us/step - loss: 0.5845 - acc: 0.6979 - val_loss: 0.5966 - val_acc: 0.7135\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 37s 173us/step - loss: 0.5846 - acc: 0.7023 - val_loss: 0.5634 - val_acc: 0.7230\n",
      "Best validation accuracy of eval run: 0.7229718504867413\n",
      "history of eval run: {'val_loss': [0.5965697658612094, 0.5633931159764102], 'val_acc': [0.713465620675943, 0.7229718504867413], 'loss': [0.5845153770246005, 0.5845546798171845], 'acc': [0.697859222785183, 0.7023032160107467]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 58s 269us/step - loss: 5.9526 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 57s 265us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290320195303, 5.973290320195303], 'val_acc': [0.6294047069725597, 0.6294047069725597], 'loss': [5.952590940831475, 5.9532902001066015], 'acc': [0.6306224820865783, 0.6306455557787881]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 40s 183us/step - loss: 0.5452 - acc: 0.7194 - val_loss: 0.5087 - val_acc: 0.7434\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 39s 180us/step - loss: 0.5126 - acc: 0.7442 - val_loss: 0.5048 - val_acc: 0.7537\n",
      "Best validation accuracy of eval run: 0.7536686663447187\n",
      "history of eval run: {'val_loss': [0.5086761923874915, 0.504825923769057], 'val_acc': [0.7433687124903652, 0.7536686663447187], 'loss': [0.5451844399733778, 0.5125899218430634], 'acc': [0.7193500602297647, 0.7442373452288048]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 40s 183us/step - loss: 0.5786 - acc: 0.6973 - val_loss: 0.5439 - val_acc: 0.7165\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 39s 180us/step - loss: 0.5606 - acc: 0.7126 - val_loss: 0.5402 - val_acc: 0.7331\n",
      "Best validation accuracy of eval run: 0.7330872173533772\n",
      "history of eval run: {'val_loss': [0.5439406644500342, 0.5401741390405408], 'val_acc': [0.7165297646537925, 0.7330872173533772], 'loss': [0.578561483797069, 0.5605920914559026], 'acc': [0.6973377573249575, 0.7126217714072142]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 55s 253us/step - loss: 0.5917 - acc: 0.6861 - val_loss: 0.5637 - val_acc: 0.7043\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 54s 250us/step - loss: 0.5764 - acc: 0.6965 - val_loss: 0.5801 - val_acc: 0.6933\n",
      "Best validation accuracy of eval run: 0.7042547300448327\n",
      "history of eval run: {'val_loss': [0.5637475199929348, 0.5801218127813766], 'val_acc': [0.7042547300448327, 0.693290263022153], 'loss': [0.5916771058932349, 0.5763814100748957], 'acc': [0.6861285573893319, 0.696497874913722]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 40s 183us/step - loss: 0.5673 - acc: 0.7028 - val_loss: 0.5439 - val_acc: 0.7201\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 39s 180us/step - loss: 0.5500 - acc: 0.7123 - val_loss: 0.5268 - val_acc: 0.7218\n",
      "Best validation accuracy of eval run: 0.7217904937547861\n",
      "history of eval run: {'val_loss': [0.5439123892685098, 0.5267844099133698], 'val_acc': [0.7201107521941709, 0.7217904937547861], 'loss': [0.5673428737783901, 0.5499584793135697], 'acc': [0.7028016077743614, 0.7123171986704847]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 40s 183us/step - loss: 0.6002 - acc: 0.6826 - val_loss: 0.6107 - val_acc: 0.7032\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 39s 179us/step - loss: 0.5940 - acc: 0.6924 - val_loss: 0.5950 - val_acc: 0.6974\n",
      "Best validation accuracy of eval run: 0.7031841255213513\n",
      "history of eval run: {'val_loss': [0.6106842325368563, 0.5949758540951386], 'val_acc': [0.7031841255213513, 0.6974065528402253], 'loss': [0.6001622185763007, 0.594020212933899], 'acc': [0.6825752087087026, 0.6923907576087797]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 55s 254us/step - loss: 10.0529 - acc: 0.3694 - val_loss: 10.0342 - val_acc: 0.3706\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 54s 249us/step - loss: 10.0540 - acc: 0.3694 - val_loss: 10.0342 - val_acc: 0.3706\n",
      "Best validation accuracy of eval run: 0.3705952930274404\n",
      "history of eval run: {'val_loss': [10.034212616822332, 10.034212616822332], 'val_acc': [0.3705952930274404, 0.3705952930274404], 'loss': [10.052853407978386, 10.053994704536777], 'acc': [0.3693636737004476, 0.3693544442233436]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 40s 184us/step - loss: 2.9499 - acc: 0.5892 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 39s 180us/step - loss: 2.9756 - acc: 0.5901 - val_loss: 10.0342 - val_acc: 0.3706\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 10.034212675688726], 'val_acc': [0.6294047069725597, 0.3705952930274404], 'loss': [2.9499262558346513, 2.97561706554156], 'acc': [0.5892052035863428, 0.5900958481169741]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 55s 252us/step - loss: 0.5869 - acc: 0.6868 - val_loss: 0.6266 - val_acc: 0.7093\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 54s 249us/step - loss: 0.5655 - acc: 0.7061 - val_loss: 0.5877 - val_acc: 0.7031\n",
      "Best validation accuracy of eval run: 0.7092939547618854\n",
      "history of eval run: {'val_loss': [0.6265502614869354, 0.5877165856843287], 'val_acc': [0.7092939547618854, 0.7031102907113013], 'loss': [0.586917002590554, 0.5655495498030944], 'acc': [0.6868299976453831, 0.7060688426705437]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 36s 167us/step - loss: 0.6443 - acc: 0.6225 - val_loss: 0.6645 - val_acc: 0.6515\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 36s 164us/step - loss: 0.6234 - acc: 0.6335 - val_loss: 0.6626 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6514813105720059\n",
      "history of eval run: {'val_loss': [0.664462038515678, 0.6625804772330804], 'val_acc': [0.6514813105720059, 0.6293677895763363], 'loss': [0.6442555242290757, 0.6234456909616214], 'acc': [0.622500542237006, 0.6335020789372421]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 32s 146us/step - loss: 6.2097 - acc: 0.6118 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 31s 142us/step - loss: 6.1182 - acc: 0.6184 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290325124307, 5.973290325124307], 'val_acc': [0.6294047069725597, 0.6294047069725597], 'loss': [6.20965234061673, 6.118164226849362], 'acc': [0.6117758898320103, 0.6183749659602519]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 41s 190us/step - loss: 5.9517 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 40s 187us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597], 'loss': [5.951729069140219, 5.9532902013696765], 'acc': [0.6305624904892533, 0.630645555773562]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 32s 146us/step - loss: 0.6038 - acc: 0.6771 - val_loss: 0.6017 - val_acc: 0.7036\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 31s 142us/step - loss: 0.5920 - acc: 0.6899 - val_loss: 0.6103 - val_acc: 0.6698\n",
      "Best validation accuracy of eval run: 0.7035717581767447\n",
      "history of eval run: {'val_loss': [0.6017403339822556, 0.6103497201251059], 'val_acc': [0.7035717581767447, 0.6698107983442182], 'loss': [0.6038321427012396, 0.5920336957998801], 'acc': [0.6770513666476706, 0.6898803398318225]}\n"
     ]
    }
   ],
   "source": [
    "init_session()\n",
    "model1_run1_trials=Trials()\n",
    "# hyperas optimize\n",
    "model1_run1_best = optim.minimize(model=create_model,\n",
    "                          data=data,\n",
    "                          algo=tpe.suggest,\n",
    "                          functions=functions,\n",
    "                          max_evals=50,\n",
    "                          trials=model1_run1_trials,\n",
    "                          notebook_name='DL_encode_attend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performing model hyper-parameters:\n",
      "({'batch_size': 1, 'dropout_rate1': 0.9322247223365211, 'dropout_rate1_1': 0.05693487970104716, 'dropout_rate1_2': 0.3513601920510685, 'learn_rate': 0.0029991793113097456, 'optimizer': 3}, None)\n"
     ]
    }
   ],
   "source": [
    "print(\"Best performing model hyper-parameters:\")\n",
    "print(model1_run1_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_optimization_history(model1_run1_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save optimization results for later use\n",
    "pickle.dump(model1_run1_best, open(data_folder+'model1_run1_best.p', 'wb'))\n",
    "pickle.dump(model1_run1_trials, open(data_folder+'model1_run1_trials.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='da_hyp_run2'></a>\n",
    "##### Deep optimization run\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function called by hyperas to build model for every evaluation (with different parameter combinations)\n",
    "# need a different definition from the shallow run, as the variable parameters are different\n",
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    K.clear_session()\n",
    "    # this is needed to free up gpu memory after every evaluation\n",
    "    gc.collect()\n",
    "    w2v = pickle.load(open(data_folder+'w2v.p', 'rb'))\n",
    "    model = build_model(vectors=w2v, max_length=50, projected_dim=200, num_classes=1, \n",
    "                        num_hidden=200, dropout_rate1={{uniform(0, 1)}}, \n",
    "                        dropout_rate2={{uniform(0, 1)}}, dropout_rate3={{uniform(0, 1)}},\n",
    "                        optimizer={{choice(['sgd', 'rmsprop','nadam', 'adam'])}},\n",
    "                        learn_rate={{loguniform(-6,-3)}})\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "    result = model.fit(x_train, y_train, batch_size= 64, epochs=20,\n",
    "      validation_split=0.2, callbacks=[early_stopping])\n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation accuracy of eval run:', validation_acc)\n",
    "    print('history of eval run:', result.history)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK,\n",
    "            'train_acc': np.amax(result.history['acc'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import spacy\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pickle\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import spacy\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pickle\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import time\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import layers, Model, models, initializers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.engine.topology import Layer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam, RMSprop, Nadam, SGD\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import EarlyStopping\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import accuracy_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import precision_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import recall_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import classification_report\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from utils.utils import init_session, data, plot_optimization_history, plot_roc\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.backend.tensorflow_backend import set_session\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, loguniform, lognormal, normal\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import gc\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import LearningRateScheduler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Layer, RNN\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import fmin, tpe, hp, Trials\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import STATUS_OK\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import LearningRateScheduler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import fmin, tpe, hp, Trials\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import STATUS_OK\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import LearningRateScheduler\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'dropout_rate1': hp.uniform('dropout_rate1', 0, 1),\n",
      "        'dropout_rate1_1': hp.uniform('dropout_rate1_1', 0, 1),\n",
      "        'dropout_rate1_2': hp.uniform('dropout_rate1_2', 0, 1),\n",
      "        'optimizer': hp.choice('optimizer', ['sgd', 'rmsprop','nadam', 'adam']),\n",
      "        'learn_rate': hp.loguniform('learn_rate', -6,-3),\n",
      "    }\n",
      "\n",
      ">>> Functions\n",
      "   1: def build_model(vectors, max_length, num_classes, projected_dim,  \n",
      "   2:                 num_hidden=200, dropout_rate1=0.2, dropout_rate2=0.2, \n",
      "   3:                 dropout_rate3=0.2, learn_rate=0.0001, optimizer='nadam'):\n",
      "   4:     K.clear_session()\n",
      "   5:     #input1 = layers.Input(shape=(max_length,), dtype='int32', name='words1')\n",
      "   6:     #input2 = layers.Input(shape=(max_length,), dtype='int32', name='words2')\n",
      "   7:     \n",
      "   8:     # modified model to take single input instead of two in order to work with hyperas specs.\n",
      "   9:     model_input = layers.Input(shape=(2, max_length), dtype='int32')\n",
      "  10:     input1 = layers.Lambda(get_input_slice(1))(model_input)\n",
      "  11:     input2 = layers.Lambda(get_input_slice(2))(model_input)\n",
      "  12:     \n",
      "  13:     # embeddings (projected)\n",
      "  14:     embed = create_embedding(vectors, max_length, projected_dim)\n",
      "  15:     a = embed(input1)\n",
      "  16:     b = embed(input2)     \n",
      "  17:     \n",
      "  18:     # step 1: attend\n",
      "  19:     F = create_feedforward(num_hidden, dropout_rate=dropout_rate1)\n",
      "  20:     att_weights = layers.dot([F(a), F(b)], axes=-1, normalize=True)\n",
      "  21:     \n",
      "  22:     G = create_feedforward(num_hidden, dropout_rate=dropout_rate2)    \n",
      "  23:     \n",
      "  24:     norm_weights_a = layers.Lambda(normalizer(1))(att_weights)\n",
      "  25:     norm_weights_b = layers.Lambda(normalizer(2))(att_weights)\n",
      "  26:     alpha = layers.dot([norm_weights_a, a], axes=1)\n",
      "  27:     beta  = layers.dot([norm_weights_b, b], axes=1)\n",
      "  28: \n",
      "  29:     # step 2: compare\n",
      "  30:     comp1 = layers.concatenate([a, beta])\n",
      "  31:     comp2 = layers.concatenate([b, alpha])\n",
      "  32:     v1 = layers.TimeDistributed(G)(comp1)\n",
      "  33:     v2 = layers.TimeDistributed(G)(comp2)\n",
      "  34: \n",
      "  35:     # step 3: aggregate\n",
      "  36:     v1_sum = layers.Lambda(sum_word)(v1)\n",
      "  37:     v2_sum = layers.Lambda(sum_word)(v2)\n",
      "  38:     concat = layers.concatenate([v1_sum, v2_sum])\n",
      "  39:         \n",
      "  40:     H = create_feedforward(num_hidden, dropout_rate=dropout_rate3)\n",
      "  41:     out = H(concat)\n",
      "  42:     out = layers.Dense(num_classes, activation='sigmoid', use_bias=True)(out)\n",
      "  43:     \n",
      "  44:     model = Model(model_input, out)\n",
      "  45:     if optimizer == 'sgd':\n",
      "  46:         opt = SGD(lr=learn_rate)\n",
      "  47:     elif optimizer == 'adam':\n",
      "  48:         opt = Adam(lr=learn_rate)\n",
      "  49:     elif optimizer == 'rmsprop':\n",
      "  50:         opt = RMSprop(lr=learn_rate)\n",
      "  51:     else:\n",
      "  52:         opt = Nadam(lr=learn_rate)\n",
      "  53:     \n",
      "  54:     model.compile(optimizer=opt,\n",
      "  55:     #model.compile(optimizer=optimizer,\n",
      "  56:                   loss='binary_crossentropy',\n",
      "  57:                   metrics=['accuracy'])\n",
      "  58:     return model\n",
      "  59: \n",
      "  60: def get_input_slice(i):\n",
      "  61:     def _slice(inputs):\n",
      "  62:         slice_i = K.squeeze(K.slice(inputs, [0,i-1,0], [-1,1,-1]),1)\n",
      "  63:         return slice_i\n",
      "  64:     return _slice\n",
      "  65: \n",
      "  66: def create_embedding(vectors, max_length, projected_dim):\n",
      "  67:     return models.Sequential([\n",
      "  68:         layers.Embedding(\n",
      "  69:             vectors.shape[0],\n",
      "  70:             vectors.shape[1],\n",
      "  71:             input_length=max_length,\n",
      "  72:             weights=[vectors],\n",
      "  73:             trainable=False),\n",
      "  74:         \n",
      "  75:         layers.TimeDistributed(\n",
      "  76:             layers.Dense(projected_dim,\n",
      "  77:                          activation=None,\n",
      "  78:                          use_bias=False))\n",
      "  79:     ])\n",
      "  80: \n",
      "  81: def create_feedforward(num_units, dropout_rate, activation='relu'):\n",
      "  82:     return models.Sequential([\n",
      "  83:         layers.Dense(num_units, activation=activation, use_bias=True),\n",
      "  84:         layers.Dropout(dropout_rate),\n",
      "  85:         layers.Dense(num_units, activation=activation, use_bias=True),\n",
      "  86:         layers.Dropout(dropout_rate)\n",
      "  87:     ])\n",
      "  88: \n",
      "  89: def normalizer(axis):\n",
      "  90:     def _normalize(att_weights):\n",
      "  91:         exp_weights = K.exp(att_weights)\n",
      "  92:         sum_weights = K.sum(exp_weights, axis=axis, keepdims=True)\n",
      "  93:         return exp_weights/sum_weights\n",
      "  94:     return _normalize\n",
      "  95: \n",
      "  96: def sum_word(x):\n",
      "  97:     return K.sum(x, axis=1)\n",
      "  98: \n",
      "  99: \n",
      ">>> Data\n",
      "  1: \n",
      "  2: data_folder = '/media/siri/78C6823EC681FD1E/minio/data/dq-data/dl/'\n",
      "  3: input_folder = '/media/siri/78C6823EC681FD1E/minio/data/dq-data/'\n",
      "  4: q1_train_w2v = pickle.load(open(data_folder+'q1_train_w2v.p', 'rb'))\n",
      "  5: q2_train_w2v = pickle.load(open(data_folder+'q2_train_w2v.p', 'rb'))\n",
      "  6: q1_test_w2v = pickle.load(open(data_folder+'q1_test_w2v.p', 'rb'))\n",
      "  7: q2_test_w2v = pickle.load(open(data_folder+'q2_test_w2v.p', 'rb'))\n",
      "  8: x_train = np.concatenate([np.expand_dims(q1_train_w2v, axis=1),np.expand_dims(q2_train_w2v, axis=1)], axis=1)\n",
      "  9: x_test = np.concatenate([np.expand_dims(q1_test_w2v, axis=1),np.expand_dims(q2_test_w2v, axis=1)], axis=1)\n",
      " 10: y_train = pickle.load(open(input_folder+'y_train.p', 'rb'))\n",
      " 11: y_test = pickle.load(open(input_folder+'y_test.p', 'rb'))\n",
      " 12: \n",
      " 13: \n",
      " 14: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     K.clear_session()\n",
      "   4:     # this is needed to free up gpu memory after every evaluation\n",
      "   5:     gc.collect()\n",
      "   6:     w2v = pickle.load(open(data_folder+'w2v.p', 'rb'))\n",
      "   7:     model = build_model(vectors=w2v, max_length=50, projected_dim=200, num_classes=1, \n",
      "   8:                         num_hidden=200, dropout_rate1=space['dropout_rate1'], \n",
      "   9:                         dropout_rate2=space['dropout_rate1_1'], dropout_rate3=space['dropout_rate1_2'],\n",
      "  10:                         optimizer=space['optimizer'],\n",
      "  11:                         learn_rate=space['learn_rate'])\n",
      "  12:     early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
      "  13:     result = model.fit(x_train, y_train, batch_size= 64, epochs=20,\n",
      "  14:       validation_split=0.2, callbacks=[early_stopping])\n",
      "  15:     validation_acc = np.amax(result.history['val_acc']) \n",
      "  16:     print('Best validation accuracy of eval run:', validation_acc)\n",
      "  17:     print('history of eval run:', result.history)\n",
      "  18:     return {'loss': -validation_acc, 'status': STATUS_OK,\n",
      "  19:             'train_acc': np.amax(result.history['acc'])}\n",
      "  20: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 40s 186us/step - loss: 0.6618 - acc: 0.6278 - val_loss: 0.6599 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 40s 184us/step - loss: 0.8999 - acc: 0.6271 - val_loss: 0.6593 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 40s 184us/step - loss: 0.9116 - acc: 0.6274 - val_loss: 0.6593 - val_acc: 0.6294\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 40s 184us/step - loss: 1.6592 - acc: 0.6216 - val_loss: 0.6595 - val_acc: 0.6294\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 40s 184us/step - loss: 1.9243 - acc: 0.6178 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 40s 184us/step - loss: 2.4517 - acc: 0.6174 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [0.6599187497989231, 0.6592820562202918, 0.6592739675888886, 0.6594745785110713, 5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597], 'loss': [0.6618034494872859, 0.8998698679974076, 0.9115508824496511, 1.6592493865789413, 1.9243055716633295, 2.4517457546999424], 'acc': [0.6277890326115322, 0.627124510260046, 0.6273552471895708, 0.621596053478891, 0.6177888941754269, 0.6174474035126777]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 36s 165us/step - loss: 0.6357 - acc: 0.6280 - val_loss: 0.6465 - val_acc: 0.6290\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6205 - acc: 0.6388 - val_loss: 0.6311 - val_acc: 0.6617\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6083 - acc: 0.6541 - val_loss: 0.6345 - val_acc: 0.6163\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5970 - acc: 0.6733 - val_loss: 0.6045 - val_acc: 0.6815\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5867 - acc: 0.6852 - val_loss: 0.6439 - val_acc: 0.6017\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5786 - acc: 0.6938 - val_loss: 0.6169 - val_acc: 0.6504\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5716 - acc: 0.7006 - val_loss: 0.5992 - val_acc: 0.6735\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5665 - acc: 0.7042 - val_loss: 0.5875 - val_acc: 0.6862\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5620 - acc: 0.7084 - val_loss: 0.5583 - val_acc: 0.7138\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5577 - acc: 0.7126 - val_loss: 0.5860 - val_acc: 0.6870\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5547 - acc: 0.7149 - val_loss: 0.5874 - val_acc: 0.6849\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 35s 162us/step - loss: 0.5522 - acc: 0.7166 - val_loss: 0.5739 - val_acc: 0.6950\n",
      "Best validation accuracy of eval run: 0.713816335935113\n",
      "history of eval run: {'val_loss': [0.6465269567859739, 0.6310591204022283, 0.6344618082013502, 0.6045084052851633, 0.6438712884909995, 0.6169149922851489, 0.5991515790288439, 0.587536578522776, 0.5583171316688381, 0.5859509432376339, 0.5874381462065481, 0.5739370812817438], 'val_acc': [0.6290170742852599, 0.6616520535357272, 0.6162805722097565, 0.6814951545794988, 0.601679741569417, 0.6503553299393365, 0.6735209967576252, 0.6862390401355671, 0.713816335935113, 0.6869589293833751, 0.684946931229245, 0.6949884633001338], 'loss': [0.6356889391803401, 0.6204833402268586, 0.6083054345901215, 0.5969760558873399, 0.5866587761293472, 0.5786447601678094, 0.571558645203838, 0.5664884297241637, 0.5620242637600531, 0.5577198731435346, 0.5546968307354374, 0.5522423830985244], 'acc': [0.6279505484627771, 0.6387582661479557, 0.6541253455257985, 0.6733365021218477, 0.6852332981066706, 0.6937844086377268, 0.7006188364431222, 0.7042091030406943, 0.7084223593367333, 0.7126263861457661, 0.714882993297688, 0.7165950613062515]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 38s 174us/step - loss: 5.9525 - acc: 0.6305 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186, 5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597], 'loss': [5.95253311659411, 5.953290200738139, 5.953290199963571, 5.953290196110532], 'acc': [0.6305486462674085, 0.630645555783189, 0.630645555783189, 0.630645555773287]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 39s 181us/step - loss: 7.4137 - acc: 0.5375 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 38s 177us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 38s 177us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 38s 177us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186, 5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597], 'loss': [7.413708974105524, 5.953290199197804, 5.953290197221774, 5.953290197127154], 'acc': [0.5374601401930066, 0.6306455557812637, 0.630645555773287, 0.6306455557789257]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 39s 181us/step - loss: 5.9518 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 38s 177us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 38s 177us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 38s 177us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186, 5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597], 'loss': [5.951785946267296, 5.95329019756945, 5.953290200867968, 5.953290193062869], 'acc': [0.630590178914789, 0.630645555783189, 0.6306455557754874, 0.630645555783189]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 41s 187us/step - loss: 0.6252 - acc: 0.6623 - val_loss: 0.6207 - val_acc: 0.6764\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 40s 184us/step - loss: 0.6189 - acc: 0.6674 - val_loss: 0.5963 - val_acc: 0.6933\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 40s 184us/step - loss: 0.6055 - acc: 0.6754 - val_loss: 0.5932 - val_acc: 0.7029\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 40s 184us/step - loss: 0.6007 - acc: 0.6801 - val_loss: 0.5865 - val_acc: 0.7002\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 40s 184us/step - loss: 0.5998 - acc: 0.6840 - val_loss: 0.5895 - val_acc: 0.6958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 40s 184us/step - loss: 0.5986 - acc: 0.6856 - val_loss: 0.5875 - val_acc: 0.7099\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 40s 184us/step - loss: 0.5969 - acc: 0.6859 - val_loss: 0.5871 - val_acc: 0.7083\n",
      "Best validation accuracy of eval run: 0.7098661744369028\n",
      "history of eval run: {'val_loss': [0.6206647954407407, 0.5962569428084356, 0.593220702148441, 0.5864934797516934, 0.5895440172753028, 0.5874503746487107, 0.5870923181981825], 'val_acc': [0.676437471161584, 0.6932533456237293, 0.7028703276441017, 0.7002307337343721, 0.6958006460401502, 0.7098661744369028, 0.708334102447978], 'loss': [0.6251815385578453, 0.6188609357335071, 0.6055439459534181, 0.6006818375858695, 0.5997512063970223, 0.5986427138521893, 0.596936946668402], 'acc': [0.6623257359365308, 0.6674111778188955, 0.6754315934283007, 0.6801247825238443, 0.6840242366063249, 0.6855840182330458, 0.6858978204543059]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 39s 181us/step - loss: 5.9517 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 38s 178us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 38s 178us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 39s 178us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186, 5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597], 'loss': [5.951687184346225, 5.9532901953865744, 5.953290194156507, 5.953290202236666], 'acc': [0.6305901789205652, 0.630645555783189, 0.630645555782914, 0.630645555773287]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 41s 188us/step - loss: 5.9515 - acc: 0.6307 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 40s 184us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 40s 184us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 40s 185us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186, 5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597], 'loss': [5.951479563534416, 5.953290203765998, 5.953290195034498, 5.953290197978739], 'acc': [0.6306824736894044, 0.630645555783189, 0.630645555783189, 0.6306455557793382]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 36s 166us/step - loss: 0.6474 - acc: 0.6219 - val_loss: 0.6477 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6271 - acc: 0.6303 - val_loss: 0.6421 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6179 - acc: 0.6309 - val_loss: 0.6404 - val_acc: 0.6804\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6094 - acc: 0.6500 - val_loss: 0.6213 - val_acc: 0.6785\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6032 - acc: 0.6704 - val_loss: 0.6453 - val_acc: 0.6203\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5972 - acc: 0.6811 - val_loss: 0.6372 - val_acc: 0.6241\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5912 - acc: 0.6881 - val_loss: 0.6147 - val_acc: 0.6620\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5872 - acc: 0.6917 - val_loss: 0.6330 - val_acc: 0.6195\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5834 - acc: 0.6955 - val_loss: 0.6053 - val_acc: 0.6753\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5808 - acc: 0.6978 - val_loss: 0.6361 - val_acc: 0.6174\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5772 - acc: 0.7005 - val_loss: 0.6136 - val_acc: 0.6538\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5751 - acc: 0.7026 - val_loss: 0.6419 - val_acc: 0.6085\n",
      "Best validation accuracy of eval run: 0.6804245500560174\n",
      "history of eval run: {'val_loss': [0.6477019359045553, 0.6421048828044904, 0.6404184152049076, 0.6212859248960959, 0.6452880836941429, 0.6372246425119918, 0.6147167908358695, 0.6330166281672409, 0.6052605184190659, 0.6360876938182195, 0.613599826748271, 0.6418867554640407], 'val_acc': [0.6294047069725597, 0.6294047069725597, 0.6804245500560174, 0.6785048454116994, 0.6203230272155783, 0.6240886017425741, 0.6620212275089621, 0.6194739270947416, 0.6753484079262382, 0.617369635448403, 0.6537701891917592, 0.6084725426769388], 'loss': [0.6474306750552912, 0.6270706314820255, 0.6179030598423769, 0.6094411887629079, 0.6031882736574705, 0.5971735678111374, 0.5911619106847457, 0.5871879776424124, 0.5833940578168153, 0.5808083547156433, 0.5771645730395906, 0.5750628426979302], 'acc': [0.621872937785959, 0.6302902209127607, 0.6308947516592202, 0.6499582366117036, 0.6703830694521515, 0.6810800334140078, 0.68806674757374, 0.6917077763011601, 0.6954918619121393, 0.6977900017049777, 0.7004711648094586, 0.7026308624418866]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 38s 174us/step - loss: 5.9525 - acc: 0.6305 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186, 5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597], 'loss': [5.952472040592572, 5.953290201498405, 5.953290192521552, 5.953290190976816], 'acc': [0.6305163430960318, 0.6306455557834642, 0.630645555769436, 0.6306455557752123]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 39s 181us/step - loss: 5.9519 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 38s 178us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 38s 177us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 38s 178us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186, 5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597], 'loss': [5.951853669935144, 5.953290200113203, 5.9532901958728806, 5.953290195668236], 'acc': [0.6305624904849899, 0.6306455557812637, 0.6306455557812637, 0.630645555783189]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 38s 174us/step - loss: 1.6785 - acc: 0.6974 - val_loss: 0.5246 - val_acc: 0.7431\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216697/216697 [==============================] - 37s 172us/step - loss: 0.5458 - acc: 0.7286 - val_loss: 0.5763 - val_acc: 0.6617\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 0.5748 - acc: 0.7160 - val_loss: 0.5795 - val_acc: 0.7025\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 0.6324 - acc: 0.6956 - val_loss: 0.6670 - val_acc: 0.6940\n",
      "Best validation accuracy of eval run: 0.7431472081075245\n",
      "history of eval run: {'val_loss': [0.5245814619594639, 0.5763193200984527, 0.5794990224864662, 0.6669995107087442], 'val_acc': [0.7431472081075245, 0.6617443470103321, 0.7025196123695285, 0.6939547761750761], 'loss': [1.6785099566407236, 0.5458267374215604, 0.5748261560734838, 0.632389973839517], 'acc': [0.6973885194490294, 0.7285795373199864, 0.7160274484723338, 0.695551853517166]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 39s 181us/step - loss: 1.1570 - acc: 0.6242 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 39s 178us/step - loss: 4.7318 - acc: 0.6180 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 38s 178us/step - loss: 4.2931 - acc: 0.6229 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 38s 178us/step - loss: 4.8271 - acc: 0.6232 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186, 5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597], 'loss': [1.1570436364747962, 4.731787125178619, 4.293072116183143, 4.827070315484906], 'acc': [0.6241526186328412, 0.6179734837098048, 0.6228789507886419, 0.6232389004053239]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 36s 166us/step - loss: 0.6369 - acc: 0.6273 - val_loss: 0.6198 - val_acc: 0.6330\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6204 - acc: 0.6412 - val_loss: 0.6030 - val_acc: 0.6495\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6106 - acc: 0.6542 - val_loss: 0.6084 - val_acc: 0.6777\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6017 - acc: 0.6653 - val_loss: 0.5841 - val_acc: 0.6928\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5942 - acc: 0.6759 - val_loss: 0.5857 - val_acc: 0.6895\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5875 - acc: 0.6850 - val_loss: 0.5830 - val_acc: 0.6926\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5818 - acc: 0.6882 - val_loss: 0.5680 - val_acc: 0.7064\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5767 - acc: 0.6946 - val_loss: 0.5665 - val_acc: 0.7014\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5724 - acc: 0.6989 - val_loss: 0.5620 - val_acc: 0.7100\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 35s 164us/step - loss: 0.5688 - acc: 0.7022 - val_loss: 0.5624 - val_acc: 0.7109\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5653 - acc: 0.7056 - val_loss: 0.5685 - val_acc: 0.7001\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5624 - acc: 0.7082 - val_loss: 0.5568 - val_acc: 0.7099\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5593 - acc: 0.7107 - val_loss: 0.5532 - val_acc: 0.7157\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5564 - acc: 0.7130 - val_loss: 0.5607 - val_acc: 0.7041\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5540 - acc: 0.7154 - val_loss: 0.5438 - val_acc: 0.7247\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5523 - acc: 0.7167 - val_loss: 0.5594 - val_acc: 0.7087\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5498 - acc: 0.7191 - val_loss: 0.5428 - val_acc: 0.7208\n",
      "Epoch 18/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5481 - acc: 0.7213 - val_loss: 0.5433 - val_acc: 0.7217\n",
      "Epoch 19/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5470 - acc: 0.7213 - val_loss: 0.5395 - val_acc: 0.7259\n",
      "Epoch 20/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5443 - acc: 0.7239 - val_loss: 0.5372 - val_acc: 0.7271\n",
      "Best validation accuracy of eval run: 0.727069681572045\n",
      "history of eval run: {'val_loss': [0.6197784855193018, 0.6030458007521161, 0.6083594953469214, 0.5840837266090888, 0.5856837601853121, 0.5829871346216682, 0.5679950509893218, 0.5665165722496673, 0.5619630661030546, 0.5623943410510559, 0.5684533458752845, 0.5568450355738828, 0.5532308643596977, 0.5607094050922534, 0.5437970820301735, 0.5594033471119806, 0.5427913795075331, 0.5433371181415423, 0.5394960499040862, 0.5371618470328163], 'val_acc': [0.6330226119102615, 0.649450853719214, 0.677729580052503, 0.6927918781582859, 0.6894877711011344, 0.6926257498901343, 0.7064143977882569, 0.7014490078482476, 0.710013844027297, 0.7108629441657373, 0.7000830641307751, 0.7099400092161466, 0.7157360405948341, 0.7040886017403737, 0.7246885094446799, 0.708684817707148, 0.7208306414254756, 0.7216612828641539, 0.7258883248576933, 0.727069681572045], 'loss': [0.6369167512618757, 0.6203832548619111, 0.6105713504506879, 0.6016561958332313, 0.5942491481452008, 0.5874573731743474, 0.5817911154501089, 0.5767414323984236, 0.572432702465459, 0.5688349597724631, 0.565294169186484, 0.5624099267204246, 0.5593336761373779, 0.5564402169586419, 0.5539997547734673, 0.5522817828887646, 0.5497694062513409, 0.5480990543937251, 0.5470140031807367, 0.5442508312414145], 'acc': [0.6273275587524826, 0.641208692319061, 0.6541899518616755, 0.6653483896922084, 0.6758930672796486, 0.684988716971392, 0.6882051897286492, 0.6946335205315676, 0.6989344568655955, 0.7022063064970291, 0.7055935245961132, 0.7081823929199274, 0.7107389580857051, 0.7129955652395522, 0.7154044587579134, 0.7167427329399151, 0.7190639464296393, 0.7212836356750707, 0.7213436272723956, 0.7239371103504402]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 38s 174us/step - loss: 0.6854 - acc: 0.6541 - val_loss: 0.6018 - val_acc: 0.6536\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 0.6484 - acc: 0.6532 - val_loss: 0.6491 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 0.6617 - acc: 0.6439 - val_loss: 0.7691 - val_acc: 0.3855\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 0.6770 - acc: 0.6340 - val_loss: 0.6760 - val_acc: 0.6285\n",
      "Best validation accuracy of eval run: 0.6536225196167682\n",
      "history of eval run: {'val_loss': [0.6018205686901702, 0.6491235299070804, 0.7690529837782173, 0.6759686908614102], 'val_acc': [0.6536225196167682, 0.6294047069725597, 0.3854730041477061, 0.6285371481354577], 'loss': [0.6854364580123753, 0.6484141010071558, 0.6617078857118716, 0.6769647402088413], 'acc': [0.6540653539345248, 0.6531931683443484, 0.6439036996334216, 0.6340373886150489]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 38s 174us/step - loss: 5.9523 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186, 5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597], 'loss': [5.952261876344121, 5.9532901970259315, 5.953290197571651, 5.953290198330816], 'acc': [0.6305624904755003, 0.630645555773287, 0.6306455557793382, 0.6306455557793382]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 38s 174us/step - loss: 0.6373 - acc: 0.6710 - val_loss: 0.6204 - val_acc: 0.6773\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 0.6326 - acc: 0.6718 - val_loss: 0.6388 - val_acc: 0.6665\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 0.6595 - acc: 0.6669 - val_loss: 0.6498 - val_acc: 0.6412\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 0.6806 - acc: 0.6602 - val_loss: 0.6657 - val_acc: 0.6059\n",
      "Best validation accuracy of eval run: 0.6773419473960095\n",
      "history of eval run: {'val_loss': [0.6203924334214258, 0.6387779773167327, 0.6498382136774438, 0.6656631425409533], 'val_acc': [0.6773419473960095, 0.6665251499637239, 0.6411998154009109, 0.6059067835585555], 'loss': [0.6373273823655333, 0.6326186909638388, 0.6595233294974476, 0.6805584581812463], 'acc': [0.6710106738855948, 0.6718136384054681, 0.6668850976258943, 0.6602444888556358]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 39s 181us/step - loss: 0.6670 - acc: 0.6278 - val_loss: 0.6589 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 39s 178us/step - loss: 0.6711 - acc: 0.6301 - val_loss: 0.6601 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 39s 178us/step - loss: 0.6730 - acc: 0.6302 - val_loss: 0.6716 - val_acc: 0.6294\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 38s 177us/step - loss: 0.6818 - acc: 0.6304 - val_loss: 0.6651 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [0.6588981984153451, 0.6601235935761733, 0.6715776992746361, 0.6651010352738947], 'val_acc': [0.6294047069725597, 0.6293862482738979, 0.6294047069725597, 0.6294047069725597], 'loss': [0.6670424937415234, 0.6711438163289292, 0.6730217487306911, 0.6817924293317501], 'acc': [0.6278259505218735, 0.6300640987198624, 0.630188696660766, 0.6304425072769995]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 36s 166us/step - loss: 0.7048 - acc: 0.6249 - val_loss: 0.6600 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6506 - acc: 0.6299 - val_loss: 0.6590 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6449 - acc: 0.6301 - val_loss: 0.6579 - val_acc: 0.6294\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6417 - acc: 0.6304 - val_loss: 0.6537 - val_acc: 0.6294\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6366 - acc: 0.6305 - val_loss: 0.6610 - val_acc: 0.6294\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6327 - acc: 0.6306 - val_loss: 0.6631 - val_acc: 0.6294\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6294 - acc: 0.6306 - val_loss: 0.6611 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [0.6599573532182829, 0.6589772031270913, 0.6579021743645468, 0.6537317497971205, 0.6609672563550216, 0.6631322059875597, 0.6610532301463562], 'val_acc': [0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597], 'loss': [0.7047741617070585, 0.6505558238649817, 0.6449374623931107, 0.6417400887995689, 0.6366408060438006, 0.632736020944083, 0.6294067325944975], 'acc': [0.6248632883637959, 0.6298656649659777, 0.6300733281911901, 0.6304240483230665, 0.6305440315273436, 0.6306409410328095, 0.630645555769436]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 38s 174us/step - loss: 5.9514 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186, 5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597], 'loss': [5.95141360763883, 5.953290204349125, 5.953290197003927, 5.953290197565049], 'acc': [0.6306178673480263, 0.630645555773287, 0.6306455557713615, 0.6306455557774128]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 36s 166us/step - loss: 0.6318 - acc: 0.6332 - val_loss: 0.6321 - val_acc: 0.6389\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 35s 164us/step - loss: 0.6176 - acc: 0.6477 - val_loss: 0.6298 - val_acc: 0.6274\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6078 - acc: 0.6599 - val_loss: 0.6175 - val_acc: 0.6378\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 35s 164us/step - loss: 0.5993 - acc: 0.6690 - val_loss: 0.6453 - val_acc: 0.5907\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5927 - acc: 0.6766 - val_loss: 0.6001 - val_acc: 0.6664\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5864 - acc: 0.6823 - val_loss: 0.5784 - val_acc: 0.6927\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5818 - acc: 0.6875 - val_loss: 0.5771 - val_acc: 0.6935\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5778 - acc: 0.6920 - val_loss: 0.6081 - val_acc: 0.6492\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5734 - acc: 0.6950 - val_loss: 0.5783 - val_acc: 0.6916\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5704 - acc: 0.6992 - val_loss: 0.5620 - val_acc: 0.7082\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5670 - acc: 0.7023 - val_loss: 0.5594 - val_acc: 0.7076\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 35s 164us/step - loss: 0.5642 - acc: 0.7047 - val_loss: 0.5832 - val_acc: 0.6830\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5616 - acc: 0.7070 - val_loss: 0.5507 - val_acc: 0.7178\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5595 - acc: 0.7085 - val_loss: 0.5635 - val_acc: 0.7007\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5572 - acc: 0.7100 - val_loss: 0.5590 - val_acc: 0.7081\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5549 - acc: 0.7119 - val_loss: 0.5600 - val_acc: 0.7052\n",
      "Best validation accuracy of eval run: 0.7177849561627911\n",
      "history of eval run: {'val_loss': [0.6320816408471133, 0.629794576893108, 0.6175167395026661, 0.6452504323130662, 0.600116827073161, 0.5783923435288197, 0.5770638247970947, 0.6080538026077383, 0.578331902772532, 0.5620222997852443, 0.5593994029997459, 0.583151801624879, 0.5507279292944119, 0.5635106562814461, 0.5589889077640757, 0.5600232505259229], 'val_acc': [0.638874019388235, 0.6274480849001118, 0.637840332262077, 0.5906783571659171, 0.6663959390917955, 0.6926995846671776, 0.6935486848221213, 0.6491924319390499, 0.6915551453490493, 0.7082048915419427, 0.7075957545015082, 0.6829533917737767, 0.7177849561627911, 0.7007106599017778, 0.7080572219691521, 0.7051961236765817], 'loss': [0.6317782304785662, 0.6175823359665875, 0.6077644125721783, 0.5993155439559026, 0.5927377079073577, 0.5863507093848831, 0.5818417506128793, 0.577797138435748, 0.5734098857994755, 0.5704273229041752, 0.5670210882034784, 0.5642190396315658, 0.5615651965309908, 0.5594947792768556, 0.5572106742101152, 0.5548530278046578], 'acc': [0.6332482683188082, 0.6477200884255422, 0.6598937687179831, 0.6689709594516676, 0.6766360401865186, 0.6823075538592094, 0.687508364207024, 0.691961586919594, 0.6949519374933051, 0.6992251853907949, 0.7023124454958275, 0.704735183231495, 0.7069871756352376, 0.7084546625065972, 0.7100098293903652, 0.7118834132523741]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 36s 166us/step - loss: 0.6281 - acc: 0.6372 - val_loss: 0.6063 - val_acc: 0.6540\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6101 - acc: 0.6558 - val_loss: 0.5931 - val_acc: 0.6799\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5979 - acc: 0.6702 - val_loss: 0.5822 - val_acc: 0.6886\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5892 - acc: 0.6805 - val_loss: 0.5799 - val_acc: 0.6896\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5811 - acc: 0.6879 - val_loss: 0.5694 - val_acc: 0.6986\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5753 - acc: 0.6938 - val_loss: 0.5792 - val_acc: 0.6813\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5702 - acc: 0.6984 - val_loss: 0.5610 - val_acc: 0.7061\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5657 - acc: 0.7022 - val_loss: 0.5760 - val_acc: 0.6919\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5623 - acc: 0.7059 - val_loss: 0.5561 - val_acc: 0.7076\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5591 - acc: 0.7092 - val_loss: 0.5495 - val_acc: 0.7159\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5557 - acc: 0.7115 - val_loss: 0.5461 - val_acc: 0.7182\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5522 - acc: 0.7142 - val_loss: 0.5425 - val_acc: 0.7204\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5493 - acc: 0.7174 - val_loss: 0.5425 - val_acc: 0.7218\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5474 - acc: 0.7192 - val_loss: 0.5380 - val_acc: 0.7250\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5444 - acc: 0.7204 - val_loss: 0.5382 - val_acc: 0.7252\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5412 - acc: 0.7242 - val_loss: 0.5341 - val_acc: 0.7270\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5387 - acc: 0.7256 - val_loss: 0.5294 - val_acc: 0.7301\n",
      "Epoch 18/20\n",
      "216697/216697 [==============================] - 35s 164us/step - loss: 0.5362 - acc: 0.7266 - val_loss: 0.5360 - val_acc: 0.7244\n",
      "Epoch 19/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5327 - acc: 0.7292 - val_loss: 0.5242 - val_acc: 0.7328\n",
      "Epoch 20/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5302 - acc: 0.7310 - val_loss: 0.5249 - val_acc: 0.7326\n",
      "Best validation accuracy of eval run: 0.7328103368558474\n",
      "history of eval run: {'val_loss': [0.6063446791060905, 0.5930802562300599, 0.5821995773659799, 0.5799387003988179, 0.5693719215700029, 0.5791504420670817, 0.560956851755577, 0.5760054730164273, 0.5561132889328654, 0.5494560264561877, 0.5460692043303343, 0.5425047866518067, 0.5425041215118308, 0.5380152727900753, 0.5382000289565141, 0.534142410841525, 0.5294062585780078, 0.5360454939998236, 0.5241597361035428, 0.5249286139060894], 'val_acc': [0.6540470696683848, 0.679907706509992, 0.6885648361812499, 0.689635440703631, 0.6986432856351591, 0.6813105676093848, 0.7061375173072304, 0.6919427780363491, 0.7075957545037087, 0.7158652514865665, 0.718246423630435, 0.7203691739754353, 0.7217720350561243, 0.7249653899246061, 0.7252053530248124, 0.7269773880798365, 0.730096908152571, 0.7244485463631775, 0.7328103368558474, 0.7326442085689919], 'loss': [0.6281484643634035, 0.6100813750553474, 0.5978805855850882, 0.5892395441888953, 0.5811093141580568, 0.5753281367345682, 0.5702071066692066, 0.5656596677060393, 0.5622663141474742, 0.5590889545683664, 0.5557264705817441, 0.5522271254674639, 0.5492925876328595, 0.5473622189196153, 0.5444347959540957, 0.5412227911373226, 0.5386765664112657, 0.5361760379604685, 0.5327419863632687, 0.5301635248535933], 'acc': [0.637221558206018, 0.655832798786183, 0.6701846356946911, 0.6805401089957236, 0.6878683138160044, 0.69383517076785, 0.6983853029781841, 0.7022201507107597, 0.7059211710432065, 0.7091514880260219, 0.7115142341624389, 0.7142323151714849, 0.7174349437287646, 0.7191700854223864, 0.7204345237834304, 0.7241724620066893, 0.7256076457040591, 0.7266367323973014, 0.7291517649023582, 0.7310484224508008]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 36s 166us/step - loss: 0.6268 - acc: 0.6373 - val_loss: 0.6333 - val_acc: 0.6152\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 36s 164us/step - loss: 0.6094 - acc: 0.6567 - val_loss: 0.5894 - val_acc: 0.6825\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 35s 164us/step - loss: 0.5961 - acc: 0.6731 - val_loss: 0.5958 - val_acc: 0.6612\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 36s 164us/step - loss: 0.5856 - acc: 0.6838 - val_loss: 0.5818 - val_acc: 0.6881\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 35s 164us/step - loss: 0.5776 - acc: 0.6918 - val_loss: 0.5640 - val_acc: 0.7029\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 36s 164us/step - loss: 0.5719 - acc: 0.6970 - val_loss: 0.5604 - val_acc: 0.7069\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 35s 164us/step - loss: 0.5665 - acc: 0.7020 - val_loss: 0.5626 - val_acc: 0.7007\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 35s 164us/step - loss: 0.5615 - acc: 0.7074 - val_loss: 0.5546 - val_acc: 0.7080\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 36s 164us/step - loss: 0.5575 - acc: 0.7093 - val_loss: 0.5472 - val_acc: 0.7188\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 35s 164us/step - loss: 0.5537 - acc: 0.7132 - val_loss: 0.5461 - val_acc: 0.7204\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 36s 164us/step - loss: 0.5498 - acc: 0.7162 - val_loss: 0.5407 - val_acc: 0.7230\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 35s 164us/step - loss: 0.5466 - acc: 0.7186 - val_loss: 0.5399 - val_acc: 0.7228\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 36s 164us/step - loss: 0.5433 - acc: 0.7218 - val_loss: 0.5430 - val_acc: 0.7181\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 36s 164us/step - loss: 0.5399 - acc: 0.7244 - val_loss: 0.5316 - val_acc: 0.7287\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 36s 164us/step - loss: 0.5372 - acc: 0.7260 - val_loss: 0.5276 - val_acc: 0.7321\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 36s 164us/step - loss: 0.5339 - acc: 0.7285 - val_loss: 0.5243 - val_acc: 0.7342\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 36s 164us/step - loss: 0.5306 - acc: 0.7306 - val_loss: 0.5237 - val_acc: 0.7350\n",
      "Epoch 18/20\n",
      "216697/216697 [==============================] - 36s 164us/step - loss: 0.5279 - acc: 0.7329 - val_loss: 0.5186 - val_acc: 0.7384\n",
      "Epoch 19/20\n",
      "216697/216697 [==============================] - 35s 164us/step - loss: 0.5245 - acc: 0.7353 - val_loss: 0.5258 - val_acc: 0.7331\n",
      "Epoch 20/20\n",
      "216697/216697 [==============================] - 36s 164us/step - loss: 0.5218 - acc: 0.7375 - val_loss: 0.5140 - val_acc: 0.7409\n",
      "Best validation accuracy of eval run: 0.7409137055848566\n",
      "history of eval run: {'val_loss': [0.6332663951175357, 0.5893761794370168, 0.5957625175842669, 0.5818466720660271, 0.563991689997221, 0.5604169335297081, 0.5625722189449087, 0.5546310721958734, 0.5472385730009559, 0.5461398274451839, 0.5407136138848682, 0.5399497710904825, 0.5430403270823756, 0.5315625079232644, 0.527573804547834, 0.5242901180628867, 0.5236995803193117, 0.5186019483116513, 0.525800857576952, 0.5139789752570726], 'val_acc': [0.6152284264025404, 0.6825288417023561, 0.6612275034665072, 0.6881218274155684, 0.7029072450425254, 0.7069497000318439, 0.7007475772980011, 0.7079833871745052, 0.7188371019700072, 0.7204430087535789, 0.7229533917880795, 0.7228241808974474, 0.7181172127387025, 0.7287309644692055, 0.7321089063067011, 0.7342316566704054, 0.7349700046168751, 0.7384402399652831, 0.7331056760355356, 0.7409137055848566], 'loss': [0.626786348343571, 0.6094458450368728, 0.596088784368915, 0.5855631055901532, 0.577553557548195, 0.5719384959124325, 0.5665237747641536, 0.5614936119116747, 0.5574507820646225, 0.5536807516704624, 0.5498176367663429, 0.5466091213647677, 0.5432746695967268, 0.5399210712871487, 0.5372349104642556, 0.5339198354944801, 0.5306288549191908, 0.5279429532865717, 0.5245081187579085, 0.5217817188117391], 'acc': [0.6372953940363277, 0.6567465170214019, 0.6731334536255602, 0.6838350323333952, 0.6917769973698128, 0.6969685782446494, 0.7020078727491958, 0.7074486494923619, 0.7093176186059167, 0.7131616958312746, 0.7162489559187033, 0.7185701694144788, 0.7218281748321819, 0.7243616662873208, 0.7259860542595458, 0.7285149309802585, 0.7305869485958771, 0.7329496947408211, 0.7353308998330964, 0.7374859827368745]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 36s 166us/step - loss: 0.6084 - acc: 0.6579 - val_loss: 0.5744 - val_acc: 0.6971\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 35s 164us/step - loss: 0.5689 - acc: 0.6995 - val_loss: 0.5508 - val_acc: 0.7152\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 35s 164us/step - loss: 0.5471 - acc: 0.7159 - val_loss: 0.5541 - val_acc: 0.7074\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 36s 164us/step - loss: 0.5334 - acc: 0.7268 - val_loss: 0.5204 - val_acc: 0.7357\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5197 - acc: 0.7361 - val_loss: 0.5099 - val_acc: 0.7376\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5073 - acc: 0.7444 - val_loss: 0.5116 - val_acc: 0.7428\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4979 - acc: 0.7491 - val_loss: 0.4894 - val_acc: 0.7521\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4889 - acc: 0.7549 - val_loss: 0.4906 - val_acc: 0.7546\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4810 - acc: 0.7611 - val_loss: 0.4864 - val_acc: 0.7556\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4740 - acc: 0.7653 - val_loss: 0.4655 - val_acc: 0.7699\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4676 - acc: 0.7687 - val_loss: 0.4605 - val_acc: 0.7753\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4623 - acc: 0.7724 - val_loss: 0.4647 - val_acc: 0.7700\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4572 - acc: 0.7755 - val_loss: 0.4605 - val_acc: 0.7731\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4522 - acc: 0.7786 - val_loss: 0.4515 - val_acc: 0.7797\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4478 - acc: 0.7819 - val_loss: 0.4455 - val_acc: 0.7831\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4436 - acc: 0.7835 - val_loss: 0.4469 - val_acc: 0.7812\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4402 - acc: 0.7865 - val_loss: 0.4399 - val_acc: 0.7873\n",
      "Epoch 18/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4374 - acc: 0.7877 - val_loss: 0.4371 - val_acc: 0.7896\n",
      "Epoch 19/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4332 - acc: 0.7893 - val_loss: 0.4564 - val_acc: 0.7729\n",
      "Epoch 20/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4300 - acc: 0.7921 - val_loss: 0.4442 - val_acc: 0.7827\n",
      "Best validation accuracy of eval run: 0.7896077526556384\n",
      "history of eval run: {'val_loss': [0.5744380724694138, 0.5508073311711326, 0.5541280441422808, 0.5204367717466544, 0.5099376303609418, 0.5116349514717873, 0.48939787912819865, 0.490572087072699, 0.4863869982988096, 0.46554183970798146, 0.46046445674867637, 0.4646524905421297, 0.46046396619612356, 0.451541249802655, 0.44546406263407334, 0.44687737895862595, 0.4398771549167862, 0.4371062238234554, 0.4563500270328601, 0.4441549328719079], 'val_acc': [0.6971481310589609, 0.7151638209517232, 0.707448084895711, 0.7357452699430648, 0.7375726811094773, 0.7428149515470163, 0.7520627595776503, 0.754573142578044, 0.7556068297031017, 0.7699123211835571, 0.7753391785736067, 0.7699861559606006, 0.7731425934482622, 0.7796769727756199, 0.7831472081064242, 0.7812275034456029, 0.7872634979255968, 0.7896077526556384, 0.7728657129518327, 0.7826857406563841], 'loss': [0.6083994519179841, 0.5688718510748896, 0.5471373912706057, 0.5333893155265734, 0.5197306072640374, 0.5073068825020892, 0.4978839069587132, 0.48894236970268096, 0.48097725930720897, 0.47401259318440747, 0.46757115108508895, 0.46231795009191656, 0.45720109673614323, 0.4521904712588457, 0.4478380166986205, 0.443637281365527, 0.44018367737336506, 0.4374147378398833, 0.4331971867887447, 0.42996341559908025], 'acc': [0.657904816404002, 0.6994513075856186, 0.7158751620998431, 0.7268167072066052, 0.736092331688673, 0.7444080905590792, 0.7490920501835702, 0.7548927765491944, 0.7610672967317536, 0.7652897825068219, 0.7686816153466585, 0.7723641767012422, 0.7754698957467298, 0.7786171474391851, 0.7818936118110961, 0.7835364587394544, 0.7864852766802256, 0.7876528055358039, 0.7893233408935487, 0.792064495583527]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 38s 174us/step - loss: 10.0520 - acc: 0.3694 - val_loss: 10.0342 - val_acc: 0.3706\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 10.0540 - acc: 0.3694 - val_loss: 10.0342 - val_acc: 0.3706\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 10.0540 - acc: 0.3694 - val_loss: 10.0342 - val_acc: 0.3706\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 10.0540 - acc: 0.3694 - val_loss: 10.0342 - val_acc: 0.3706\n",
      "Best validation accuracy of eval run: 0.3705952930274404\n",
      "history of eval run: {'val_loss': [10.034212675688726, 10.034212675688726, 10.034212675688726, 10.034212675688726], 'val_acc': [0.3705952930274404, 0.3705952930274404, 0.3705952930274404, 0.3705952930274404], 'loss': [10.051977746308877, 10.053994761357542, 10.053994763870488, 10.053994762616215], 'acc': [0.36944673899183905, 0.36935444422444386, 0.36935444422272473, 0.36935444422698815]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 41s 188us/step - loss: 10.0518 - acc: 0.3694 - val_loss: 10.0342 - val_acc: 0.3706\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 40s 184us/step - loss: 10.0540 - acc: 0.3694 - val_loss: 10.0342 - val_acc: 0.3706\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 40s 184us/step - loss: 10.0540 - acc: 0.3694 - val_loss: 10.0342 - val_acc: 0.3706\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 40s 185us/step - loss: 10.0540 - acc: 0.3694 - val_loss: 10.0342 - val_acc: 0.3706\n",
      "Best validation accuracy of eval run: 0.3705952930274404\n",
      "history of eval run: {'val_loss': [10.034212675688726, 10.034212675688726, 10.034212675688726, 10.034212675688726], 'val_acc': [0.3705952930274404, 0.3705952930274404, 0.3705952930274404, 0.3705952930274404], 'loss': [10.051770294223182, 10.053994762312549, 10.05399476218492, 10.053994763179539], 'acc': [0.3693636737017541, 0.36935444422093683, 0.3693544442247877, 0.36935444422657554]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 38s 174us/step - loss: 5.9515 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 37s 171us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186, 5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597], 'loss': [5.95149227848855, 5.953290205510978, 5.953290204476753, 5.953290203079448], 'acc': [0.6306270968152281, 0.6306455557697112, 0.630645555769436, 0.6306455557771378]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 36s 166us/step - loss: 0.6197 - acc: 0.6483 - val_loss: 0.5970 - val_acc: 0.6909\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5871 - acc: 0.6894 - val_loss: 0.6019 - val_acc: 0.6631\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5683 - acc: 0.7067 - val_loss: 0.5865 - val_acc: 0.6746\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5537 - acc: 0.7184 - val_loss: 0.5375 - val_acc: 0.7279\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5431 - acc: 0.7257 - val_loss: 0.5277 - val_acc: 0.7346\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5340 - acc: 0.7318 - val_loss: 0.5243 - val_acc: 0.7366\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5266 - acc: 0.7354 - val_loss: 0.5120 - val_acc: 0.7446\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5203 - acc: 0.7399 - val_loss: 0.5199 - val_acc: 0.7374\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5126 - acc: 0.7446 - val_loss: 0.4976 - val_acc: 0.7535\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5079 - acc: 0.7472 - val_loss: 0.5112 - val_acc: 0.7401\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5035 - acc: 0.7508 - val_loss: 0.4968 - val_acc: 0.7536\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4982 - acc: 0.7534 - val_loss: 0.4853 - val_acc: 0.7604\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4937 - acc: 0.7564 - val_loss: 0.4848 - val_acc: 0.7606\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4889 - acc: 0.7589 - val_loss: 0.4790 - val_acc: 0.7642\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4855 - acc: 0.7617 - val_loss: 0.4818 - val_acc: 0.7633\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4817 - acc: 0.7630 - val_loss: 0.4749 - val_acc: 0.7665\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4788 - acc: 0.7641 - val_loss: 0.4781 - val_acc: 0.7637\n",
      "Epoch 18/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4755 - acc: 0.7678 - val_loss: 0.4675 - val_acc: 0.7712\n",
      "Epoch 19/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4714 - acc: 0.7702 - val_loss: 0.4629 - val_acc: 0.7727\n",
      "Epoch 20/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4682 - acc: 0.7713 - val_loss: 0.4622 - val_acc: 0.7760\n",
      "Best validation accuracy of eval run: 0.775985233043271\n",
      "history of eval run: {'val_loss': [0.5969965202004116, 0.6018859312949116, 0.5864869086039358, 0.5374572026900118, 0.5277036514538579, 0.5243450712233465, 0.5120468223160953, 0.5198929502807788, 0.49757911244844805, 0.5112208052923671, 0.4967679300036472, 0.48525996439182983, 0.4847601412324021, 0.47899101678104955, 0.48175842394958623, 0.4749383098219857, 0.4781464463205562, 0.4675453885024079, 0.46285662955108964, 0.462216331857], 'val_acc': [0.6908537148164063, 0.6630733733326816, 0.6746285186938333, 0.7278634056144999, 0.73460083064254, 0.7365574526841815, 0.7445869866020404, 0.7373511767277368, 0.7534840793570011, 0.740083064129675, 0.7536317489473953, 0.7604245500549172, 0.7606275957566998, 0.7642270419781363, 0.7632856483474876, 0.7664790032159694, 0.763728657132973, 0.7712228887885411, 0.7727365020799043, 0.775985233043271], 'loss': [0.6196817137568155, 0.587094446576909, 0.5682562771641487, 0.5536936184345281, 0.5431248912260191, 0.5340226036196624, 0.5265651306332022, 0.5203085660949852, 0.5126071624709551, 0.5079252042817949, 0.5035014056951809, 0.4981995861527839, 0.49374305305514754, 0.48894435104323913, 0.4854758215707416, 0.4816840906545315, 0.47877539684097636, 0.4755297799809329, 0.4714002346309464, 0.468225594097419], 'acc': [0.6483338486392035, 0.6893773333343319, 0.7067241355512524, 0.7183901946026996, 0.7257137846811277, 0.7318190837873324, 0.7353955061728242, 0.7398948762491843, 0.7446388274946554, 0.7471861631615994, 0.7508087329386623, 0.7534022159971776, 0.756406410802223, 0.7589214433012286, 0.7617364338217917, 0.76295472479374, 0.7641360978630488, 0.7678463476706743, 0.7702090938112172, 0.7712981721037099]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 36s 166us/step - loss: 0.6232 - acc: 0.6405 - val_loss: 0.6214 - val_acc: 0.6546\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5917 - acc: 0.6860 - val_loss: 0.5688 - val_acc: 0.7070\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5716 - acc: 0.7056 - val_loss: 0.5522 - val_acc: 0.7167\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5576 - acc: 0.7165 - val_loss: 0.5425 - val_acc: 0.7266\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5463 - acc: 0.7243 - val_loss: 0.5305 - val_acc: 0.7352\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5366 - acc: 0.7311 - val_loss: 0.5213 - val_acc: 0.7379\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5281 - acc: 0.7372 - val_loss: 0.5151 - val_acc: 0.7414\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5206 - acc: 0.7406 - val_loss: 0.5036 - val_acc: 0.7485\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5147 - acc: 0.7454 - val_loss: 0.5143 - val_acc: 0.7390\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5090 - acc: 0.7479 - val_loss: 0.5038 - val_acc: 0.7483\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5044 - acc: 0.7514 - val_loss: 0.4959 - val_acc: 0.7537\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4998 - acc: 0.7531 - val_loss: 0.4949 - val_acc: 0.7555\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4949 - acc: 0.7566 - val_loss: 0.4850 - val_acc: 0.7618\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4916 - acc: 0.7590 - val_loss: 0.5166 - val_acc: 0.7398\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4869 - acc: 0.7606 - val_loss: 0.4843 - val_acc: 0.7611\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4848 - acc: 0.7621 - val_loss: 0.4741 - val_acc: 0.7693\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4815 - acc: 0.7647 - val_loss: 0.4743 - val_acc: 0.7708\n",
      "Epoch 18/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4782 - acc: 0.7669 - val_loss: 0.4815 - val_acc: 0.7625\n",
      "Epoch 19/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4759 - acc: 0.7672 - val_loss: 0.4650 - val_acc: 0.7712\n",
      "Epoch 20/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4735 - acc: 0.7695 - val_loss: 0.4855 - val_acc: 0.7579\n",
      "Best validation accuracy of eval run: 0.7711859713901174\n",
      "history of eval run: {'val_loss': [0.6214191605200897, 0.568839331057196, 0.5522274050478237, 0.5424543841153617, 0.530520199693509, 0.5213398556556358, 0.5151238317356497, 0.5035654814594288, 0.5143421832361033, 0.5038222967021522, 0.4958580070483723, 0.4949376761066127, 0.4849991161981705, 0.5166275854599217, 0.48429766493895443, 0.4741426600643056, 0.4742636814670844, 0.4814719769340756, 0.46503821143809143, 0.48548836029450576], 'val_acc': [0.6546192893445024, 0.7070050761443324, 0.7166774342265831, 0.7266082141055014, 0.7352099676818742, 0.7379418550838124, 0.7413567143362351, 0.7485002307172299, 0.7389570835102083, 0.7483156437493163, 0.7536871250598839, 0.7554776188124694, 0.7617720350726277, 0.7398061836497488, 0.761144439301625, 0.7692847254290579, 0.7707983387193209, 0.7624550069242124, 0.7711859713901174, 0.7579141670545235], 'loss': [0.6232251139489362, 0.5917329918404838, 0.5716106159923363, 0.5576029680518196, 0.5462756229969031, 0.5366035957609824, 0.5280855428941905, 0.5205936633294592, 0.5146745509760068, 0.5089685305800161, 0.5043751953067871, 0.49984659792671154, 0.4949378391749922, 0.4915600923820582, 0.48688254020929533, 0.48482640862606285, 0.481485261939042, 0.47823766683506413, 0.4759389910755887, 0.4735119573269729], 'acc': [0.6404934078470788, 0.6859624268039358, 0.7055658361650764, 0.7164704633595717, 0.7242924452109665, 0.7310576519342312, 0.7371767952481151, 0.7406378491601802, 0.7454141035653379, 0.7478506855227126, 0.7513671162996021, 0.7530930285100685, 0.7566371477259716, 0.7590414265035802, 0.7605965933936746, 0.7621009981695992, 0.7646898664991895, 0.7668957115171373, 0.767177210580636, 0.769516883014666]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 36s 165us/step - loss: 3.2537 - acc: 0.6029 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 2.4023 - acc: 0.6153 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 2.3859 - acc: 0.6157 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 2.2586 - acc: 0.6161 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186, 5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597], 'loss': [3.2537463630644767, 2.402344305951523, 2.385902643495299, 2.2586364111805732], 'acc': [0.6028832886505824, 0.6152877058821752, 0.6156845733896694, 0.6160629819509323]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 36s 165us/step - loss: 0.6200 - acc: 0.6471 - val_loss: 0.5852 - val_acc: 0.6925\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5810 - acc: 0.6957 - val_loss: 0.5613 - val_acc: 0.7090\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5568 - acc: 0.7147 - val_loss: 0.5398 - val_acc: 0.7236\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5451 - acc: 0.7226 - val_loss: 0.5628 - val_acc: 0.7025\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5301 - acc: 0.7334 - val_loss: 0.5208 - val_acc: 0.7358\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5191 - acc: 0.7401 - val_loss: 0.5059 - val_acc: 0.7448\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5065 - acc: 0.7480 - val_loss: 0.4908 - val_acc: 0.7530\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4991 - acc: 0.7525 - val_loss: 0.4847 - val_acc: 0.7597\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4904 - acc: 0.7583 - val_loss: 0.4812 - val_acc: 0.7610\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4836 - acc: 0.7622 - val_loss: 0.5113 - val_acc: 0.7382\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4787 - acc: 0.7663 - val_loss: 0.4664 - val_acc: 0.7714\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4716 - acc: 0.7703 - val_loss: 0.4623 - val_acc: 0.7761\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4664 - acc: 0.7738 - val_loss: 0.4578 - val_acc: 0.7806\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4618 - acc: 0.7761 - val_loss: 0.4589 - val_acc: 0.7795\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4577 - acc: 0.7800 - val_loss: 0.4515 - val_acc: 0.7830\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4533 - acc: 0.7825 - val_loss: 0.4489 - val_acc: 0.7826\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4494 - acc: 0.7854 - val_loss: 0.4457 - val_acc: 0.7863\n",
      "Epoch 18/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4456 - acc: 0.7875 - val_loss: 0.4593 - val_acc: 0.7713\n",
      "Epoch 19/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4421 - acc: 0.7901 - val_loss: 0.4392 - val_acc: 0.7911\n",
      "Epoch 20/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4380 - acc: 0.7918 - val_loss: 0.4344 - val_acc: 0.7937\n",
      "Best validation accuracy of eval run: 0.7936502076449568\n",
      "history of eval run: {'val_loss': [0.5852050808546562, 0.5613337258259492, 0.539838694745514, 0.5628405134662484, 0.5208463043670364, 0.505862704416437, 0.4907754278254498, 0.48467478332455716, 0.48116428961058505, 0.5113185184740173, 0.46636596972952254, 0.4623475068573804, 0.45775107800052856, 0.4589205878013062, 0.45149344504952266, 0.4488717865784376, 0.44568811646627843, 0.45929177222133066, 0.43923144186593777, 0.4344494844060589], 'val_acc': [0.692496538979698, 0.7089986155821972, 0.7236363636385641, 0.702538071051687, 0.7358006460555534, 0.7448084909859812, 0.7530226119069608, 0.7597231195046706, 0.7610336871261579, 0.7381818181675153, 0.77142593447272, 0.7760959852341413, 0.780636825088427, 0.7794923857890025, 0.7829810798184685, 0.782630364561499, 0.7863221042938479, 0.7713336409816117, 0.7911398246280598, 0.7936502076449568], 'loss': [0.6199831209298287, 0.5810396127133258, 0.5567611036673883, 0.5450599453279009, 0.5301391507790746, 0.5191390863069905, 0.506516702867565, 0.4990636575506262, 0.49043760386568525, 0.48364443509807126, 0.47870610136945885, 0.47164100311347923, 0.4663804128618284, 0.46180575427454756, 0.45774200173922447, 0.45332036659007985, 0.44939475713186267, 0.44558397518576087, 0.44213245898383274, 0.43803380560405025], 'acc': [0.6470555660644289, 0.6957364430493433, 0.7147491658854566, 0.7225942214257605, 0.7333511769885176, 0.7400563921064806, 0.748035275055165, 0.7525161862028977, 0.7583122978184176, 0.7621932929384384, 0.7663465576333016, 0.7702783148853711, 0.773776286709978, 0.7760513528180332, 0.7800061837436083, 0.7825304457353962, 0.7854469605079539, 0.7875005191633132, 0.7901493790885782, 0.7917783818010055]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 36s 166us/step - loss: 0.6369 - acc: 0.6289 - val_loss: 0.6284 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6133 - acc: 0.6495 - val_loss: 0.6187 - val_acc: 0.6851\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6064 - acc: 0.6682 - val_loss: 0.6128 - val_acc: 0.6910\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5851 - acc: 0.6979 - val_loss: 0.6161 - val_acc: 0.6763\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5765 - acc: 0.7065 - val_loss: 0.5959 - val_acc: 0.7010\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5684 - acc: 0.7128 - val_loss: 0.6381 - val_acc: 0.6138\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5611 - acc: 0.7178 - val_loss: 0.5934 - val_acc: 0.6940\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5566 - acc: 0.7210 - val_loss: 0.5986 - val_acc: 0.6867\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5500 - acc: 0.7256 - val_loss: 0.6262 - val_acc: 0.6242\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5445 - acc: 0.7296 - val_loss: 0.5816 - val_acc: 0.6996\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5402 - acc: 0.7306 - val_loss: 0.5856 - val_acc: 0.6891\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5357 - acc: 0.7346 - val_loss: 0.5651 - val_acc: 0.7099\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5332 - acc: 0.7351 - val_loss: 0.6129 - val_acc: 0.6366\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5300 - acc: 0.7387 - val_loss: 0.5617 - val_acc: 0.7190\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5262 - acc: 0.7405 - val_loss: 0.5767 - val_acc: 0.6961\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5231 - acc: 0.7423 - val_loss: 0.5567 - val_acc: 0.7146\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5191 - acc: 0.7434 - val_loss: 0.5741 - val_acc: 0.6885\n",
      "Epoch 18/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5176 - acc: 0.7457 - val_loss: 0.5994 - val_acc: 0.6520\n",
      "Epoch 19/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5145 - acc: 0.7470 - val_loss: 0.6159 - val_acc: 0.6274\n",
      "Best validation accuracy of eval run: 0.7189847715604014\n",
      "history of eval run: {'val_loss': [0.6283944649315673, 0.6187032193734892, 0.6128157447052266, 0.6161007128611213, 0.5958601409749716, 0.6380647256355655, 0.5933770639543221, 0.5986493505142116, 0.6262362677378758, 0.581584538181201, 0.5856475520398026, 0.565068078572485, 0.6129085656548074, 0.5617197605883629, 0.5766541567286644, 0.5567432027623354, 0.5740605396029583, 0.5993523584349562, 0.6158826819696237], 'val_acc': [0.6294047069725597, 0.6851499769299273, 0.6909829257081388, 0.6762898015744905, 0.7010059990638624, 0.6138071066055861, 0.6939916935889029, 0.6867005076175139, 0.6242178126497097, 0.6996031379820732, 0.6890632210297137, 0.7099030918353265, 0.6365666820379133, 0.7189847715604014, 0.6961329026347655, 0.7145916012965098, 0.6884910013888033, 0.651979695435873, 0.6274480848979114], 'loss': [0.6368707037635161, 0.6133292221504577, 0.6063833726276118, 0.5851174110228008, 0.5764578106763162, 0.5684232549542437, 0.561134666429314, 0.5565868812796676, 0.5500051128550005, 0.5445290771229441, 0.5401690632842446, 0.5356769241212826, 0.5332365798680196, 0.529962718560777, 0.526166344502922, 0.5231047811463927, 0.5190812619816068, 0.5175786643436844, 0.5145047767085075], 'acc': [0.6288781109059504, 0.649529065932145, 0.6682326012852751, 0.6979422880791187, 0.706451865967058, 0.7127555988249468, 0.7178087375531259, 0.7209652187149835, 0.7255707277898669, 0.7295670913778134, 0.7306423254543751, 0.734569467960466, 0.7351278513351586, 0.7386673557984815, 0.7404578743563776, 0.7423129992625283, 0.7433513154345249, 0.7457325205292759, 0.7470384915315115]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 36s 166us/step - loss: 0.6306 - acc: 0.6389 - val_loss: 0.7142 - val_acc: 0.3942\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6099 - acc: 0.6619 - val_loss: 0.7238 - val_acc: 0.3838\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6015 - acc: 0.6727 - val_loss: 0.7478 - val_acc: 0.3828\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5949 - acc: 0.6794 - val_loss: 0.7618 - val_acc: 0.3866\n",
      "Best validation accuracy of eval run: 0.39416705121793777\n",
      "history of eval run: {'val_loss': [0.7142094437648032, 0.7237830378088359, 0.7477583191666561, 0.7618085361748287], 'val_acc': [0.39416705121793777, 0.3838117212692493, 0.3827964928428534, 0.3865620673698492], 'loss': [0.6305523327863705, 0.6099100238457856, 0.6014717990508957, 0.594943506318339], 'acc': [0.6388967083023148, 0.6618504178598998, 0.6727273566291345, 0.6793633506627664]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 41s 188us/step - loss: 5.9521 - acc: 0.6305 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 40s 184us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 40s 185us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 40s 185us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186, 5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597], 'loss': [5.95206526202809, 5.953290195335963, 5.9532901986212785, 5.953290199437657], 'acc': [0.6305486462636952, 0.630645555773287, 0.6306455557774128, 0.630645555782914]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 36s 165us/step - loss: 0.6217 - acc: 0.6437 - val_loss: 0.6519 - val_acc: 0.6003\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5916 - acc: 0.6839 - val_loss: 0.6065 - val_acc: 0.6639\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5786 - acc: 0.6966 - val_loss: 0.5793 - val_acc: 0.7038\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5689 - acc: 0.7045 - val_loss: 0.5590 - val_acc: 0.7159\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5608 - acc: 0.7122 - val_loss: 0.5822 - val_acc: 0.6982\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5540 - acc: 0.7170 - val_loss: 0.5930 - val_acc: 0.6940\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5488 - acc: 0.7215 - val_loss: 0.5594 - val_acc: 0.7249\n",
      "Best validation accuracy of eval run: 0.7248730964500012\n",
      "history of eval run: {'val_loss': [0.6518887415997268, 0.6064759760547686, 0.5793086947523398, 0.5589796693136905, 0.5821985831489528, 0.5929607193097025, 0.5594215877781169], 'val_acc': [0.6002584217878658, 0.6639040147713599, 0.7038117212769509, 0.7159021688992933, 0.6982187355846428, 0.6940470696881887, 0.7248730964500012], 'loss': [0.6216808059407241, 0.5916295839120574, 0.578641714371479, 0.5688672289193487, 0.5608190975649131, 0.5539699055321953, 0.5488400522349772], 'acc': [0.6436868069212032, 0.6839365565818138, 0.6966455465479358, 0.7044629136473006, 0.7122433628536529, 0.7170103877759304, 0.721546675772534]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 36s 166us/step - loss: 0.6177 - acc: 0.6529 - val_loss: 0.7233 - val_acc: 0.4234\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5925 - acc: 0.6830 - val_loss: 0.7367 - val_acc: 0.4423\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5793 - acc: 0.6958 - val_loss: 0.6765 - val_acc: 0.5826\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5712 - acc: 0.7016 - val_loss: 0.7341 - val_acc: 0.5345\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5648 - acc: 0.7074 - val_loss: 0.7598 - val_acc: 0.5322\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5600 - acc: 0.7125 - val_loss: 0.7551 - val_acc: 0.5323\n",
      "Best validation accuracy of eval run: 0.5825749884534113\n",
      "history of eval run: {'val_loss': [0.7232875939828215, 0.7367411198294762, 0.6765375110879712, 0.7341027447535907, 0.759766840324129, 0.7551353149838749], 'val_acc': [0.4234240885946021, 0.44227041992879323, 0.5825749884534113, 0.5344900784417678, 0.5321827411079495, 0.532311951999682], 'loss': [0.6176587007200076, 0.5924912995439803, 0.5792906662784375, 0.5711972443815533, 0.5647651706252306, 0.559974654384069], 'acc': [0.6528701366479097, 0.682962846735517, 0.6958425820360391, 0.7015648578501305, 0.7074301905378788, 0.712506402954967]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 41s 188us/step - loss: 5.9519 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 40s 184us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 40s 184us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 40s 185us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186, 5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597], 'loss': [5.951922557222792, 5.953290205051077, 5.953290197666271, 5.953290203350107], 'acc': [0.63061786734225, 0.6306455557771378, 0.6306455557752123, 0.630645555769436]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 36s 166us/step - loss: 0.6474 - acc: 0.6280 - val_loss: 0.6679 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6270 - acc: 0.6306 - val_loss: 0.6731 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6163 - acc: 0.6391 - val_loss: 0.6657 - val_acc: 0.6822\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6089 - acc: 0.6706 - val_loss: 0.6819 - val_acc: 0.5213\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6042 - acc: 0.6801 - val_loss: 0.6808 - val_acc: 0.5416\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6013 - acc: 0.6851 - val_loss: 0.6591 - val_acc: 0.6231\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5985 - acc: 0.6889 - val_loss: 0.6647 - val_acc: 0.5933\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5966 - acc: 0.6900 - val_loss: 0.6781 - val_acc: 0.5375\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5941 - acc: 0.6915 - val_loss: 0.6680 - val_acc: 0.5772\n",
      "Best validation accuracy of eval run: 0.6821596677456245\n",
      "history of eval run: {'val_loss': [0.6678662288678534, 0.6730673499815906, 0.6657083043691251, 0.6819477052197899, 0.6807993755045716, 0.659052999266073, 0.6647499450715281, 0.6780605167807882, 0.6680378375902779], 'val_acc': [0.6294047069725597, 0.6294047069725597, 0.6821596677456245, 0.5212736501988585, 0.5415597600446189, 0.6231102907311052, 0.5932810336772231, 0.5375173050376969, 0.5772404245577708], 'loss': [0.6474475646343333, 0.6270155206297846, 0.6162758984378122, 0.6089211582332688, 0.604197268876949, 0.601293994003375, 0.5985059013582764, 0.5966078664613773, 0.5940609216415553], 'acc': [0.6280013105851986, 0.63064094103501, 0.6391274452282638, 0.6705768884597825, 0.6800832498906294, 0.6851133149122957, 0.6888927857844478, 0.690037240937639, 0.6914954983239179]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 36s 166us/step - loss: 0.6216 - acc: 0.6426 - val_loss: 0.5999 - val_acc: 0.6807\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5879 - acc: 0.6845 - val_loss: 0.5844 - val_acc: 0.6778\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5685 - acc: 0.7022 - val_loss: 0.5537 - val_acc: 0.7126\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5544 - acc: 0.7138 - val_loss: 0.5428 - val_acc: 0.7212\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5451 - acc: 0.7208 - val_loss: 0.5398 - val_acc: 0.7194\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5367 - acc: 0.7264 - val_loss: 0.5254 - val_acc: 0.7340\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5297 - acc: 0.7320 - val_loss: 0.5197 - val_acc: 0.7376\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5234 - acc: 0.7360 - val_loss: 0.5120 - val_acc: 0.7447\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5158 - acc: 0.7409 - val_loss: 0.5093 - val_acc: 0.7434\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5088 - acc: 0.7459 - val_loss: 0.4976 - val_acc: 0.7520\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5021 - acc: 0.7493 - val_loss: 0.4977 - val_acc: 0.7514\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4963 - acc: 0.7529 - val_loss: 0.4892 - val_acc: 0.7552\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4916 - acc: 0.7557 - val_loss: 0.4823 - val_acc: 0.7584\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4864 - acc: 0.7590 - val_loss: 0.4941 - val_acc: 0.7488\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4824 - acc: 0.7622 - val_loss: 0.4802 - val_acc: 0.7643\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4783 - acc: 0.7648 - val_loss: 0.4710 - val_acc: 0.7670\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4747 - acc: 0.7664 - val_loss: 0.4700 - val_acc: 0.7697\n",
      "Epoch 18/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4708 - acc: 0.7685 - val_loss: 0.4716 - val_acc: 0.7688\n",
      "Epoch 19/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4672 - acc: 0.7706 - val_loss: 0.4624 - val_acc: 0.7756\n",
      "Epoch 20/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4631 - acc: 0.7736 - val_loss: 0.4585 - val_acc: 0.7767\n",
      "Best validation accuracy of eval run: 0.776742039670799\n",
      "history of eval run: {'val_loss': [0.5999232410746452, 0.584409442183385, 0.5536535550808653, 0.5427816183274388, 0.5397750787890117, 0.5254163896421381, 0.5196972366924049, 0.5119728630052275, 0.5093344707751968, 0.49760114159185764, 0.49769797824691064, 0.4891719947430083, 0.4822736976126894, 0.4940948614427226, 0.4801843609961706, 0.47099409267491993, 0.4700488683772516, 0.4716326043151412, 0.46243812810841056, 0.4584543854793096], 'val_acc': [0.6806829718559857, 0.67776649746743, 0.7125611444239139, 0.7211813566989485, 0.719427780327183, 0.7339916935878027, 0.737609598507901, 0.7446977388116144, 0.7434240886028538, 0.7519889247642996, 0.7514167051233891, 0.7551822796338816, 0.758431010614852, 0.7488140286131834, 0.7642824180741216, 0.766995846777398, 0.7697277341793362, 0.768767881866529, 0.7755976003537708, 0.776742039670799], 'loss': [0.6215730253019353, 0.5878511905216868, 0.5685078739960949, 0.5543687275091005, 0.5451369323655906, 0.5366933613969782, 0.5297185244118804, 0.5234233283872035, 0.5158469566924114, 0.5087539473066239, 0.5020893144618931, 0.49628447975184403, 0.49156984682450544, 0.4864075801065583, 0.48237371379788024, 0.4783083548663981, 0.4746588075969711, 0.4707518956351804, 0.467214744955283, 0.4630624852094684], 'acc': [0.6426300317966489, 0.6845180136256113, 0.7022063064973042, 0.713756997094854, 0.720771399699926, 0.7263736922921364, 0.7319575259419665, 0.7360461843009527, 0.7408962745289936, 0.745861733197179, 0.7493043281649383, 0.7529315126588236, 0.7556911263247396, 0.7589675906867485, 0.7622302108449289, 0.7647683170485219, 0.7663788608012401, 0.768501640542856, 0.7706290350175226, 0.7736009266450025]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 39s 181us/step - loss: 5.9521 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 38s 177us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 38s 178us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 38s 177us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186, 5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597], 'loss': [5.952062090689217, 5.953290200975791, 5.953290204566972, 5.953290197917125], 'acc': [0.6306317115557055, 0.6306455557752123, 0.6306455557752123, 0.6306455557774128]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 41s 188us/step - loss: 5.9515 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 40s 185us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 40s 185us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 40s 184us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186, 5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597], 'loss': [5.9514846476307, 5.953290196957717, 5.953290204815627, 5.953290195952098], 'acc': [0.630645555773287, 0.630645555769436, 0.6306455557812637, 0.6306455557793382]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 36s 166us/step - loss: 0.6303 - acc: 0.6321 - val_loss: 0.6250 - val_acc: 0.6779\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6015 - acc: 0.6776 - val_loss: 0.6058 - val_acc: 0.6907\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5903 - acc: 0.6910 - val_loss: 0.5793 - val_acc: 0.7017\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5747 - acc: 0.7048 - val_loss: 0.5699 - val_acc: 0.7112\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5646 - acc: 0.7123 - val_loss: 0.5615 - val_acc: 0.7172\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5541 - acc: 0.7212 - val_loss: 0.5510 - val_acc: 0.7295\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5423 - acc: 0.7289 - val_loss: 0.5407 - val_acc: 0.7370\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5353 - acc: 0.7342 - val_loss: 0.5511 - val_acc: 0.7226\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5267 - acc: 0.7400 - val_loss: 0.5520 - val_acc: 0.7153\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5204 - acc: 0.7438 - val_loss: 0.5282 - val_acc: 0.7390\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5149 - acc: 0.7483 - val_loss: 0.5438 - val_acc: 0.7193\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5090 - acc: 0.7509 - val_loss: 0.5159 - val_acc: 0.7475\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5046 - acc: 0.7549 - val_loss: 0.5096 - val_acc: 0.7525\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4989 - acc: 0.7576 - val_loss: 0.5196 - val_acc: 0.7418\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4948 - acc: 0.7611 - val_loss: 0.5321 - val_acc: 0.7261\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4908 - acc: 0.7635 - val_loss: 0.5334 - val_acc: 0.7249\n",
      "Best validation accuracy of eval run: 0.7525242270287908\n",
      "history of eval run: {'val_loss': [0.624958750933175, 0.6058153552375304, 0.5793481574621847, 0.5698862886813911, 0.56153819204477, 0.550983225789332, 0.540719196059267, 0.5511377650304055, 0.5519599114000164, 0.5281729174138876, 0.5437933777415043, 0.5159190963477542, 0.5095808323842345, 0.5196173603313553, 0.5320867149154327, 0.5333735329102524], 'val_acc': [0.6779141670380202, 0.6907245039257741, 0.7016520535324265, 0.7112321181202683, 0.7172127365042771, 0.7295431472103223, 0.7369820027710052, 0.7226026764981033, 0.7152745731293908, 0.7390493770222208, 0.7193170281528162, 0.7474665436119763, 0.7525242270287908, 0.7418366405036408, 0.7260544531643528, 0.7248546377513394], 'loss': [0.6303057287775962, 0.6014884552416532, 0.5903269599701311, 0.5747326816963031, 0.564624921952579, 0.5540600932971823, 0.5422926341194498, 0.5353115410641517, 0.5266562100181779, 0.5203648707644717, 0.5149119061555661, 0.5089592956545266, 0.504603119551831, 0.49886325560184386, 0.4947588479901719, 0.49078444985051], 'acc': [0.6320899689362081, 0.6775820615836247, 0.6910294097304421, 0.704767486397508, 0.7122710512791885, 0.721223644070044, 0.7288610363755085, 0.7341679857179956, 0.7399964005072301, 0.7438404777191102, 0.7483352330668205, 0.7509379456139922, 0.7549435386771172, 0.7576016280833369, 0.7611319030714814, 0.7635223376414108]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 36s 166us/step - loss: 0.6169 - acc: 0.6492 - val_loss: 0.6276 - val_acc: 0.6407\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5849 - acc: 0.6901 - val_loss: 0.6072 - val_acc: 0.6579\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5720 - acc: 0.7015 - val_loss: 0.5751 - val_acc: 0.6978\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5626 - acc: 0.7093 - val_loss: 0.6553 - val_acc: 0.6129\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5544 - acc: 0.7160 - val_loss: 0.5448 - val_acc: 0.7244\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5502 - acc: 0.7203 - val_loss: 0.5743 - val_acc: 0.7058\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5447 - acc: 0.7244 - val_loss: 0.5501 - val_acc: 0.7256\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5390 - acc: 0.7297 - val_loss: 0.5864 - val_acc: 0.6939\n",
      "Best validation accuracy of eval run: 0.7256483617926942\n",
      "history of eval run: {'val_loss': [0.6276449553670562, 0.6072013780297252, 0.575086754145136, 0.655335595300795, 0.5448027277663935, 0.5743003997650904, 0.5501448279324906, 0.5864040636136117], 'val_acc': [0.6407198892522089, 0.6578680203100696, 0.6977572681158988, 0.6129395477519808, 0.7243931702660921, 0.7058421781308429, 0.7256483617926942, 0.6938624826993709], 'loss': [0.6168871118467222, 0.5849389758728469, 0.5719721823533417, 0.5625726319863072, 0.5544101803440141, 0.5501667944581375, 0.5447020801486105, 0.5389661099277361], 'acc': [0.6492198784472364, 0.6900556998995486, 0.7015094809817304, 0.7093176186097675, 0.7159766863463366, 0.7203237700504811, 0.7243893547285349, 0.7296547714023244]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 39s 181us/step - loss: 5.9516 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 38s 177us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 38s 178us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 39s 178us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186, 5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597], 'loss': [5.951628409893697, 5.9532901996092935, 5.953290195694642, 5.953290203471134], 'acc': [0.6306409410347349, 0.6306455557713615, 0.6306455557752123, 0.6306455557793382]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 41s 187us/step - loss: 5.9521 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 40s 184us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 40s 184us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 40s 184us/step - loss: 5.9534 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [5.973290318646186, 5.973290318646186, 5.973290318646186, 5.973290318646186], 'val_acc': [0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597], 'loss': [5.952112997369344, 5.953290197364805, 5.953290194493181, 5.9533637746979915], 'acc': [0.6305809494396104, 0.630645555769436, 0.630645555769436, 0.6306409410385858]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 36s 166us/step - loss: 0.6186 - acc: 0.6453 - val_loss: 0.6005 - val_acc: 0.6510\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 35s 164us/step - loss: 0.5973 - acc: 0.6681 - val_loss: 0.5803 - val_acc: 0.6915\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5837 - acc: 0.6831 - val_loss: 0.5768 - val_acc: 0.6904\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 35s 164us/step - loss: 0.5742 - acc: 0.6925 - val_loss: 0.5621 - val_acc: 0.7041\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5662 - acc: 0.7005 - val_loss: 0.5556 - val_acc: 0.7104\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5594 - acc: 0.7060 - val_loss: 0.5505 - val_acc: 0.7147\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5536 - acc: 0.7113 - val_loss: 0.5439 - val_acc: 0.7195\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5484 - acc: 0.7159 - val_loss: 0.5424 - val_acc: 0.7210\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5430 - acc: 0.7197 - val_loss: 0.5372 - val_acc: 0.7236\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5390 - acc: 0.7233 - val_loss: 0.5340 - val_acc: 0.7264\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5344 - acc: 0.7263 - val_loss: 0.5283 - val_acc: 0.7298\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5298 - acc: 0.7298 - val_loss: 0.5228 - val_acc: 0.7342\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5261 - acc: 0.7318 - val_loss: 0.5181 - val_acc: 0.7384\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5217 - acc: 0.7359 - val_loss: 0.5141 - val_acc: 0.7397\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5170 - acc: 0.7376 - val_loss: 0.5098 - val_acc: 0.7429\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5125 - acc: 0.7413 - val_loss: 0.5101 - val_acc: 0.7418\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5083 - acc: 0.7439 - val_loss: 0.5049 - val_acc: 0.7462\n",
      "Epoch 18/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5044 - acc: 0.7465 - val_loss: 0.4984 - val_acc: 0.7491\n",
      "Epoch 19/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5014 - acc: 0.7489 - val_loss: 0.5234 - val_acc: 0.7275\n",
      "Epoch 20/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4974 - acc: 0.7502 - val_loss: 0.4985 - val_acc: 0.7486\n",
      "Best validation accuracy of eval run: 0.7490724503779445\n",
      "history of eval run: {'val_loss': [0.6005311652521521, 0.5803345949054589, 0.5768080705038457, 0.5621179126757986, 0.5555832435845999, 0.5504784585503207, 0.5439229214350969, 0.5423957972890725, 0.5371754201046046, 0.533996812204199, 0.5282908882399446, 0.5227571234562235, 0.5181367739544082, 0.5140848526175692, 0.5098266314981608, 0.5101066784199669, 0.5048567345393216, 0.4984341202571087, 0.5234262156844084, 0.49852983968457043], 'val_acc': [0.6510198431054622, 0.6914628518546403, 0.6903553299547397, 0.7041439778374592, 0.710419935381352, 0.7146654360911567, 0.7194831564396716, 0.7209598523337114, 0.723617904923399, 0.7264420858351492, 0.7298200276737451, 0.7342316566715056, 0.7384402399487797, 0.7397138901564401, 0.7428703276441017, 0.7417997231217205, 0.7462482694826975, 0.7490724503779445, 0.7274757729437036, 0.7486479003087243], 'loss': [0.6185983128616808, 0.5973365415898976, 0.5837108340446094, 0.5741946214378539, 0.5662027198045964, 0.5594303665444686, 0.5535950865051656, 0.5483731103454647, 0.543044628385438, 0.538979283919216, 0.534412248235604, 0.5298475940646139, 0.5260834251040325, 0.5217059995789571, 0.5170362673874965, 0.5124930285314713, 0.5083187192909089, 0.5044046514127143, 0.5013608532024476, 0.49742222904087363], 'acc': [0.6453388833211642, 0.6680757001687313, 0.6830551415161839, 0.6924784376374166, 0.7005173121872769, 0.7059580889516224, 0.7112742677500339, 0.7158705473538646, 0.7196500182295926, 0.7232587437755963, 0.7262813975326492, 0.7297562956526686, 0.7317867806174685, 0.7358892831921106, 0.7375875069872186, 0.7413254452063517, 0.7439235430188222, 0.7464847229014222, 0.7489166901301471, 0.7501626695353328]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 39s 181us/step - loss: 3.2703 - acc: 0.6235 - val_loss: 9.5212 - val_acc: 0.4026\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 38s 177us/step - loss: 6.0186 - acc: 0.6265 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 38s 177us/step - loss: 5.9690 - acc: 0.6296 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 38s 177us/step - loss: 5.9597 - acc: 0.6302 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 38s 177us/step - loss: 5.9593 - acc: 0.6303 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "history of eval run: {'val_loss': [9.521180178435872, 5.973290318646186, 5.973290318646186, 5.973290318646186, 5.973290318646186], 'val_acc': [0.4026395939031283, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597, 0.6294047069725597], 'loss': [3.2703394801834396, 6.018559523355866, 5.969006071497919, 5.959705967891849, 5.9592641884628925], 'acc': [0.6235388584088651, 0.626464602643261, 0.6296487722441322, 0.6302394587886888, 0.6302671472100986]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 36s 166us/step - loss: 0.6186 - acc: 0.6472 - val_loss: 0.5890 - val_acc: 0.6883\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5828 - acc: 0.6878 - val_loss: 0.5661 - val_acc: 0.6984\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5638 - acc: 0.7047 - val_loss: 0.5664 - val_acc: 0.6967\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5514 - acc: 0.7151 - val_loss: 0.5384 - val_acc: 0.7261\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5421 - acc: 0.7233 - val_loss: 0.5454 - val_acc: 0.7177\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5349 - acc: 0.7278 - val_loss: 0.5256 - val_acc: 0.7353\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5277 - acc: 0.7324 - val_loss: 0.5163 - val_acc: 0.7402\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5213 - acc: 0.7379 - val_loss: 0.5091 - val_acc: 0.7444\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5152 - acc: 0.7409 - val_loss: 0.5081 - val_acc: 0.7445\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5090 - acc: 0.7445 - val_loss: 0.5467 - val_acc: 0.7137\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5034 - acc: 0.7477 - val_loss: 0.4920 - val_acc: 0.7547\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4980 - acc: 0.7510 - val_loss: 0.4920 - val_acc: 0.7539\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4936 - acc: 0.7536 - val_loss: 0.4846 - val_acc: 0.7568\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4902 - acc: 0.7556 - val_loss: 0.4826 - val_acc: 0.7589\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4860 - acc: 0.7588 - val_loss: 0.4765 - val_acc: 0.7644\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4832 - acc: 0.7596 - val_loss: 0.4753 - val_acc: 0.7651\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4798 - acc: 0.7623 - val_loss: 0.4713 - val_acc: 0.7664\n",
      "Epoch 18/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4765 - acc: 0.7647 - val_loss: 0.4732 - val_acc: 0.7670\n",
      "Epoch 19/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4743 - acc: 0.7651 - val_loss: 0.4727 - val_acc: 0.7665\n",
      "Epoch 20/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.4714 - acc: 0.7661 - val_loss: 0.4660 - val_acc: 0.7695\n",
      "Best validation accuracy of eval run: 0.7694877710967335\n",
      "history of eval run: {'val_loss': [0.5890393931963612, 0.5661057594287874, 0.5663924388848091, 0.538352215875939, 0.5453842699307696, 0.5255695415577363, 0.5162736706487253, 0.5091142037286701, 0.508141275858846, 0.5467249676924891, 0.49200886703714775, 0.49195447174260626, 0.4845893919935521, 0.48255596352547725, 0.47652018538256313, 0.4752698709764511, 0.47131684743098307, 0.4731982932158973, 0.4726755851988535, 0.4660153272688856], 'val_acc': [0.6882694970037622, 0.6983848638714982, 0.6966866635935176, 0.7261467466565613, 0.717692662652979, 0.7352838024765211, 0.7401753576218835, 0.7443839409167612, 0.7444577757290117, 0.7137055837420423, 0.754739270882503, 0.7539086294262213, 0.7567697277187916, 0.7589109367635539, 0.7643747115674303, 0.7650576834179148, 0.7663867097215604, 0.7669958467950015, 0.766460544533811, 0.7694877710967335], 'loss': [0.6186474874505852, 0.582794890102359, 0.5637615372030116, 0.5513627870655848, 0.5420721001155357, 0.5349130059570684, 0.5277061015944446, 0.5213471061137152, 0.5152150602337001, 0.5090316037911402, 0.5033668000024732, 0.4980206347779262, 0.4935720488133724, 0.49015534294663415, 0.48596391676929496, 0.48320553997079224, 0.47980058372889633, 0.47648662501347655, 0.47430942040348767, 0.4713568186828659], 'acc': [0.6472263113966288, 0.6878498548617965, 0.7046567326741857, 0.7150906565383037, 0.7233141206321688, 0.7278457938921459, 0.732386696635003, 0.7379243828896862, 0.7409331924253069, 0.7444773116434104, 0.7476660959809082, 0.7510302403773302, 0.7536237234416217, 0.7555849873281417, 0.758810689585883, 0.7596367277748611, 0.76229943192926, 0.7647221696687784, 0.7651236519150996, 0.7661065912346496]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 36s 166us/step - loss: 0.6329 - acc: 0.6292 - val_loss: 0.6334 - val_acc: 0.6699\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.6118 - acc: 0.6524 - val_loss: 0.6328 - val_acc: 0.6262\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5970 - acc: 0.6803 - val_loss: 0.6389 - val_acc: 0.6076\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5858 - acc: 0.6913 - val_loss: 0.6336 - val_acc: 0.6143\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 35s 163us/step - loss: 0.5758 - acc: 0.7001 - val_loss: 0.6724 - val_acc: 0.5616\n",
      "Best validation accuracy of eval run: 0.6699400092161466\n",
      "history of eval run: {'val_loss': [0.6334056080746442, 0.6328104924132432, 0.6388791149829905, 0.6336408191724037, 0.6724242588117147], 'val_acc': [0.6699400092161466, 0.6262482694859982, 0.6075865251565783, 0.6143239501516116, 0.5616059067912733], 'loss': [0.6329082958057284, 0.6118445715666057, 0.5970063678970089, 0.5858138533033965, 0.5757554155860212], 'acc': [0.6291642246961735, 0.6524040480481077, 0.6803462899823164, 0.6912693761290942, 0.7000512236031532]}\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 39s 181us/step - loss: 7.2911 - acc: 0.5452 - val_loss: 10.0342 - val_acc: 0.3706\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 38s 177us/step - loss: 7.8599 - acc: 0.5091 - val_loss: 10.0342 - val_acc: 0.3706\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 38s 177us/step - loss: 8.3839 - acc: 0.4757 - val_loss: 10.0342 - val_acc: 0.3706\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 38s 177us/step - loss: 8.5399 - acc: 0.4658 - val_loss: 10.0342 - val_acc: 0.3706\n",
      "Best validation accuracy of eval run: 0.3705952930274404\n",
      "history of eval run: {'val_loss': [10.034212675688726, 10.034212675688726, 10.034212675688726, 10.034212675688726], 'val_acc': [0.3705952930274404, 0.3705952930274404, 0.3705952930274404, 0.3705952930274404], 'loss': [7.291112471967582, 7.859922034679154, 8.383911086584842, 8.539928704691503], 'acc': [0.5452129009661216, 0.5090887275830901, 0.4757333973210739, 0.46575633258185445]}\n",
      "16042.32226729393\n"
     ]
    }
   ],
   "source": [
    "init_session()\n",
    "model1_run2_trials=Trials()\n",
    "\n",
    "t0 = time.time()\n",
    "# hyperas optimizer\n",
    "model1_run2_best = optim.minimize(model=create_model,\n",
    "                          data=data,\n",
    "                          algo=tpe.suggest,\n",
    "                          functions=functions,\n",
    "                          max_evals=50,\n",
    "                          trials=model1_run2_trials,\n",
    "                          notebook_name='DL_encode_attend')\n",
    "t1 = time.time() - t0\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performing model hyper-parameters:\n",
      "({'dropout_rate1': 0.6429988031383259, 'dropout_rate1_1': 0.04755846424484607, 'dropout_rate1_2': 0.5947232870685206, 'learn_rate': 0.022122461782015913, 'optimizer': 0}, None)\n"
     ]
    }
   ],
   "source": [
    "print(\"Best performing model hyper-parameters:\")\n",
    "print(model1_run2_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save optimization results for later use\n",
    "pickle.dump(model1_run2_best, open(data_folder+'model1_run2_best.p', 'wb'))\n",
    "pickle.dump(model1_run2_trials, open(data_folder+'model1_run2_trials.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 578.93x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_optimization_history(model1_run2_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='da_train'></a>\n",
    "#### Train optimized model\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler\n",
    "# function for exponential decay (used for learning rate decay)\n",
    "def exp_decay(epoch):\n",
    "    init_lr = 0.0221\n",
    "    k = 0.1\n",
    "    lrate = init_lr * np.exp(-k*epoch)\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 243784 samples, validate on 27088 samples\n",
      "Epoch 1/50\n",
      "243784/243784 [==============================] - 43s 175us/step - loss: 0.6161 - acc: 0.6516 - val_loss: 0.5812 - val_acc: 0.6971\n",
      "Epoch 2/50\n",
      "243784/243784 [==============================] - 42s 173us/step - loss: 0.5761 - acc: 0.7001 - val_loss: 0.5584 - val_acc: 0.7115\n",
      "Epoch 3/50\n",
      "243784/243784 [==============================] - 42s 173us/step - loss: 0.5524 - acc: 0.7192 - val_loss: 0.5740 - val_acc: 0.6941\n",
      "Epoch 4/50\n",
      "243784/243784 [==============================] - 42s 173us/step - loss: 0.5367 - acc: 0.7306 - val_loss: 0.5325 - val_acc: 0.7277\n",
      "Epoch 5/50\n",
      "243784/243784 [==============================] - 42s 173us/step - loss: 0.5231 - acc: 0.7393 - val_loss: 0.5148 - val_acc: 0.7415\n",
      "Epoch 6/50\n",
      "243784/243784 [==============================] - 42s 173us/step - loss: 0.5129 - acc: 0.7455 - val_loss: 0.5142 - val_acc: 0.7410\n",
      "Epoch 7/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.5039 - acc: 0.7510 - val_loss: 0.5429 - val_acc: 0.7178\n",
      "Epoch 8/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4965 - acc: 0.7561 - val_loss: 0.5142 - val_acc: 0.7366\n",
      "Epoch 9/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4908 - acc: 0.7600 - val_loss: 0.4865 - val_acc: 0.7562\n",
      "Epoch 10/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4861 - acc: 0.7623 - val_loss: 0.6525 - val_acc: 0.6518\n",
      "Epoch 11/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4813 - acc: 0.7652 - val_loss: 0.5171 - val_acc: 0.7346\n",
      "Epoch 12/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4767 - acc: 0.7689 - val_loss: 0.5416 - val_acc: 0.7185\n",
      "Epoch 13/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4742 - acc: 0.7699 - val_loss: 0.4720 - val_acc: 0.7686\n",
      "Epoch 14/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4705 - acc: 0.7718 - val_loss: 0.4955 - val_acc: 0.7540\n",
      "Epoch 15/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4678 - acc: 0.7741 - val_loss: 0.4677 - val_acc: 0.7731\n",
      "Epoch 16/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4650 - acc: 0.7752 - val_loss: 0.4658 - val_acc: 0.7737\n",
      "Epoch 17/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4630 - acc: 0.7762 - val_loss: 0.4772 - val_acc: 0.7644\n",
      "Epoch 18/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4612 - acc: 0.7776 - val_loss: 0.4703 - val_acc: 0.7705\n",
      "Epoch 19/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4590 - acc: 0.7799 - val_loss: 0.4637 - val_acc: 0.7745\n",
      "Epoch 20/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4575 - acc: 0.7801 - val_loss: 0.4734 - val_acc: 0.7690\n",
      "Epoch 21/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4561 - acc: 0.7811 - val_loss: 0.4594 - val_acc: 0.7779\n",
      "Epoch 22/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4547 - acc: 0.7818 - val_loss: 0.4598 - val_acc: 0.7780\n",
      "Epoch 23/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4535 - acc: 0.7826 - val_loss: 0.4588 - val_acc: 0.7792\n",
      "Epoch 24/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4527 - acc: 0.7835 - val_loss: 0.4738 - val_acc: 0.7686\n",
      "Epoch 25/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4513 - acc: 0.7844 - val_loss: 0.4610 - val_acc: 0.7766\n",
      "Epoch 26/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4497 - acc: 0.7848 - val_loss: 0.4763 - val_acc: 0.7659\n",
      "Epoch 27/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4488 - acc: 0.7863 - val_loss: 0.4549 - val_acc: 0.7804\n",
      "Epoch 28/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4479 - acc: 0.7862 - val_loss: 0.4550 - val_acc: 0.7814\n",
      "Epoch 29/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4474 - acc: 0.7877 - val_loss: 0.4538 - val_acc: 0.7806\n",
      "Epoch 30/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4466 - acc: 0.7874 - val_loss: 0.4531 - val_acc: 0.7832\n",
      "Epoch 31/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4461 - acc: 0.7878 - val_loss: 0.4561 - val_acc: 0.7820\n",
      "Epoch 32/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4453 - acc: 0.7880 - val_loss: 0.4554 - val_acc: 0.7781\n",
      "Epoch 33/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4445 - acc: 0.7880 - val_loss: 0.4627 - val_acc: 0.7754\n",
      "Epoch 34/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4444 - acc: 0.7890 - val_loss: 0.4541 - val_acc: 0.7822\n",
      "Epoch 35/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4438 - acc: 0.7887 - val_loss: 0.4525 - val_acc: 0.7830\n",
      "Epoch 36/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4430 - acc: 0.7893 - val_loss: 0.4534 - val_acc: 0.7831\n",
      "Epoch 37/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4428 - acc: 0.7895 - val_loss: 0.4546 - val_acc: 0.7812\n",
      "Epoch 38/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4418 - acc: 0.7906 - val_loss: 0.4510 - val_acc: 0.7843\n",
      "Epoch 39/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4424 - acc: 0.7902 - val_loss: 0.4538 - val_acc: 0.7829\n",
      "Epoch 40/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4415 - acc: 0.7900 - val_loss: 0.4528 - val_acc: 0.7836\n",
      "Epoch 41/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4416 - acc: 0.7907 - val_loss: 0.4520 - val_acc: 0.7842\n",
      "Epoch 42/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4416 - acc: 0.7906 - val_loss: 0.4510 - val_acc: 0.7832\n",
      "Epoch 43/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4409 - acc: 0.7908 - val_loss: 0.4529 - val_acc: 0.7828\n",
      "Epoch 44/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4405 - acc: 0.7908 - val_loss: 0.4497 - val_acc: 0.7849\n",
      "Epoch 45/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4407 - acc: 0.7911 - val_loss: 0.4503 - val_acc: 0.7848\n",
      "Epoch 46/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4402 - acc: 0.7918 - val_loss: 0.4538 - val_acc: 0.7824\n",
      "Epoch 47/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4405 - acc: 0.7913 - val_loss: 0.4509 - val_acc: 0.7843\n",
      "Epoch 48/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4399 - acc: 0.7919 - val_loss: 0.4516 - val_acc: 0.7833\n",
      "Epoch 49/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4400 - acc: 0.7913 - val_loss: 0.4516 - val_acc: 0.7840\n",
      "Epoch 50/50\n",
      "243784/243784 [==============================] - 42s 174us/step - loss: 0.4391 - acc: 0.7917 - val_loss: 0.4495 - val_acc: 0.7856\n",
      "2121.00804066658\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "init_session()\n",
    "K.clear_session()\n",
    "# this is needed to free up gpu memory after every evaluation\n",
    "gc.collect()\n",
    "# load word vectors (if already not loaded)\n",
    "w2v = pickle.load(open(data_folder+'w2v.p', 'rb'))\n",
    "x_train, y_train, x_test, y_test = data()\n",
    "\n",
    "# set parameters to the values returned by the deep optimization run\n",
    "dropout_rate1 = 0.643\n",
    "dropout_rate2 = 0.0475\n",
    "dropout_rate3 = 0.595\n",
    "optimizer = 'sgd'\n",
    "#learn_rate = 0.0221\n",
    "learn_rate = LearningRateScheduler(exp_decay)\n",
    "m1 = build_model(vectors=w2v, max_length=50, projected_dim=300, num_classes=1, \n",
    "                        num_hidden=200, dropout_rate1=dropout_rate1, \n",
    "                        dropout_rate2=dropout_rate2, dropout_rate3=dropout_rate3,\n",
    "                        optimizer=optimizer,\n",
    "                        learn_rate=0.0)\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
    "result_1 = m1.fit(x_train, y_train, batch_size= 64, epochs=50,\n",
    "      validation_split=0.1, callbacks=[learn_rate])\n",
    "t1 = time.time() - t0\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='da_pred'></a>\n",
    "#### Predict test data\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['not duplicate', 'duplicate']\n",
    "\n",
    "convert_binary = lambda x: 1 if x[0] >= .5 else 0\n",
    "\n",
    "#m.evaluate(test_sents,y_test)\n",
    "\n",
    "y_pred_dl1 = m1.predict(x_test, batch_size=64)\n",
    "\n",
    "y_pred_dl1_classes = np.array([convert_binary(y) for y in y_pred_dl1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='da_eval'></a>\n",
    "#### Evaluate Model\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for DL method 1  0.7860435483266499\n",
      "Recall score for DL method 1   0.7300805729632945\n",
      "Precision score for DL method 1   0.7013545474091594\n"
     ]
    }
   ],
   "source": [
    "score_dl1 = accuracy_score(y_test.values, y_pred_dl1_classes)\n",
    "rscore_dl1 = recall_score(y_test.values, y_pred_dl1_classes)\n",
    "pscore_dl1 = precision_score(y_test.values, y_pred_dl1_classes)\n",
    "print('Accuracy score for DL method 1 ', score_dl1)\n",
    "print('Recall score for DL method 1  ', rscore_dl1)\n",
    "print('Precision score for DL method 1  ', pscore_dl1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "not duplicate       0.84      0.82      0.83     84267\n",
      "    duplicate       0.70      0.73      0.72     49148\n",
      "\n",
      "    micro avg       0.79      0.79      0.79    133415\n",
      "    macro avg       0.77      0.77      0.77    133415\n",
      " weighted avg       0.79      0.79      0.79    133415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test.values, y_pred_dl1_classes, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_fpr, m1_tpr, _ = roc_curve(y_test.values, y_pred_dl1_classes)\n",
    "m1_roc_auc = auc(m1_fpr, m1_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 598.055x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc(m1_fpr, m1_tpr, m1_roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ha'></a>\n",
    "### Hierarchical Attention Model\n",
    "(Code implemented based on algorithm described at Example 2: <a href='https://explosion.ai/blog/deep-learning-formula-nlp'>Hierarchical Attention Networks for Document Classification</a>). \n",
    "\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='c_att_ha'></a>\n",
    "#### Class - Attention_Layer\n",
    "(custom keras layer to implement the attention mechanism (with trainable weights) for the NN. Implementation based on word and sentence attention layers described in <a href='https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf'>Yang et al.</a> and <a href='https://keras.io/layers/writing-your-own-keras-layers/'>keras custom layer example</a>)\n",
    "\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer, RNN\n",
    "\n",
    "class Attention_Layer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim):\n",
    "        self.output_dim = output_dim\n",
    "        super(Attention_Layer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_weight(name='W', \n",
    "                                      shape=(input_shape[-1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.b = self.add_weight(name='b', \n",
    "                                      shape=(self.output_dim,),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.u = self.add_weight(name='u', \n",
    "                                      shape=(self.output_dim,1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(Attention_Layer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, h_it):        \n",
    "        u_it = K.tanh(K.bias_add(K.dot(h_it, self.W), self.b))\n",
    "        att_weights = K.dot(u_it, self.u)\n",
    "        exp_weights = K.exp(att_weights)\n",
    "        sum_weights = K.sum(exp_weights, axis=1, keepdims=True)\n",
    "        alpha_it = exp_weights/sum_weights\n",
    "        return K.sum(h_it*alpha_it, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='f_bld_ha'></a>\n",
    "#### Function - Build hierarchical attention model\n",
    "(implementation based on GRU-based word and sentence encoders and word and sentence attention layers described at <a href='https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf'>Yang et al.</a>)\n",
    "\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hier_model(vectors, optimizer, learn_rate, dropout_rate1, dropout_rate2, max_length=50, num_hidden=200, num_classes=1, \n",
    "                projected_dim=200):\n",
    "    #optimizer=params['optimizer']\n",
    "    #learn_rate=params['learn_rate']\n",
    "    #dropout_rate1=params['dropout_rate1']\n",
    "    #dropout_rate2=params['dropout_rate1']\n",
    "    #K.clear_session()\n",
    "    # input    \n",
    "    model_input = layers.Input(shape=(2, max_length), dtype='int32')\n",
    "    \n",
    "    # embeddings (projected)\n",
    "    embed = create_embedding(vectors, max_length, projected_dim)\n",
    "    \n",
    "    # step 1: word encoder\n",
    "    word_sequence_input = layers.Input(shape=(max_length,), dtype='int32')\n",
    "    h_w = layers.Bidirectional(layers.GRU(num_hidden, dropout=dropout_rate1, return_sequences=True))(embed(word_sequence_input))\n",
    "    \n",
    "    # step 2: word attention\n",
    "    s_w = Attention_Layer(num_hidden)(h_w)\n",
    "    word_encode_attend = Model(word_sequence_input, s_w)\n",
    "    \n",
    "    # step 3: sentence encoder\n",
    "    sent_encode_attend = layers.TimeDistributed(word_encode_attend)(model_input)\n",
    "    h = layers.Bidirectional(layers.GRU(num_hidden, dropout=dropout_rate2, return_sequences=True))(sent_encode_attend)\n",
    "    \n",
    "    # step 4: sentence attention\n",
    "    v = Attention_Layer(num_hidden)(h)\n",
    "    \n",
    "    # step 5: final classification\n",
    "    out = layers.Dense(num_classes, activation='sigmoid', use_bias=True)(v)\n",
    "    \n",
    "    model = Model(model_input, out)\n",
    "    \n",
    "    if optimizer == 'sgd':\n",
    "        opt = SGD(lr=learn_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        opt = Adam(lr=learn_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(lr=learn_rate)\n",
    "    else:\n",
    "        opt = Nadam(lr=learn_rate)\n",
    "    \n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ha_hyp'></a>\n",
    "#### Hyperparameter optimization - Hyperopt\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ha_hyp_run1'></a>\n",
    "##### Shallow optimization run\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from hyperopt import STATUS_OK\n",
    "\n",
    "x_train, y_train, x_test, y_test = data()\n",
    "w2v = pickle.load(open(data_folder+'w2v.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization function which will be called by hyperopt with the different parameter combinations\n",
    "def opt_fn(params):\n",
    "    K.clear_session()\n",
    "    # this is needed to free up gpu memory after every evaluation\n",
    "    gc.collect()\n",
    "    model = build_hier_model(vectors=w2v, params=params, max_length=50, projected_dim=200, num_classes=1, \n",
    "                        num_hidden=200)\n",
    "    result = model.fit(x_train, y_train, batch_size= params['batch_size'], epochs=2,\n",
    "      validation_split=0.2)\n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation accuracy of eval run:', validation_acc)\n",
    "    #print('history of eval run:', result.history)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK,\n",
    "            'train_acc': np.amax(result.history['acc'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search space for all variable hyperparameters\n",
    "search_space = {\n",
    "                'optimizer':hp.choice('optimizer',['sgd', 'rmsprop','nadam', 'adam']),\n",
    "                'learn_rate':hp.loguniform('learn_rate',-6,-3),\n",
    "                'dropout_rate1':hp.uniform('dropout_rate1',0, 1),\n",
    "                'dropout_rate2':hp.uniform('dropout_rate2',0, 1),\n",
    "                'batch_size':hp.choice('batch_size',[32, 64, 128])\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 277s 1ms/step - loss: 0.6385 - acc: 0.6466 - val_loss: 0.5898 - val_acc: 0.6887\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 275s 1ms/step - loss: 0.6118 - acc: 0.6699 - val_loss: 0.6190 - val_acc: 0.6958\n",
      "Best validation accuracy of eval run: 0.6958191047399123\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 539s 2ms/step - loss: 0.5782 - acc: 0.6941 - val_loss: 0.5558 - val_acc: 0.7083\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 537s 2ms/step - loss: 0.5634 - acc: 0.7064 - val_loss: 0.5683 - val_acc: 0.7007\n",
      "Best validation accuracy of eval run: 0.7083341024501785\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 151s 699us/step - loss: 0.5657 - acc: 0.7026 - val_loss: 0.5545 - val_acc: 0.7084\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 150s 690us/step - loss: 0.5442 - acc: 0.7164 - val_loss: 0.5298 - val_acc: 0.7271\n",
      "Best validation accuracy of eval run: 0.7270696815885483\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 153s 706us/step - loss: 0.6155 - acc: 0.6570 - val_loss: 0.6008 - val_acc: 0.6662\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 150s 694us/step - loss: 0.6235 - acc: 0.6392 - val_loss: 0.6007 - val_acc: 0.6716\n",
      "Best validation accuracy of eval run: 0.6715643747148835\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 541s 2ms/step - loss: 3.7657 - acc: 0.3863 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 540s 2ms/step - loss: 10.0540 - acc: 0.0000e+00 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Best validation accuracy of eval run: 0.0\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 275s 1ms/step - loss: 0.6584 - acc: 0.6302 - val_loss: 0.6571 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 273s 1ms/step - loss: 0.6547 - acc: 0.6306 - val_loss: 0.6529 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.6609 - acc: 0.6306 - val_loss: 0.6584 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 147s 679us/step - loss: 0.6575 - acc: 0.6306 - val_loss: 0.6579 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 152s 701us/step - loss: 0.6350 - acc: 0.6349 - val_loss: 0.6018 - val_acc: 0.6743\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 150s 690us/step - loss: 0.6287 - acc: 0.6444 - val_loss: 0.6143 - val_acc: 0.6476\n",
      "Best validation accuracy of eval run: 0.6742593447205985\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 151s 695us/step - loss: 0.5497 - acc: 0.7126 - val_loss: 0.4973 - val_acc: 0.7477\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.4836 - acc: 0.7587 - val_loss: 0.4583 - val_acc: 0.7780\n",
      "Best validation accuracy of eval run: 0.7779787724987395\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 151s 695us/step - loss: 0.5892 - acc: 0.6825 - val_loss: 0.5583 - val_acc: 0.7081\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5612 - acc: 0.7094 - val_loss: 0.5450 - val_acc: 0.7251\n",
      "Best validation accuracy of eval run: 0.7250946008328419\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 151s 695us/step - loss: 5.9482 - acc: 0.6304 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 148s 684us/step - loss: 5.9533 - acc: 0.6306 - val_loss: 5.9733 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 277s 1ms/step - loss: 0.6202 - acc: 0.6575 - val_loss: 0.6126 - val_acc: 0.6890\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 276s 1ms/step - loss: 0.6122 - acc: 0.6681 - val_loss: 0.6196 - val_acc: 0.6615\n",
      "Best validation accuracy of eval run: 0.6889709275551089\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 153s 706us/step - loss: 0.6027 - acc: 0.6715 - val_loss: 0.5740 - val_acc: 0.6931\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 150s 694us/step - loss: 0.6174 - acc: 0.6514 - val_loss: 0.6050 - val_acc: 0.6629\n",
      "Best validation accuracy of eval run: 0.6930502999406506\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 541s 2ms/step - loss: 0.5556 - acc: 0.7111 - val_loss: 0.5351 - val_acc: 0.7315\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 537s 2ms/step - loss: 0.5344 - acc: 0.7274 - val_loss: 0.5292 - val_acc: 0.7321\n",
      "Best validation accuracy of eval run: 0.732145823706225\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 542s 2ms/step - loss: 0.6580 - acc: 0.6272 - val_loss: 0.6075 - val_acc: 0.6701\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 541s 2ms/step - loss: 0.6597 - acc: 0.6213 - val_loss: 0.6583 - val_acc: 0.5996\n",
      "Best validation accuracy of eval run: 0.6700507614235203\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 535s 2ms/step - loss: 0.6598 - acc: 0.6297 - val_loss: 0.6582 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 534s 2ms/step - loss: 0.6573 - acc: 0.6306 - val_loss: 0.6568 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 276s 1ms/step - loss: 0.6600 - acc: 0.6304 - val_loss: 0.6583 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 274s 1ms/step - loss: 0.6573 - acc: 0.6306 - val_loss: 0.6575 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 535s 2ms/step - loss: 0.6522 - acc: 0.6306 - val_loss: 0.6422 - val_acc: 0.6285\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 535s 2ms/step - loss: 0.6354 - acc: 0.6264 - val_loss: 0.6277 - val_acc: 0.6272\n",
      "Best validation accuracy of eval run: 0.6285186894367959\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 277s 1ms/step - loss: 0.7231 - acc: 0.5886 - val_loss: 0.6376 - val_acc: 0.6410\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 276s 1ms/step - loss: 3.0696 - acc: 0.4531 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Best validation accuracy of eval run: 0.6409967697145315\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 278s 1ms/step - loss: 0.6132 - acc: 0.6614 - val_loss: 0.6502 - val_acc: 0.6026\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 276s 1ms/step - loss: 0.6150 - acc: 0.6615 - val_loss: 0.5961 - val_acc: 0.6713\n",
      "Best validation accuracy of eval run: 0.6712505768387341\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 539s 2ms/step - loss: 0.5455 - acc: 0.7193 - val_loss: 0.5209 - val_acc: 0.7371\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 538s 2ms/step - loss: 0.5144 - acc: 0.7443 - val_loss: 0.5068 - val_acc: 0.7491\n",
      "Best validation accuracy of eval run: 0.7491093677928715\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216697/216697 [==============================] - 539s 2ms/step - loss: 0.6151 - acc: 0.6673 - val_loss: 0.6230 - val_acc: 0.6826\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 537s 2ms/step - loss: 0.5978 - acc: 0.6861 - val_loss: 0.5793 - val_acc: 0.6895\n",
      "Best validation accuracy of eval run: 0.6894508537016104\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 539s 2ms/step - loss: 0.5608 - acc: 0.7073 - val_loss: 0.5512 - val_acc: 0.7143\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 539s 2ms/step - loss: 0.5349 - acc: 0.7293 - val_loss: 0.5788 - val_acc: 0.6885\n",
      "Best validation accuracy of eval run: 0.7142593447029948\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 150s 692us/step - loss: 0.5724 - acc: 0.6978 - val_loss: 0.5295 - val_acc: 0.7265\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5260 - acc: 0.7311 - val_loss: 0.4979 - val_acc: 0.7493\n",
      "Best validation accuracy of eval run: 0.7493308721592089\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 151s 695us/step - loss: 0.5859 - acc: 0.6910 - val_loss: 0.5572 - val_acc: 0.7177\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 148s 685us/step - loss: 0.5628 - acc: 0.7100 - val_loss: 0.5735 - val_acc: 0.7021\n",
      "Best validation accuracy of eval run: 0.7176742039543172\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 150s 694us/step - loss: 0.5760 - acc: 0.6952 - val_loss: 0.5387 - val_acc: 0.7262\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5445 - acc: 0.7211 - val_loss: 0.5286 - val_acc: 0.7367\n",
      "Best validation accuracy of eval run: 0.7366866635935175\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 151s 695us/step - loss: 0.5954 - acc: 0.6817 - val_loss: 0.5686 - val_acc: 0.6706\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5696 - acc: 0.7041 - val_loss: 0.5572 - val_acc: 0.7132\n",
      "Best validation accuracy of eval run: 0.7131518227810897\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 150s 694us/step - loss: 3.1278 - acc: 0.4286 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 149s 685us/step - loss: 10.0540 - acc: 0.0000e+00 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Best validation accuracy of eval run: 0.0\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 151s 695us/step - loss: 0.5532 - acc: 0.7105 - val_loss: 0.5046 - val_acc: 0.7453\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.4886 - acc: 0.7569 - val_loss: 0.4630 - val_acc: 0.7756\n",
      "Best validation accuracy of eval run: 0.7755606829575475\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 150s 694us/step - loss: 0.9431 - acc: 0.5489 - val_loss: 0.6391 - val_acc: 0.6292\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 148s 684us/step - loss: 0.6918 - acc: 0.5971 - val_loss: 0.6439 - val_acc: 0.6357\n",
      "Best validation accuracy of eval run: 0.635662205818891\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 152s 701us/step - loss: 0.5484 - acc: 0.7138 - val_loss: 0.5098 - val_acc: 0.7408\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 150s 690us/step - loss: 0.5401 - acc: 0.7198 - val_loss: 0.5209 - val_acc: 0.7332\n",
      "Best validation accuracy of eval run: 0.740784494677721\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 153s 706us/step - loss: 0.5734 - acc: 0.6972 - val_loss: 0.5537 - val_acc: 0.7133\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 151s 695us/step - loss: 0.5720 - acc: 0.6958 - val_loss: 0.5647 - val_acc: 0.7012\n",
      "Best validation accuracy of eval run: 0.713336409786411\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 152s 700us/step - loss: 0.5698 - acc: 0.6993 - val_loss: 0.5491 - val_acc: 0.7145\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 150s 691us/step - loss: 0.5593 - acc: 0.7059 - val_loss: 0.5452 - val_acc: 0.7187\n",
      "Best validation accuracy of eval run: 0.7187448084766984\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 151s 696us/step - loss: 0.6161 - acc: 0.6605 - val_loss: 0.5825 - val_acc: 0.6926\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 149s 685us/step - loss: 0.5776 - acc: 0.6959 - val_loss: 0.5676 - val_acc: 0.6803\n",
      "Best validation accuracy of eval run: 0.6925888324906103\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 153s 707us/step - loss: 0.5692 - acc: 0.6990 - val_loss: 0.5651 - val_acc: 0.7045\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 151s 695us/step - loss: 0.5402 - acc: 0.7206 - val_loss: 0.5506 - val_acc: 0.7095\n",
      "Best validation accuracy of eval run: 0.7094785417474025\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 278s 1ms/step - loss: 0.5594 - acc: 0.7072 - val_loss: 0.5196 - val_acc: 0.7364\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 276s 1ms/step - loss: 0.5154 - acc: 0.7399 - val_loss: 0.4889 - val_acc: 0.7587\n",
      "Best validation accuracy of eval run: 0.7587078910958783\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 152s 701us/step - loss: 0.6375 - acc: 0.6269 - val_loss: 0.6289 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 149s 690us/step - loss: 0.6374 - acc: 0.6218 - val_loss: 0.6603 - val_acc: 0.5523\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.6644 - acc: 0.6295 - val_loss: 0.6590 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 147s 679us/step - loss: 0.6582 - acc: 0.6306 - val_loss: 0.6587 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 150s 694us/step - loss: 0.5515 - acc: 0.7114 - val_loss: 0.4964 - val_acc: 0.7523\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.4817 - acc: 0.7602 - val_loss: 0.4763 - val_acc: 0.7660\n",
      "Best validation accuracy of eval run: 0.7659806183686056\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 280s 1ms/step - loss: 0.7870 - acc: 0.5595 - val_loss: 0.9054 - val_acc: 0.3762\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 278s 1ms/step - loss: 0.7169 - acc: 0.5753 - val_loss: 0.6571 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 150s 693us/step - loss: 0.5747 - acc: 0.6995 - val_loss: 0.5340 - val_acc: 0.7316\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5398 - acc: 0.7226 - val_loss: 0.5347 - val_acc: 0.7251\n",
      "Best validation accuracy of eval run: 0.7316474388577612\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 152s 701us/step - loss: 0.5674 - acc: 0.7009 - val_loss: 0.5513 - val_acc: 0.7109\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 150s 691us/step - loss: 0.5522 - acc: 0.7134 - val_loss: 0.5529 - val_acc: 0.7122\n",
      "Best validation accuracy of eval run: 0.7122473465642678\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.6588 - acc: 0.6302 - val_loss: 0.6575 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 147s 680us/step - loss: 0.6562 - acc: 0.6306 - val_loss: 0.6557 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 278s 1ms/step - loss: 0.5521 - acc: 0.7116 - val_loss: 0.5074 - val_acc: 0.7446\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 276s 1ms/step - loss: 0.5045 - acc: 0.7476 - val_loss: 0.4811 - val_acc: 0.7633\n",
      "Best validation accuracy of eval run: 0.7632671896653292\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 153s 705us/step - loss: 0.5934 - acc: 0.6813 - val_loss: 0.5807 - val_acc: 0.6877\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 150s 694us/step - loss: 0.5924 - acc: 0.6812 - val_loss: 0.5853 - val_acc: 0.6868\n",
      "Best validation accuracy of eval run: 0.6876603599479246\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 150s 694us/step - loss: 0.5818 - acc: 0.6956 - val_loss: 0.5638 - val_acc: 0.6963\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5492 - acc: 0.7165 - val_loss: 0.5362 - val_acc: 0.7219\n",
      "Best validation accuracy of eval run: 0.7219012459643601\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 535s 2ms/step - loss: 0.6579 - acc: 0.6306 - val_loss: 0.6566 - val_acc: 0.6294\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 533s 2ms/step - loss: 0.6544 - acc: 0.6306 - val_loss: 0.6531 - val_acc: 0.6294\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 278s 1ms/step - loss: 0.5574 - acc: 0.7103 - val_loss: 0.5199 - val_acc: 0.7350\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 275s 1ms/step - loss: 0.5230 - acc: 0.7349 - val_loss: 0.5183 - val_acc: 0.7396\n",
      "Best validation accuracy of eval run: 0.7395846792834114\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 152s 701us/step - loss: 0.5489 - acc: 0.7151 - val_loss: 0.5173 - val_acc: 0.7296\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 150s 690us/step - loss: 0.4936 - acc: 0.7536 - val_loss: 0.4752 - val_acc: 0.7623\n",
      "Best validation accuracy of eval run: 0.762344254732242\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/2\n",
      "216697/216697 [==============================] - 153s 705us/step - loss: 0.6954 - acc: 0.5879 - val_loss: 0.6681 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "216697/216697 [==============================] - 150s 694us/step - loss: 0.6928 - acc: 0.5912 - val_loss: 0.6730 - val_acc: 0.6282\n",
      "Best validation accuracy of eval run: 0.6282418089392662\n"
     ]
    }
   ],
   "source": [
    "init_session()\n",
    "model2_run1_trials=Trials()\n",
    "\n",
    "# hyperopt optimization\n",
    "model2_run1_best = fmin(opt_fn,\n",
    "                        search_space,\n",
    "                          algo=tpe.suggest,\n",
    "                          max_evals=50,\n",
    "                           trials=model2_run1_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performing model hyper-parameters:\n",
      "{'batch_size': 2, 'dropout_rate1': 0.3017008313578352, 'dropout_rate2': 0.8239975787678413, 'learn_rate': 0.0032509902881898665, 'optimizer': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best performing model hyper-parameters:\")\n",
    "print(model2_run1_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save optimization results for later use\n",
    "pickle.dump(model2_run1_best, open(data_folder+'model2_run1_best.p', 'wb'))\n",
    "pickle.dump(model2_run1_trials, open(data_folder+'model2_run1_trials.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previously saved optimization results saved earlier\n",
    "model2_run1_trials = pickle.load(open(data_folder+'model2_run1_trials.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 578.93x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_optimization_history(model2_run1_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ha_hyp_run2'></a>\n",
    "##### Deep optimization run\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler\n",
    "# function for exponential decay (used for laerning rate)\n",
    "def model2_exp_decay(init_lr, k):\n",
    "    def _exp_decay(epoch):\n",
    "        lrate = init_lr * np.exp(-k*epoch)\n",
    "        return lrate\n",
    "    return _exp_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization function which will be called by hyperopt for each combination of variable parameters\n",
    "def opt_deep_fn(params):\n",
    "    K.clear_session()\n",
    "    # this is needed to free up gpu memory after every evaluation\n",
    "    gc.collect()\n",
    "    model = build_hier_model(vectors=w2v, optimizer='rmsprop', \n",
    "                             learn_rate=0.0, dropout_rate1=params['dropout_rate1'], \n",
    "                             dropout_rate2=params['dropout_rate2'], max_length=50, projected_dim=200, \n",
    "                             num_classes=1, num_hidden=200)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "    learn_rate = LearningRateScheduler(model2_exp_decay(params['learn_rate'], params['decay']))\n",
    "    result = model.fit(x_train, y_train, batch_size= 128, epochs=20,\n",
    "      validation_split=0.2, callbacks=[early_stopping, learn_rate])\n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation accuracy of eval run:', validation_acc)\n",
    "    #print('history of eval run:', result.history)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK,\n",
    "            'train_acc': np.amax(result.history['acc'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search space for all variable hyperparameters\n",
    "search_deep_space = {\n",
    "                'learn_rate':hp.loguniform('learn_rate',-6,-3),\n",
    "                'dropout_rate1':hp.uniform('dropout_rate1',0, 1),\n",
    "                'dropout_rate2':hp.uniform('dropout_rate2',0, 1),\n",
    "                'decay':hp.uniform('decay',0, 1)\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 150s 690us/step - loss: 0.7015 - acc: 0.5931 - val_loss: 0.6658 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 148s 681us/step - loss: 6.9155 - acc: 0.2025 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 148s 683us/step - loss: 10.0540 - acc: 0.0000e+00 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 148s 682us/step - loss: 10.0540 - acc: 0.0000e+00 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Best validation accuracy of eval run: 0.6294416243698832\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 695us/step - loss: 0.6409 - acc: 0.6412 - val_loss: 0.6430 - val_acc: 0.6844\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.6052 - acc: 0.6719 - val_loss: 0.6268 - val_acc: 0.6971\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5869 - acc: 0.6917 - val_loss: 0.5765 - val_acc: 0.7031\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5743 - acc: 0.7018 - val_loss: 0.5891 - val_acc: 0.7101\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5665 - acc: 0.7085 - val_loss: 0.5639 - val_acc: 0.7166\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5613 - acc: 0.7125 - val_loss: 0.5685 - val_acc: 0.7171\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5581 - acc: 0.7155 - val_loss: 0.5661 - val_acc: 0.7172\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5561 - acc: 0.7167 - val_loss: 0.5649 - val_acc: 0.7183\n",
      "Best validation accuracy of eval run: 0.7182833410090547\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 696us/step - loss: 0.6723 - acc: 0.6184 - val_loss: 0.6438 - val_acc: 0.6087\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.6338 - acc: 0.6408 - val_loss: 0.6313 - val_acc: 0.6351\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.6158 - acc: 0.6566 - val_loss: 0.6053 - val_acc: 0.6758\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.6060 - acc: 0.6668 - val_loss: 0.6125 - val_acc: 0.6787\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5998 - acc: 0.6724 - val_loss: 0.5925 - val_acc: 0.6805\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5930 - acc: 0.6788 - val_loss: 0.5896 - val_acc: 0.6851\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5884 - acc: 0.6831 - val_loss: 0.5821 - val_acc: 0.6910\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5845 - acc: 0.6862 - val_loss: 0.5811 - val_acc: 0.6924\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5821 - acc: 0.6896 - val_loss: 0.5814 - val_acc: 0.6950\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5808 - acc: 0.6901 - val_loss: 0.5784 - val_acc: 0.6950\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5791 - acc: 0.6916 - val_loss: 0.5789 - val_acc: 0.6963\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5790 - acc: 0.6921 - val_loss: 0.5783 - val_acc: 0.6963\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5794 - acc: 0.6908 - val_loss: 0.5789 - val_acc: 0.6963\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5786 - acc: 0.6928 - val_loss: 0.5781 - val_acc: 0.6964\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5784 - acc: 0.6928 - val_loss: 0.5782 - val_acc: 0.6963\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5782 - acc: 0.6918 - val_loss: 0.5783 - val_acc: 0.6968\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5778 - acc: 0.6936 - val_loss: 0.5782 - val_acc: 0.6967\n",
      "Best validation accuracy of eval run: 0.696778957085726\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 697us/step - loss: 0.6547 - acc: 0.6352 - val_loss: 0.8686 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5816 - acc: 0.6946 - val_loss: 0.5628 - val_acc: 0.7078\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5681 - acc: 0.7049 - val_loss: 0.5546 - val_acc: 0.7162\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5626 - acc: 0.7089 - val_loss: 0.5525 - val_acc: 0.7194\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5550 - acc: 0.7158 - val_loss: 0.5513 - val_acc: 0.7205\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5512 - acc: 0.7189 - val_loss: 0.5506 - val_acc: 0.7232\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5492 - acc: 0.7191 - val_loss: 0.5416 - val_acc: 0.7270\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5483 - acc: 0.7199 - val_loss: 0.5426 - val_acc: 0.7262\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5468 - acc: 0.7215 - val_loss: 0.5413 - val_acc: 0.7258\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5463 - acc: 0.7211 - val_loss: 0.5407 - val_acc: 0.7270\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5463 - acc: 0.7217 - val_loss: 0.5407 - val_acc: 0.7264\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5467 - acc: 0.7216 - val_loss: 0.5403 - val_acc: 0.7267\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5462 - acc: 0.7221 - val_loss: 0.5405 - val_acc: 0.7266\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5463 - acc: 0.7218 - val_loss: 0.5404 - val_acc: 0.7266\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5461 - acc: 0.7217 - val_loss: 0.5403 - val_acc: 0.7266\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5461 - acc: 0.7213 - val_loss: 0.5403 - val_acc: 0.7267\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5461 - acc: 0.7220 - val_loss: 0.5403 - val_acc: 0.7267\n",
      "Epoch 18/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5463 - acc: 0.7214 - val_loss: 0.5403 - val_acc: 0.7266\n",
      "Epoch 19/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5466 - acc: 0.7218 - val_loss: 0.5403 - val_acc: 0.7266\n",
      "Epoch 20/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5461 - acc: 0.7216 - val_loss: 0.5403 - val_acc: 0.7266\n",
      "Best validation accuracy of eval run: 0.7270327641758216\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 698us/step - loss: 0.5801 - acc: 0.6944 - val_loss: 0.5224 - val_acc: 0.7337\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5254 - acc: 0.7327 - val_loss: 0.5103 - val_acc: 0.7420\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5078 - acc: 0.7463 - val_loss: 0.4940 - val_acc: 0.7570\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.4938 - acc: 0.7552 - val_loss: 0.4862 - val_acc: 0.7612\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.4837 - acc: 0.7620 - val_loss: 0.4768 - val_acc: 0.7670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4742 - acc: 0.7688 - val_loss: 0.4835 - val_acc: 0.7609\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4668 - acc: 0.7731 - val_loss: 0.4630 - val_acc: 0.7745\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4621 - acc: 0.7768 - val_loss: 0.4657 - val_acc: 0.7726\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4574 - acc: 0.7799 - val_loss: 0.4592 - val_acc: 0.7774\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4554 - acc: 0.7805 - val_loss: 0.4597 - val_acc: 0.7770\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4536 - acc: 0.7813 - val_loss: 0.4588 - val_acc: 0.7778\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4515 - acc: 0.7832 - val_loss: 0.4611 - val_acc: 0.7757\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4512 - acc: 0.7833 - val_loss: 0.4568 - val_acc: 0.7791\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4506 - acc: 0.7848 - val_loss: 0.4586 - val_acc: 0.7775\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4493 - acc: 0.7845 - val_loss: 0.4579 - val_acc: 0.7783\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.4492 - acc: 0.7844 - val_loss: 0.4560 - val_acc: 0.7799\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4492 - acc: 0.7861 - val_loss: 0.4561 - val_acc: 0.7799\n",
      "Epoch 18/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4487 - acc: 0.7847 - val_loss: 0.4561 - val_acc: 0.7800\n",
      "Epoch 19/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.4484 - acc: 0.7854 - val_loss: 0.4568 - val_acc: 0.7792\n",
      "Best validation accuracy of eval run: 0.779990770635266\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 697us/step - loss: 0.5648 - acc: 0.7021 - val_loss: 0.5215 - val_acc: 0.7341\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4955 - acc: 0.7513 - val_loss: 0.4883 - val_acc: 0.7561\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4651 - acc: 0.7712 - val_loss: 0.4670 - val_acc: 0.7721\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4432 - acc: 0.7850 - val_loss: 0.4581 - val_acc: 0.7771\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4322 - acc: 0.7916 - val_loss: 0.4565 - val_acc: 0.7771\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4267 - acc: 0.7955 - val_loss: 0.4556 - val_acc: 0.7775\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4248 - acc: 0.7959 - val_loss: 0.4551 - val_acc: 0.7786\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4238 - acc: 0.7979 - val_loss: 0.4551 - val_acc: 0.7784\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4240 - acc: 0.7965 - val_loss: 0.4552 - val_acc: 0.7783\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.4239 - acc: 0.7971 - val_loss: 0.4552 - val_acc: 0.7783\n",
      "Best validation accuracy of eval run: 0.7786432856505623\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 697us/step - loss: 0.6287 - acc: 0.6545 - val_loss: 0.5564 - val_acc: 0.7130\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5568 - acc: 0.7128 - val_loss: 0.5701 - val_acc: 0.7204\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5408 - acc: 0.7259 - val_loss: 0.5457 - val_acc: 0.7266\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5226 - acc: 0.7362 - val_loss: 0.5116 - val_acc: 0.7452\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5076 - acc: 0.7472 - val_loss: 0.5052 - val_acc: 0.7478\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4966 - acc: 0.7548 - val_loss: 0.4927 - val_acc: 0.7575\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4882 - acc: 0.7596 - val_loss: 0.4817 - val_acc: 0.7631\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4784 - acc: 0.7665 - val_loss: 0.4781 - val_acc: 0.7675\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4697 - acc: 0.7718 - val_loss: 0.4677 - val_acc: 0.7748\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4620 - acc: 0.7771 - val_loss: 0.4613 - val_acc: 0.7780\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4551 - acc: 0.7817 - val_loss: 0.4562 - val_acc: 0.7808\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4492 - acc: 0.7860 - val_loss: 0.4538 - val_acc: 0.7834\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4444 - acc: 0.7885 - val_loss: 0.4488 - val_acc: 0.7872\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4402 - acc: 0.7912 - val_loss: 0.4480 - val_acc: 0.7866\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4367 - acc: 0.7936 - val_loss: 0.4473 - val_acc: 0.7884\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4347 - acc: 0.7947 - val_loss: 0.4450 - val_acc: 0.7896\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4333 - acc: 0.7965 - val_loss: 0.4452 - val_acc: 0.7891\n",
      "Epoch 18/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4304 - acc: 0.7972 - val_loss: 0.4434 - val_acc: 0.7911\n",
      "Epoch 19/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4292 - acc: 0.7977 - val_loss: 0.4425 - val_acc: 0.7902\n",
      "Epoch 20/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4286 - acc: 0.7982 - val_loss: 0.4437 - val_acc: 0.7896\n",
      "Best validation accuracy of eval run: 0.7910659898334128\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 697us/step - loss: 0.6209 - acc: 0.6625 - val_loss: 0.5969 - val_acc: 0.6685\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5912 - acc: 0.6836 - val_loss: 0.5991 - val_acc: 0.6714\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5870 - acc: 0.6875 - val_loss: 0.5755 - val_acc: 0.7061\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5770 - acc: 0.6966 - val_loss: 0.5628 - val_acc: 0.7076\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5734 - acc: 0.7000 - val_loss: 0.5782 - val_acc: 0.6944\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5707 - acc: 0.7016 - val_loss: 0.5671 - val_acc: 0.7012\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5698 - acc: 0.7025 - val_loss: 0.5611 - val_acc: 0.7077\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5681 - acc: 0.7036 - val_loss: 0.5640 - val_acc: 0.7061\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5669 - acc: 0.7044 - val_loss: 0.5612 - val_acc: 0.7092\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5668 - acc: 0.7049 - val_loss: 0.5626 - val_acc: 0.7089\n",
      "Best validation accuracy of eval run: 0.7092016612861802\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 697us/step - loss: 0.6019 - acc: 0.6752 - val_loss: 0.5637 - val_acc: 0.7017\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5611 - acc: 0.7115 - val_loss: 0.5507 - val_acc: 0.7216\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5485 - acc: 0.7192 - val_loss: 0.5508 - val_acc: 0.7284\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5411 - acc: 0.7258 - val_loss: 0.5364 - val_acc: 0.7266\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5357 - acc: 0.7289 - val_loss: 0.5317 - val_acc: 0.7291\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5314 - acc: 0.7325 - val_loss: 0.5299 - val_acc: 0.7352\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5285 - acc: 0.7329 - val_loss: 0.5270 - val_acc: 0.7369\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5262 - acc: 0.7350 - val_loss: 0.5275 - val_acc: 0.7343\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5252 - acc: 0.7369 - val_loss: 0.5255 - val_acc: 0.7355\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5242 - acc: 0.7365 - val_loss: 0.5268 - val_acc: 0.7337\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5240 - acc: 0.7367 - val_loss: 0.5265 - val_acc: 0.7330\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5234 - acc: 0.7369 - val_loss: 0.5253 - val_acc: 0.7344\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5230 - acc: 0.7376 - val_loss: 0.5249 - val_acc: 0.7358\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5236 - acc: 0.7369 - val_loss: 0.5253 - val_acc: 0.7346\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5228 - acc: 0.7379 - val_loss: 0.5255 - val_acc: 0.7341\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5226 - acc: 0.7375 - val_loss: 0.5253 - val_acc: 0.7345\n",
      "Best validation accuracy of eval run: 0.7368712505790348\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 697us/step - loss: 0.6419 - acc: 0.6291 - val_loss: 0.6405 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.6288 - acc: 0.6309 - val_loss: 0.7609 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.6123 - acc: 0.6516 - val_loss: 0.7902 - val_acc: 0.6298\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.6016 - acc: 0.6653 - val_loss: 0.8362 - val_acc: 0.6311\n",
      "Best validation accuracy of eval run: 0.6311398246467637\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 696us/step - loss: 0.7331 - acc: 0.5735 - val_loss: 0.7115 - val_acc: 0.4257\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.7553 - acc: 0.5539 - val_loss: 0.9745 - val_acc: 0.6294\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.7395 - acc: 0.5607 - val_loss: 0.7172 - val_acc: 0.3931\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.7253 - acc: 0.5701 - val_loss: 0.6860 - val_acc: 0.6294\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 148s 685us/step - loss: 2.2051 - acc: 0.4811 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 685us/step - loss: 10.0540 - acc: 0.0000e+00 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 148s 685us/step - loss: 10.0540 - acc: 0.0000e+00 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 696us/step - loss: 2.9973 - acc: 0.5661 - val_loss: 1.0296 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.7141 - acc: 0.5799 - val_loss: 0.6357 - val_acc: 0.6302\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.6620 - acc: 0.6128 - val_loss: 0.7129 - val_acc: 0.4576\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.6492 - acc: 0.6254 - val_loss: 0.6387 - val_acc: 0.6256\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.6425 - acc: 0.6313 - val_loss: 0.6250 - val_acc: 0.6496\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.6381 - acc: 0.6356 - val_loss: 0.6340 - val_acc: 0.6266\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.6346 - acc: 0.6394 - val_loss: 0.6200 - val_acc: 0.6468\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.6319 - acc: 0.6409 - val_loss: 0.6229 - val_acc: 0.6529\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.6287 - acc: 0.6447 - val_loss: 0.6159 - val_acc: 0.6574\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.6262 - acc: 0.6491 - val_loss: 0.6209 - val_acc: 0.6466\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.6267 - acc: 0.6466 - val_loss: 0.6219 - val_acc: 0.6410\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.6258 - acc: 0.6476 - val_loss: 0.6220 - val_acc: 0.6418\n",
      "Best validation accuracy of eval run: 0.6573880941448643\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 697us/step - loss: 0.6821 - acc: 0.5996 - val_loss: 0.6104 - val_acc: 0.6749\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5953 - acc: 0.6761 - val_loss: 0.6095 - val_acc: 0.6433\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5756 - acc: 0.6953 - val_loss: 0.5676 - val_acc: 0.7083\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5672 - acc: 0.7031 - val_loss: 0.5551 - val_acc: 0.7124\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5628 - acc: 0.7060 - val_loss: 0.5521 - val_acc: 0.7150\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5588 - acc: 0.7091 - val_loss: 0.5491 - val_acc: 0.7174\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5574 - acc: 0.7089 - val_loss: 0.5503 - val_acc: 0.7133\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5566 - acc: 0.7106 - val_loss: 0.5493 - val_acc: 0.7133\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5556 - acc: 0.7114 - val_loss: 0.5487 - val_acc: 0.7149\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5554 - acc: 0.7110 - val_loss: 0.5480 - val_acc: 0.7159\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5554 - acc: 0.7110 - val_loss: 0.5484 - val_acc: 0.7158\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5549 - acc: 0.7112 - val_loss: 0.5485 - val_acc: 0.7157\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5552 - acc: 0.7101 - val_loss: 0.5486 - val_acc: 0.7155\n",
      "Best validation accuracy of eval run: 0.7174157821752534\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 698us/step - loss: 0.6581 - acc: 0.6300 - val_loss: 0.5820 - val_acc: 0.6972\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5738 - acc: 0.6994 - val_loss: 0.5630 - val_acc: 0.7047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5483 - acc: 0.7177 - val_loss: 0.5301 - val_acc: 0.7295\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5312 - acc: 0.7290 - val_loss: 0.5266 - val_acc: 0.7326\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.5222 - acc: 0.7355 - val_loss: 0.5159 - val_acc: 0.7388\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5166 - acc: 0.7393 - val_loss: 0.5118 - val_acc: 0.7425\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5142 - acc: 0.7398 - val_loss: 0.5091 - val_acc: 0.7437\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5127 - acc: 0.7408 - val_loss: 0.5079 - val_acc: 0.7458\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5121 - acc: 0.7415 - val_loss: 0.5081 - val_acc: 0.7460\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5115 - acc: 0.7425 - val_loss: 0.5073 - val_acc: 0.7465\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5112 - acc: 0.7428 - val_loss: 0.5073 - val_acc: 0.7466\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5112 - acc: 0.7426 - val_loss: 0.5072 - val_acc: 0.7464\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5106 - acc: 0.7431 - val_loss: 0.5073 - val_acc: 0.7461\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.5113 - acc: 0.7430 - val_loss: 0.5073 - val_acc: 0.7461\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 149s 690us/step - loss: 0.5114 - acc: 0.7422 - val_loss: 0.5073 - val_acc: 0.7463\n",
      "Best validation accuracy of eval run: 0.7465805260751123\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 696us/step - loss: 7.2767 - acc: 0.1675 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 10.0540 - acc: 0.0000e+00 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 10.0540 - acc: 0.0000e+00 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 10.0540 - acc: 0.0000e+00 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Best validation accuracy of eval run: 0.0\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 698us/step - loss: 2.7863 - acc: 0.4488 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 10.0540 - acc: 0.0000e+00 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 10.0540 - acc: 0.0000e+00 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 10.0540 - acc: 0.0000e+00 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Best validation accuracy of eval run: 0.0\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 697us/step - loss: 0.5702 - acc: 0.6975 - val_loss: 0.5395 - val_acc: 0.7213\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5224 - acc: 0.7331 - val_loss: 0.5028 - val_acc: 0.7484\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4999 - acc: 0.7483 - val_loss: 0.4861 - val_acc: 0.7592\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4865 - acc: 0.7576 - val_loss: 0.4763 - val_acc: 0.7664\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.4796 - acc: 0.7611 - val_loss: 0.4727 - val_acc: 0.7681\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4765 - acc: 0.7637 - val_loss: 0.4697 - val_acc: 0.7691\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4734 - acc: 0.7661 - val_loss: 0.4689 - val_acc: 0.7693\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4725 - acc: 0.7666 - val_loss: 0.4683 - val_acc: 0.7703\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4715 - acc: 0.7663 - val_loss: 0.4681 - val_acc: 0.7706\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.4717 - acc: 0.7672 - val_loss: 0.4679 - val_acc: 0.7705\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4722 - acc: 0.7669 - val_loss: 0.4678 - val_acc: 0.7706\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4721 - acc: 0.7670 - val_loss: 0.4678 - val_acc: 0.7707\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4719 - acc: 0.7668 - val_loss: 0.4678 - val_acc: 0.7707\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4712 - acc: 0.7670 - val_loss: 0.4678 - val_acc: 0.7707\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4717 - acc: 0.7663 - val_loss: 0.4677 - val_acc: 0.7707\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4710 - acc: 0.7670 - val_loss: 0.4677 - val_acc: 0.7707\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.4718 - acc: 0.7666 - val_loss: 0.4677 - val_acc: 0.7707\n",
      "Epoch 18/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4718 - acc: 0.7670 - val_loss: 0.4677 - val_acc: 0.7707\n",
      "Epoch 19/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4719 - acc: 0.7664 - val_loss: 0.4677 - val_acc: 0.7707\n",
      "Epoch 20/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4724 - acc: 0.7659 - val_loss: 0.4677 - val_acc: 0.7707\n",
      "Best validation accuracy of eval run: 0.7707245039081706\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 697us/step - loss: 0.6391 - acc: 0.6497 - val_loss: 0.5491 - val_acc: 0.7175\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5491 - acc: 0.7149 - val_loss: 0.5504 - val_acc: 0.7179\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5408 - acc: 0.7222 - val_loss: 0.5350 - val_acc: 0.7265\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5359 - acc: 0.7267 - val_loss: 0.5233 - val_acc: 0.7362\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5276 - acc: 0.7328 - val_loss: 0.5285 - val_acc: 0.7307\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5212 - acc: 0.7366 - val_loss: 0.5219 - val_acc: 0.7379\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5184 - acc: 0.7395 - val_loss: 0.5124 - val_acc: 0.7459\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5105 - acc: 0.7453 - val_loss: 0.5160 - val_acc: 0.7432\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5052 - acc: 0.7487 - val_loss: 0.5039 - val_acc: 0.7487\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5152 - acc: 0.7417 - val_loss: 0.5279 - val_acc: 0.7327\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5124 - acc: 0.7429 - val_loss: 0.5194 - val_acc: 0.7407\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5025 - acc: 0.7502 - val_loss: 0.5039 - val_acc: 0.7492\n",
      "Best validation accuracy of eval run: 0.7492385786824035\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 698us/step - loss: 0.5516 - acc: 0.7116 - val_loss: 0.5090 - val_acc: 0.7474\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4697 - acc: 0.7689 - val_loss: 0.4580 - val_acc: 0.7769\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4256 - acc: 0.7957 - val_loss: 0.4289 - val_acc: 0.7930\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3919 - acc: 0.8161 - val_loss: 0.4135 - val_acc: 0.8054\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3650 - acc: 0.8315 - val_loss: 0.4125 - val_acc: 0.8074\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3483 - acc: 0.8409 - val_loss: 0.4088 - val_acc: 0.8107\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3368 - acc: 0.8474 - val_loss: 0.4130 - val_acc: 0.8106\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.3316 - acc: 0.8500 - val_loss: 0.4125 - val_acc: 0.8111\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3270 - acc: 0.8529 - val_loss: 0.4126 - val_acc: 0.8111\n",
      "Best validation accuracy of eval run: 0.8111305952776287\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 697us/step - loss: 0.5941 - acc: 0.6828 - val_loss: 0.6315 - val_acc: 0.7086\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5519 - acc: 0.7188 - val_loss: 0.5459 - val_acc: 0.7231\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5341 - acc: 0.7300 - val_loss: 0.5317 - val_acc: 0.7326\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5236 - acc: 0.7364 - val_loss: 0.5295 - val_acc: 0.7366\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5191 - acc: 0.7398 - val_loss: 0.5211 - val_acc: 0.7388\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5162 - acc: 0.7413 - val_loss: 0.5216 - val_acc: 0.7397\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5149 - acc: 0.7424 - val_loss: 0.5199 - val_acc: 0.7402\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5143 - acc: 0.7418 - val_loss: 0.5210 - val_acc: 0.7399\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5142 - acc: 0.7425 - val_loss: 0.5200 - val_acc: 0.7400\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5142 - acc: 0.7413 - val_loss: 0.5202 - val_acc: 0.7400\n",
      "Best validation accuracy of eval run: 0.740249192434134\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 697us/step - loss: 0.5496 - acc: 0.7121 - val_loss: 0.5073 - val_acc: 0.7400\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4732 - acc: 0.7658 - val_loss: 0.4508 - val_acc: 0.7828\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4263 - acc: 0.7962 - val_loss: 0.4323 - val_acc: 0.7951\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.3923 - acc: 0.8160 - val_loss: 0.4113 - val_acc: 0.8068\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.3601 - acc: 0.8341 - val_loss: 0.4025 - val_acc: 0.8140\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.3296 - acc: 0.8513 - val_loss: 0.3997 - val_acc: 0.8170\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.3011 - acc: 0.8653 - val_loss: 0.4141 - val_acc: 0.8203\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.2763 - acc: 0.8796 - val_loss: 0.4182 - val_acc: 0.8198\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.2554 - acc: 0.8903 - val_loss: 0.4251 - val_acc: 0.8244\n",
      "Best validation accuracy of eval run: 0.8244393170292498\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 697us/step - loss: 0.5515 - acc: 0.7115 - val_loss: 0.4974 - val_acc: 0.7454\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4676 - acc: 0.7680 - val_loss: 0.4602 - val_acc: 0.7748\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4238 - acc: 0.7957 - val_loss: 0.4300 - val_acc: 0.7931\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3981 - acc: 0.8119 - val_loss: 0.4215 - val_acc: 0.8004\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.3830 - acc: 0.8206 - val_loss: 0.4197 - val_acc: 0.8010\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.3762 - acc: 0.8248 - val_loss: 0.4187 - val_acc: 0.8022\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3733 - acc: 0.8264 - val_loss: 0.4179 - val_acc: 0.8029\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.3712 - acc: 0.8275 - val_loss: 0.4183 - val_acc: 0.8028\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3705 - acc: 0.8274 - val_loss: 0.4182 - val_acc: 0.8029\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.3700 - acc: 0.8284 - val_loss: 0.4181 - val_acc: 0.8029\n",
      "Best validation accuracy of eval run: 0.8029349330894178\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 696us/step - loss: 0.5544 - acc: 0.7108 - val_loss: 0.5053 - val_acc: 0.7448\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4768 - acc: 0.7621 - val_loss: 0.4677 - val_acc: 0.7751\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4374 - acc: 0.7890 - val_loss: 0.4467 - val_acc: 0.7896\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4068 - acc: 0.8072 - val_loss: 0.4293 - val_acc: 0.7921\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.3803 - acc: 0.8241 - val_loss: 0.4113 - val_acc: 0.8073\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3569 - acc: 0.8367 - val_loss: 0.4046 - val_acc: 0.8136\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3348 - acc: 0.8492 - val_loss: 0.4056 - val_acc: 0.8142\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.3159 - acc: 0.8598 - val_loss: 0.4018 - val_acc: 0.8209\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.2984 - acc: 0.8687 - val_loss: 0.4107 - val_acc: 0.8194\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.2857 - acc: 0.8750 - val_loss: 0.4114 - val_acc: 0.8212\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.2739 - acc: 0.8812 - val_loss: 0.4149 - val_acc: 0.8212\n",
      "Best validation accuracy of eval run: 0.8212459621629684\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 697us/step - loss: 0.5611 - acc: 0.7045 - val_loss: 0.5054 - val_acc: 0.7453\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4997 - acc: 0.7490 - val_loss: 0.4801 - val_acc: 0.7623\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4737 - acc: 0.7659 - val_loss: 0.4650 - val_acc: 0.7737\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4557 - acc: 0.7775 - val_loss: 0.4498 - val_acc: 0.7819\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4411 - acc: 0.7881 - val_loss: 0.4442 - val_acc: 0.7876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4296 - acc: 0.7939 - val_loss: 0.4402 - val_acc: 0.7881\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4195 - acc: 0.8005 - val_loss: 0.4420 - val_acc: 0.7862\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4098 - acc: 0.8069 - val_loss: 0.4295 - val_acc: 0.7945\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4010 - acc: 0.8119 - val_loss: 0.4268 - val_acc: 0.7988\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.3948 - acc: 0.8154 - val_loss: 0.4225 - val_acc: 0.8001\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.3893 - acc: 0.8184 - val_loss: 0.4243 - val_acc: 0.7994\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.3847 - acc: 0.8222 - val_loss: 0.4213 - val_acc: 0.8020\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.3805 - acc: 0.8238 - val_loss: 0.4258 - val_acc: 0.7980\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3758 - acc: 0.8267 - val_loss: 0.4252 - val_acc: 0.8011\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3743 - acc: 0.8270 - val_loss: 0.4210 - val_acc: 0.8026\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3719 - acc: 0.8293 - val_loss: 0.4239 - val_acc: 0.8006\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.3701 - acc: 0.8299 - val_loss: 0.4241 - val_acc: 0.8009\n",
      "Epoch 18/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3690 - acc: 0.8307 - val_loss: 0.4224 - val_acc: 0.8017\n",
      "Best validation accuracy of eval run: 0.8026211352110679\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 698us/step - loss: 0.5476 - acc: 0.7141 - val_loss: 0.4995 - val_acc: 0.7485\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4768 - acc: 0.7640 - val_loss: 0.4525 - val_acc: 0.7815\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4356 - acc: 0.7910 - val_loss: 0.4291 - val_acc: 0.7910\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4015 - acc: 0.8105 - val_loss: 0.4166 - val_acc: 0.8040\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.3773 - acc: 0.8249 - val_loss: 0.4003 - val_acc: 0.8117\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3568 - acc: 0.8376 - val_loss: 0.4015 - val_acc: 0.8129\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3420 - acc: 0.8446 - val_loss: 0.4105 - val_acc: 0.8166\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.3312 - acc: 0.8518 - val_loss: 0.3940 - val_acc: 0.8204\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3661 - acc: 0.8307 - val_loss: 0.4235 - val_acc: 0.8028\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.4021 - acc: 0.8106 - val_loss: 0.4394 - val_acc: 0.7880\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.4025 - acc: 0.8109 - val_loss: 0.4206 - val_acc: 0.8040\n",
      "Best validation accuracy of eval run: 0.8204153207044861\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 696us/step - loss: 5.3203 - acc: 0.3004 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 10.0540 - acc: 0.0000e+00 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 10.0540 - acc: 0.0000e+00 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 10.0540 - acc: 0.0000e+00 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Best validation accuracy of eval run: 0.0\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 698us/step - loss: 0.5556 - acc: 0.7093 - val_loss: 0.5049 - val_acc: 0.7421\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4913 - acc: 0.7550 - val_loss: 0.4711 - val_acc: 0.7678\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4595 - acc: 0.7757 - val_loss: 0.4459 - val_acc: 0.7862\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4326 - acc: 0.7927 - val_loss: 0.4292 - val_acc: 0.7961\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4095 - acc: 0.8067 - val_loss: 0.4312 - val_acc: 0.7964\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3893 - acc: 0.8180 - val_loss: 0.4198 - val_acc: 0.7998\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3702 - acc: 0.8293 - val_loss: 0.4150 - val_acc: 0.8064\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.3562 - acc: 0.8369 - val_loss: 0.4212 - val_acc: 0.8060\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.3438 - acc: 0.8444 - val_loss: 0.4203 - val_acc: 0.8079\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.3336 - acc: 0.8504 - val_loss: 0.4194 - val_acc: 0.8116\n",
      "Best validation accuracy of eval run: 0.8115920627441723\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 697us/step - loss: 0.5485 - acc: 0.7132 - val_loss: 0.5260 - val_acc: 0.7291\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4744 - acc: 0.7646 - val_loss: 0.4566 - val_acc: 0.7776\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4273 - acc: 0.7945 - val_loss: 0.4240 - val_acc: 0.8000\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.3902 - acc: 0.8170 - val_loss: 0.4056 - val_acc: 0.8102\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.3556 - acc: 0.8369 - val_loss: 0.4005 - val_acc: 0.8168\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3236 - acc: 0.8541 - val_loss: 0.3932 - val_acc: 0.8235\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.2929 - acc: 0.8712 - val_loss: 0.3969 - val_acc: 0.8282\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.2653 - acc: 0.8848 - val_loss: 0.4074 - val_acc: 0.8274\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.2384 - acc: 0.8982 - val_loss: 0.4317 - val_acc: 0.8288\n",
      "Best validation accuracy of eval run: 0.828795569896918\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 697us/step - loss: 0.7739 - acc: 0.5781 - val_loss: 0.6387 - val_acc: 0.6166\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.6541 - acc: 0.6187 - val_loss: 0.6267 - val_acc: 0.6333\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.6583 - acc: 0.6167 - val_loss: 0.6321 - val_acc: 0.6389\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.6535 - acc: 0.6221 - val_loss: 0.6765 - val_acc: 0.5465\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.6538 - acc: 0.6245 - val_loss: 0.7174 - val_acc: 0.5270\n",
      "Best validation accuracy of eval run: 0.6389293954820198\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 697us/step - loss: 0.5486 - acc: 0.7128 - val_loss: 0.5003 - val_acc: 0.7464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4770 - acc: 0.7622 - val_loss: 0.4589 - val_acc: 0.7771\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4325 - acc: 0.7909 - val_loss: 0.4274 - val_acc: 0.7961\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3971 - acc: 0.8116 - val_loss: 0.4148 - val_acc: 0.8048\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3661 - acc: 0.8299 - val_loss: 0.4011 - val_acc: 0.8140\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3423 - acc: 0.8433 - val_loss: 0.4030 - val_acc: 0.8171\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3201 - acc: 0.8554 - val_loss: 0.4004 - val_acc: 0.8193\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.3028 - acc: 0.8651 - val_loss: 0.4003 - val_acc: 0.8229\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.2881 - acc: 0.8726 - val_loss: 0.4091 - val_acc: 0.8214\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.2766 - acc: 0.8781 - val_loss: 0.4065 - val_acc: 0.8238\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.2701 - acc: 0.8811 - val_loss: 0.4125 - val_acc: 0.8244\n",
      "Best validation accuracy of eval run: 0.8243839409332645\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 698us/step - loss: 0.5481 - acc: 0.7146 - val_loss: 0.4988 - val_acc: 0.7501\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4740 - acc: 0.7649 - val_loss: 0.4580 - val_acc: 0.7790\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4262 - acc: 0.7960 - val_loss: 0.4250 - val_acc: 0.7974\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3804 - acc: 0.8233 - val_loss: 0.4095 - val_acc: 0.8082\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3354 - acc: 0.8480 - val_loss: 0.3995 - val_acc: 0.8191\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.2897 - acc: 0.8732 - val_loss: 0.4119 - val_acc: 0.8207\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.2451 - acc: 0.8956 - val_loss: 0.4302 - val_acc: 0.8212\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.2043 - acc: 0.9153 - val_loss: 0.4537 - val_acc: 0.8282\n",
      "Best validation accuracy of eval run: 0.828241808953569\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 697us/step - loss: 0.5481 - acc: 0.7147 - val_loss: 0.4964 - val_acc: 0.7500\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4757 - acc: 0.7631 - val_loss: 0.4531 - val_acc: 0.7795\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4238 - acc: 0.7961 - val_loss: 0.4260 - val_acc: 0.7974\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3780 - acc: 0.8242 - val_loss: 0.4003 - val_acc: 0.8118\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3344 - acc: 0.8490 - val_loss: 0.4026 - val_acc: 0.8188\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.2911 - acc: 0.8715 - val_loss: 0.4019 - val_acc: 0.8194\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.2510 - acc: 0.8926 - val_loss: 0.4301 - val_acc: 0.8207\n",
      "Best validation accuracy of eval run: 0.8206552837870887\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 696us/step - loss: 0.5813 - acc: 0.6906 - val_loss: 0.5342 - val_acc: 0.7306\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5300 - acc: 0.7293 - val_loss: 0.5223 - val_acc: 0.7355\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5186 - acc: 0.7376 - val_loss: 0.5149 - val_acc: 0.7387\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5129 - acc: 0.7426 - val_loss: 0.5139 - val_acc: 0.7414\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5065 - acc: 0.7469 - val_loss: 0.5106 - val_acc: 0.7388\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5035 - acc: 0.7493 - val_loss: 0.5047 - val_acc: 0.7482\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5004 - acc: 0.7508 - val_loss: 0.5120 - val_acc: 0.7397\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5085 - acc: 0.7441 - val_loss: 0.5204 - val_acc: 0.7353\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5029 - acc: 0.7476 - val_loss: 0.4967 - val_acc: 0.7535\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4973 - acc: 0.7520 - val_loss: 0.4966 - val_acc: 0.7537\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4935 - acc: 0.7549 - val_loss: 0.4968 - val_acc: 0.7537\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4878 - acc: 0.7584 - val_loss: 0.4884 - val_acc: 0.7575\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4836 - acc: 0.7621 - val_loss: 0.4809 - val_acc: 0.7662\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4802 - acc: 0.7653 - val_loss: 0.4798 - val_acc: 0.7665\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4761 - acc: 0.7676 - val_loss: 0.4777 - val_acc: 0.7669\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4729 - acc: 0.7699 - val_loss: 0.4850 - val_acc: 0.7616\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4684 - acc: 0.7730 - val_loss: 0.4775 - val_acc: 0.7676\n",
      "Epoch 18/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4669 - acc: 0.7753 - val_loss: 0.4755 - val_acc: 0.7702\n",
      "Epoch 19/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.4661 - acc: 0.7753 - val_loss: 0.4681 - val_acc: 0.7731\n",
      "Epoch 20/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4599 - acc: 0.7780 - val_loss: 0.4699 - val_acc: 0.7716\n",
      "Best validation accuracy of eval run: 0.7730872173533772\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 696us/step - loss: 0.9970 - acc: 0.5313 - val_loss: 0.6446 - val_acc: 0.6338\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.6445 - acc: 0.6414 - val_loss: 0.6014 - val_acc: 0.6812\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.6076 - acc: 0.6709 - val_loss: 0.5947 - val_acc: 0.6796\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5887 - acc: 0.6859 - val_loss: 0.5770 - val_acc: 0.6946\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5723 - acc: 0.6996 - val_loss: 0.5559 - val_acc: 0.7112\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5625 - acc: 0.7053 - val_loss: 0.5547 - val_acc: 0.7126\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5545 - acc: 0.7129 - val_loss: 0.5460 - val_acc: 0.7195\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5472 - acc: 0.7197 - val_loss: 0.5394 - val_acc: 0.7267\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5432 - acc: 0.7217 - val_loss: 0.5368 - val_acc: 0.7272\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5398 - acc: 0.7243 - val_loss: 0.5357 - val_acc: 0.7287\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5366 - acc: 0.7277 - val_loss: 0.5330 - val_acc: 0.7299\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5340 - acc: 0.7300 - val_loss: 0.5295 - val_acc: 0.7337\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5322 - acc: 0.7316 - val_loss: 0.5299 - val_acc: 0.7331\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5310 - acc: 0.7324 - val_loss: 0.5285 - val_acc: 0.7332\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5301 - acc: 0.7333 - val_loss: 0.5274 - val_acc: 0.7354\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5299 - acc: 0.7324 - val_loss: 0.5267 - val_acc: 0.7352\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5290 - acc: 0.7333 - val_loss: 0.5273 - val_acc: 0.7353\n",
      "Epoch 18/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5287 - acc: 0.7334 - val_loss: 0.5266 - val_acc: 0.7354\n",
      "Epoch 19/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5288 - acc: 0.7335 - val_loss: 0.5269 - val_acc: 0.7353\n",
      "Epoch 20/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5285 - acc: 0.7346 - val_loss: 0.5267 - val_acc: 0.7355\n",
      "Best validation accuracy of eval run: 0.7354868481640008\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 697us/step - loss: 0.5621 - acc: 0.7033 - val_loss: 0.5198 - val_acc: 0.7327\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5026 - acc: 0.7461 - val_loss: 0.4774 - val_acc: 0.7650\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4718 - acc: 0.7667 - val_loss: 0.4609 - val_acc: 0.7757\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4532 - acc: 0.7780 - val_loss: 0.4448 - val_acc: 0.7857\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4399 - acc: 0.7864 - val_loss: 0.4385 - val_acc: 0.7891\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4313 - acc: 0.7924 - val_loss: 0.4366 - val_acc: 0.7898\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4265 - acc: 0.7947 - val_loss: 0.4364 - val_acc: 0.7898\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4240 - acc: 0.7957 - val_loss: 0.4328 - val_acc: 0.7924\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4214 - acc: 0.7980 - val_loss: 0.4329 - val_acc: 0.7925\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4204 - acc: 0.7984 - val_loss: 0.4319 - val_acc: 0.7928\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.4198 - acc: 0.7988 - val_loss: 0.4325 - val_acc: 0.7918\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4194 - acc: 0.7993 - val_loss: 0.4320 - val_acc: 0.7929\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4193 - acc: 0.7989 - val_loss: 0.4318 - val_acc: 0.7928\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4190 - acc: 0.7994 - val_loss: 0.4318 - val_acc: 0.7929\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4203 - acc: 0.7982 - val_loss: 0.4318 - val_acc: 0.7929\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4189 - acc: 0.7996 - val_loss: 0.4318 - val_acc: 0.7928\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4192 - acc: 0.7995 - val_loss: 0.4317 - val_acc: 0.7928\n",
      "Epoch 18/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4185 - acc: 0.7994 - val_loss: 0.4317 - val_acc: 0.7929\n",
      "Epoch 19/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4193 - acc: 0.7996 - val_loss: 0.4317 - val_acc: 0.7929\n",
      "Epoch 20/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4193 - acc: 0.7998 - val_loss: 0.4317 - val_acc: 0.7929\n",
      "Best validation accuracy of eval run: 0.7929303184147524\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 697us/step - loss: 0.6054 - acc: 0.6769 - val_loss: 0.5625 - val_acc: 0.7110\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5545 - acc: 0.7130 - val_loss: 0.5869 - val_acc: 0.7071\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5459 - acc: 0.7186 - val_loss: 0.5400 - val_acc: 0.7243\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5395 - acc: 0.7227 - val_loss: 0.5326 - val_acc: 0.7256\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5320 - acc: 0.7291 - val_loss: 0.5279 - val_acc: 0.7319\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5274 - acc: 0.7323 - val_loss: 0.5211 - val_acc: 0.7364\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5203 - acc: 0.7379 - val_loss: 0.5193 - val_acc: 0.7425\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5136 - acc: 0.7421 - val_loss: 0.5184 - val_acc: 0.7392\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5095 - acc: 0.7449 - val_loss: 0.5099 - val_acc: 0.7443\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5034 - acc: 0.7499 - val_loss: 0.5025 - val_acc: 0.7530\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4976 - acc: 0.7539 - val_loss: 0.4975 - val_acc: 0.7542\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.4917 - acc: 0.7581 - val_loss: 0.4932 - val_acc: 0.7571\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4856 - acc: 0.7627 - val_loss: 0.4912 - val_acc: 0.7581\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4804 - acc: 0.7651 - val_loss: 0.4829 - val_acc: 0.7645\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4748 - acc: 0.7695 - val_loss: 0.4801 - val_acc: 0.7660\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4690 - acc: 0.7738 - val_loss: 0.4770 - val_acc: 0.7691\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4641 - acc: 0.7770 - val_loss: 0.4756 - val_acc: 0.7709\n",
      "Epoch 18/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.4586 - acc: 0.7805 - val_loss: 0.4716 - val_acc: 0.7725\n",
      "Epoch 19/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4539 - acc: 0.7836 - val_loss: 0.4689 - val_acc: 0.7758\n",
      "Epoch 20/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4493 - acc: 0.7867 - val_loss: 0.4686 - val_acc: 0.7750\n",
      "Best validation accuracy of eval run: 0.7758191047564156\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 697us/step - loss: 0.5742 - acc: 0.6949 - val_loss: 0.5385 - val_acc: 0.7131\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.5448 - acc: 0.7175 - val_loss: 0.5479 - val_acc: 0.7105\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5482 - acc: 0.7163 - val_loss: 0.5300 - val_acc: 0.7289\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5520 - acc: 0.7139 - val_loss: 0.5491 - val_acc: 0.7177\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5550 - acc: 0.7110 - val_loss: 0.5427 - val_acc: 0.7187\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5574 - acc: 0.7094 - val_loss: 0.5470 - val_acc: 0.7189\n",
      "Best validation accuracy of eval run: 0.7288601753433344\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 697us/step - loss: 0.5517 - acc: 0.7118 - val_loss: 0.4997 - val_acc: 0.7519\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4759 - acc: 0.7653 - val_loss: 0.4619 - val_acc: 0.7739\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4322 - acc: 0.7929 - val_loss: 0.4400 - val_acc: 0.7905\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3938 - acc: 0.8159 - val_loss: 0.4215 - val_acc: 0.8020\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3577 - acc: 0.8360 - val_loss: 0.4178 - val_acc: 0.8066\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.3270 - acc: 0.8531 - val_loss: 0.4198 - val_acc: 0.8092\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3047 - acc: 0.8661 - val_loss: 0.4241 - val_acc: 0.8118\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.2882 - acc: 0.8740 - val_loss: 0.4312 - val_acc: 0.8107\n",
      "Best validation accuracy of eval run: 0.8117766497483934\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 696us/step - loss: 0.5532 - acc: 0.7089 - val_loss: 0.5081 - val_acc: 0.7419\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4763 - acc: 0.7641 - val_loss: 0.4605 - val_acc: 0.7775\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.4310 - acc: 0.7920 - val_loss: 0.4282 - val_acc: 0.7946\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.3982 - acc: 0.8122 - val_loss: 0.4151 - val_acc: 0.8054\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.3742 - acc: 0.8256 - val_loss: 0.4126 - val_acc: 0.8085\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.3575 - acc: 0.8361 - val_loss: 0.4084 - val_acc: 0.8128\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3466 - acc: 0.8410 - val_loss: 0.4079 - val_acc: 0.8143\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3388 - acc: 0.8457 - val_loss: 0.4085 - val_acc: 0.8156\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.3341 - acc: 0.8482 - val_loss: 0.4090 - val_acc: 0.8157\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.3309 - acc: 0.8495 - val_loss: 0.4111 - val_acc: 0.8146\n",
      "Best validation accuracy of eval run: 0.8156529764497561\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 696us/step - loss: 0.6499 - acc: 0.6295 - val_loss: 0.6144 - val_acc: 0.6536\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.6058 - acc: 0.6736 - val_loss: 0.6112 - val_acc: 0.6879\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5818 - acc: 0.6897 - val_loss: 0.5640 - val_acc: 0.7025\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5658 - acc: 0.7029 - val_loss: 0.5581 - val_acc: 0.7097\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5532 - acc: 0.7106 - val_loss: 0.5461 - val_acc: 0.7137\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5429 - acc: 0.7181 - val_loss: 0.5298 - val_acc: 0.7273\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5355 - acc: 0.7232 - val_loss: 0.5231 - val_acc: 0.7326\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5289 - acc: 0.7284 - val_loss: 0.5197 - val_acc: 0.7352\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5243 - acc: 0.7328 - val_loss: 0.5160 - val_acc: 0.7378\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5202 - acc: 0.7345 - val_loss: 0.5128 - val_acc: 0.7406\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5180 - acc: 0.7360 - val_loss: 0.5114 - val_acc: 0.7404\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5163 - acc: 0.7386 - val_loss: 0.5098 - val_acc: 0.7420\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 148s 683us/step - loss: 0.5152 - acc: 0.7390 - val_loss: 0.5089 - val_acc: 0.7436\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5137 - acc: 0.7402 - val_loss: 0.5088 - val_acc: 0.7440\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5133 - acc: 0.7401 - val_loss: 0.5083 - val_acc: 0.7443\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5122 - acc: 0.7408 - val_loss: 0.5080 - val_acc: 0.7448\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5121 - acc: 0.7416 - val_loss: 0.5079 - val_acc: 0.7451\n",
      "Epoch 18/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5133 - acc: 0.7394 - val_loss: 0.5078 - val_acc: 0.7451\n",
      "Epoch 19/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5122 - acc: 0.7424 - val_loss: 0.5078 - val_acc: 0.7453\n",
      "Epoch 20/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5130 - acc: 0.7398 - val_loss: 0.5079 - val_acc: 0.7450\n",
      "Best validation accuracy of eval run: 0.745343793248272\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 696us/step - loss: 0.5664 - acc: 0.7010 - val_loss: 0.5262 - val_acc: 0.7275\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5139 - acc: 0.7383 - val_loss: 0.5031 - val_acc: 0.7493\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5038 - acc: 0.7452 - val_loss: 0.4949 - val_acc: 0.7530\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5021 - acc: 0.7478 - val_loss: 0.5198 - val_acc: 0.7332\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5152 - acc: 0.7400 - val_loss: 0.4885 - val_acc: 0.7546\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5017 - acc: 0.7479 - val_loss: 0.5096 - val_acc: 0.7450\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5011 - acc: 0.7481 - val_loss: 0.4936 - val_acc: 0.7553\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5037 - acc: 0.7470 - val_loss: 0.4918 - val_acc: 0.7567\n",
      "Best validation accuracy of eval run: 0.7566774342265831\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 697us/step - loss: 0.6708 - acc: 0.6130 - val_loss: 0.6230 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.6209 - acc: 0.6591 - val_loss: 0.6127 - val_acc: 0.6624\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.6157 - acc: 0.6665 - val_loss: 0.5984 - val_acc: 0.6801\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.6021 - acc: 0.6786 - val_loss: 0.5974 - val_acc: 0.6843\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5971 - acc: 0.6817 - val_loss: 0.5821 - val_acc: 0.6916\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5927 - acc: 0.6846 - val_loss: 0.5789 - val_acc: 0.6969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5898 - acc: 0.6864 - val_loss: 0.5789 - val_acc: 0.6962\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5883 - acc: 0.6879 - val_loss: 0.5782 - val_acc: 0.6950\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5876 - acc: 0.6881 - val_loss: 0.5784 - val_acc: 0.6933\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5864 - acc: 0.6890 - val_loss: 0.5783 - val_acc: 0.6966\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5855 - acc: 0.6887 - val_loss: 0.5757 - val_acc: 0.6986\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5842 - acc: 0.6900 - val_loss: 0.5782 - val_acc: 0.6954\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5842 - acc: 0.6899 - val_loss: 0.5756 - val_acc: 0.6973\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5842 - acc: 0.6899 - val_loss: 0.5754 - val_acc: 0.6976\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5835 - acc: 0.6911 - val_loss: 0.5755 - val_acc: 0.6982\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5837 - acc: 0.6906 - val_loss: 0.5760 - val_acc: 0.6982\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5835 - acc: 0.6911 - val_loss: 0.5762 - val_acc: 0.6981\n",
      "Best validation accuracy of eval run: 0.6985509921396499\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 696us/step - loss: 0.6080 - acc: 0.6721 - val_loss: 0.5615 - val_acc: 0.7065\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5576 - acc: 0.7102 - val_loss: 0.5420 - val_acc: 0.7241\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5458 - acc: 0.7181 - val_loss: 0.5264 - val_acc: 0.7303\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5392 - acc: 0.7218 - val_loss: 0.5276 - val_acc: 0.7299\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5322 - acc: 0.7261 - val_loss: 0.5196 - val_acc: 0.7363\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 152s 704us/step - loss: 0.5243 - acc: 0.7315 - val_loss: 0.5108 - val_acc: 0.7426\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5177 - acc: 0.7369 - val_loss: 0.5142 - val_acc: 0.7391\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5109 - acc: 0.7413 - val_loss: 0.5012 - val_acc: 0.7494\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5058 - acc: 0.7447 - val_loss: 0.5006 - val_acc: 0.7492\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5029 - acc: 0.7470 - val_loss: 0.4963 - val_acc: 0.7529\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5001 - acc: 0.7495 - val_loss: 0.4925 - val_acc: 0.7556\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4981 - acc: 0.7516 - val_loss: 0.4953 - val_acc: 0.7527\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4968 - acc: 0.7515 - val_loss: 0.4925 - val_acc: 0.7552\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.4954 - acc: 0.7524 - val_loss: 0.4938 - val_acc: 0.7539\n",
      "Best validation accuracy of eval run: 0.755625288419367\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 696us/step - loss: 0.5830 - acc: 0.6917 - val_loss: 0.5445 - val_acc: 0.7225\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5519 - acc: 0.7163 - val_loss: 0.5375 - val_acc: 0.7240\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5515 - acc: 0.7145 - val_loss: 0.5419 - val_acc: 0.7233\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5463 - acc: 0.7177 - val_loss: 0.5338 - val_acc: 0.7255\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5437 - acc: 0.7197 - val_loss: 0.5314 - val_acc: 0.7308\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5420 - acc: 0.7205 - val_loss: 0.5270 - val_acc: 0.7320\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5405 - acc: 0.7221 - val_loss: 0.5257 - val_acc: 0.7318\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5397 - acc: 0.7225 - val_loss: 0.5280 - val_acc: 0.7289\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5380 - acc: 0.7232 - val_loss: 0.5209 - val_acc: 0.7349\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5372 - acc: 0.7234 - val_loss: 0.5240 - val_acc: 0.7349\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5354 - acc: 0.7256 - val_loss: 0.5218 - val_acc: 0.7353\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5350 - acc: 0.7255 - val_loss: 0.5193 - val_acc: 0.7368\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5344 - acc: 0.7263 - val_loss: 0.5200 - val_acc: 0.7359\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5342 - acc: 0.7253 - val_loss: 0.5188 - val_acc: 0.7378\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 148s 685us/step - loss: 0.5337 - acc: 0.7262 - val_loss: 0.5177 - val_acc: 0.7376\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5324 - acc: 0.7276 - val_loss: 0.5167 - val_acc: 0.7391\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5324 - acc: 0.7271 - val_loss: 0.5166 - val_acc: 0.7376\n",
      "Epoch 18/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5318 - acc: 0.7269 - val_loss: 0.5155 - val_acc: 0.7387\n",
      "Epoch 19/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5313 - acc: 0.7276 - val_loss: 0.5173 - val_acc: 0.7376\n",
      "Epoch 20/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5317 - acc: 0.7271 - val_loss: 0.5196 - val_acc: 0.7373\n",
      "Best validation accuracy of eval run: 0.7391416705144294\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 696us/step - loss: 0.8921 - acc: 0.5773 - val_loss: 0.6975 - val_acc: 0.6298\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.6140 - acc: 0.6603 - val_loss: 0.6073 - val_acc: 0.6721\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5801 - acc: 0.6907 - val_loss: 0.5622 - val_acc: 0.6997\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5623 - acc: 0.7034 - val_loss: 0.5502 - val_acc: 0.7139\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5490 - acc: 0.7148 - val_loss: 0.5411 - val_acc: 0.7211\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5395 - acc: 0.7221 - val_loss: 0.5360 - val_acc: 0.7242\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5339 - acc: 0.7271 - val_loss: 0.5314 - val_acc: 0.7267\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5320 - acc: 0.7281 - val_loss: 0.5304 - val_acc: 0.7287\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5307 - acc: 0.7291 - val_loss: 0.5294 - val_acc: 0.7294\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5295 - acc: 0.7295 - val_loss: 0.5293 - val_acc: 0.7303\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5293 - acc: 0.7290 - val_loss: 0.5294 - val_acc: 0.7299\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5293 - acc: 0.7302 - val_loss: 0.5292 - val_acc: 0.7299\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5295 - acc: 0.7308 - val_loss: 0.5292 - val_acc: 0.7302\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5293 - acc: 0.7301 - val_loss: 0.5292 - val_acc: 0.7302\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5294 - acc: 0.7302 - val_loss: 0.5291 - val_acc: 0.7302\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5290 - acc: 0.7301 - val_loss: 0.5291 - val_acc: 0.7302\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5295 - acc: 0.7295 - val_loss: 0.5291 - val_acc: 0.7301\n",
      "Epoch 18/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5290 - acc: 0.7307 - val_loss: 0.5291 - val_acc: 0.7300\n",
      "Epoch 19/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5290 - acc: 0.7311 - val_loss: 0.5291 - val_acc: 0.7300\n",
      "Epoch 20/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5292 - acc: 0.7307 - val_loss: 0.5291 - val_acc: 0.7301\n",
      "Best validation accuracy of eval run: 0.7303184125376122\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 696us/step - loss: 0.5996 - acc: 0.6759 - val_loss: 0.6111 - val_acc: 0.6912\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5727 - acc: 0.7048 - val_loss: 0.5741 - val_acc: 0.7190\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5666 - acc: 0.7084 - val_loss: 0.5673 - val_acc: 0.7220\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5633 - acc: 0.7112 - val_loss: 0.5903 - val_acc: 0.7254\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5621 - acc: 0.7137 - val_loss: 0.6044 - val_acc: 0.7206\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5609 - acc: 0.7140 - val_loss: 0.5578 - val_acc: 0.7244\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5588 - acc: 0.7146 - val_loss: 0.5625 - val_acc: 0.7227\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.5595 - acc: 0.7162 - val_loss: 0.5612 - val_acc: 0.7193\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5596 - acc: 0.7155 - val_loss: 0.5927 - val_acc: 0.7249\n",
      "Best validation accuracy of eval run: 0.7254268574087532\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 697us/step - loss: 0.5768 - acc: 0.6977 - val_loss: 0.8636 - val_acc: 0.6324\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5561 - acc: 0.7132 - val_loss: 0.4942 - val_acc: 0.7529\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4917 - acc: 0.7548 - val_loss: 0.4851 - val_acc: 0.7598\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4721 - acc: 0.7685 - val_loss: 0.4634 - val_acc: 0.7753\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4557 - acc: 0.7785 - val_loss: 0.4535 - val_acc: 0.7818\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4445 - acc: 0.7856 - val_loss: 0.4479 - val_acc: 0.7863\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4347 - acc: 0.7922 - val_loss: 0.4432 - val_acc: 0.7873\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4261 - acc: 0.7968 - val_loss: 0.4358 - val_acc: 0.7930\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4211 - acc: 0.8005 - val_loss: 0.4338 - val_acc: 0.7921\n",
      "Epoch 10/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4135 - acc: 0.8055 - val_loss: 0.4352 - val_acc: 0.7951\n",
      "Epoch 11/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4101 - acc: 0.8071 - val_loss: 0.4276 - val_acc: 0.7978\n",
      "Epoch 12/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4068 - acc: 0.8084 - val_loss: 0.4265 - val_acc: 0.7983\n",
      "Epoch 13/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4043 - acc: 0.8097 - val_loss: 0.4278 - val_acc: 0.7974\n",
      "Epoch 14/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4018 - acc: 0.8120 - val_loss: 0.4284 - val_acc: 0.7966\n",
      "Epoch 15/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4015 - acc: 0.8124 - val_loss: 0.4240 - val_acc: 0.7999\n",
      "Epoch 16/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3987 - acc: 0.8141 - val_loss: 0.4268 - val_acc: 0.7987\n",
      "Epoch 17/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3982 - acc: 0.8144 - val_loss: 0.4253 - val_acc: 0.7988\n",
      "Epoch 18/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.3982 - acc: 0.8144 - val_loss: 0.4249 - val_acc: 0.7994\n",
      "Best validation accuracy of eval run: 0.7998892478091297\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 698us/step - loss: 0.5503 - acc: 0.7134 - val_loss: 0.5031 - val_acc: 0.7512\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.4654 - acc: 0.7718 - val_loss: 0.4560 - val_acc: 0.7818\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.4037 - acc: 0.8103 - val_loss: 0.4226 - val_acc: 0.8012\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.3434 - acc: 0.8436 - val_loss: 0.4154 - val_acc: 0.8092\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.2884 - acc: 0.8737 - val_loss: 0.4281 - val_acc: 0.8160\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.2447 - acc: 0.8960 - val_loss: 0.4610 - val_acc: 0.8169\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.2133 - acc: 0.9111 - val_loss: 0.4889 - val_acc: 0.8162\n",
      "Best validation accuracy of eval run: 0.8169450853560781\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 696us/step - loss: 1.0512 - acc: 0.5311 - val_loss: 0.6876 - val_acc: 0.6294\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.7037 - acc: 0.5881 - val_loss: 0.6664 - val_acc: 0.6292\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 6.6557 - acc: 0.2180 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 10.0540 - acc: 0.0000e+00 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 10.0540 - acc: 0.0000e+00 - val_loss: 10.0342 - val_acc: 0.0000e+00\n",
      "Best validation accuracy of eval run: 0.6294047069725597\n",
      "Train on 216697 samples, validate on 54175 samples\n",
      "Epoch 1/20\n",
      "216697/216697 [==============================] - 151s 696us/step - loss: 0.6188 - acc: 0.6613 - val_loss: 0.5584 - val_acc: 0.7110\n",
      "Epoch 2/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5471 - acc: 0.7203 - val_loss: 0.5394 - val_acc: 0.7244\n",
      "Epoch 3/20\n",
      "216697/216697 [==============================] - 149s 686us/step - loss: 0.5291 - acc: 0.7321 - val_loss: 0.5254 - val_acc: 0.7346\n",
      "Epoch 4/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5185 - acc: 0.7391 - val_loss: 0.5177 - val_acc: 0.7388\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5139 - acc: 0.7421 - val_loss: 0.5136 - val_acc: 0.7427\n",
      "Epoch 6/20\n",
      "216697/216697 [==============================] - 149s 687us/step - loss: 0.5116 - acc: 0.7447 - val_loss: 0.5136 - val_acc: 0.7417\n",
      "Epoch 7/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5106 - acc: 0.7447 - val_loss: 0.5141 - val_acc: 0.7401\n",
      "Epoch 8/20\n",
      "216697/216697 [==============================] - 149s 688us/step - loss: 0.5103 - acc: 0.7449 - val_loss: 0.5143 - val_acc: 0.7397\n",
      "Epoch 9/20\n",
      "216697/216697 [==============================] - 149s 689us/step - loss: 0.5100 - acc: 0.7449 - val_loss: 0.5141 - val_acc: 0.7398\n",
      "Best validation accuracy of eval run: 0.7427226580372042\n"
     ]
    }
   ],
   "source": [
    "init_session()\n",
    "model2_run2_trials=Trials()\n",
    "\n",
    "# hyperopt optimization\n",
    "model2_run2_best = fmin(opt_deep_fn,\n",
    "                        search_deep_space,\n",
    "                          algo=tpe.suggest,\n",
    "                          max_evals=50,\n",
    "                           trials=model2_run2_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performing model hyper-parameters:\n",
      "{'decay': 0.18751200473274587, 'dropout_rate1': 0.3503452759526128, 'dropout_rate2': 0.007445348682955651, 'learn_rate': 0.0027504223239299217}\n"
     ]
    }
   ],
   "source": [
    "#X_train, Y_train, X_test, Y_test = data()\n",
    "#print(\"Evaluation of best performing model:\")\n",
    "#print(best_model.evaluate(X_test, Y_test))\n",
    "print(\"Best performing model hyper-parameters:\")\n",
    "print(model2_run2_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model2_run2_best, open(data_folder+'model2_run2_best.p', 'wb'))\n",
    "pickle.dump(model2_run2_trials, open(data_folder+'model2_run2_trials.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_run2_trials = pickle.load(open(data_folder+'model2_run2_trials.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 578.93x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_optimization_history(model2_run2_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ha_train'></a>\n",
    "#### Train optimized model\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from hyperopt import STATUS_OK\n",
    "\n",
    "x_train, y_train, x_test, y_test = data()\n",
    "w2v = pickle.load(open(data_folder+'w2v.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2, 50)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 2, 400)            321943200 \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 2, 400)            721200    \n",
      "_________________________________________________________________\n",
      "attention__layer_2 (Attentio (None, 400)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 322,745,201\n",
      "Trainable params: 1,423,601\n",
      "Non-trainable params: 321,321,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "m2 = build_hier_model(vectors=w2v, optimizer='rmsprop', \n",
    "                             learn_rate=0.0, dropout_rate1=0.3504, \n",
    "                             dropout_rate2=0.0074, max_length=50, projected_dim=200, \n",
    "                             num_classes=1, num_hidden=200)\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
    "m2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 243784 samples, validate on 27088 samples\n",
      "Epoch 1/40\n",
      "243784/243784 [==============================] - 161s 660us/step - loss: 0.5419 - acc: 0.7173 - val_loss: 0.5068 - val_acc: 0.7480\n",
      "Epoch 2/40\n",
      "243784/243784 [==============================] - 160s 654us/step - loss: 0.4715 - acc: 0.7676 - val_loss: 0.4535 - val_acc: 0.7813\n",
      "Epoch 3/40\n",
      "243784/243784 [==============================] - 159s 654us/step - loss: 0.4252 - acc: 0.7963 - val_loss: 0.4240 - val_acc: 0.7979\n",
      "Epoch 4/40\n",
      "243784/243784 [==============================] - 160s 655us/step - loss: 0.3866 - acc: 0.8189 - val_loss: 0.4066 - val_acc: 0.8104\n",
      "Epoch 5/40\n",
      "243784/243784 [==============================] - 160s 654us/step - loss: 0.3533 - acc: 0.8375 - val_loss: 0.3950 - val_acc: 0.8184\n",
      "Epoch 6/40\n",
      "243784/243784 [==============================] - 160s 655us/step - loss: 0.3222 - acc: 0.8552 - val_loss: 0.4026 - val_acc: 0.8200\n",
      "Epoch 7/40\n",
      "243784/243784 [==============================] - 160s 656us/step - loss: 0.2944 - acc: 0.8700 - val_loss: 0.3959 - val_acc: 0.8272\n",
      "Epoch 8/40\n",
      "243784/243784 [==============================] - 160s 654us/step - loss: 0.2687 - acc: 0.8833 - val_loss: 0.4064 - val_acc: 0.8273\n",
      "Epoch 9/40\n",
      "243784/243784 [==============================] - 160s 655us/step - loss: 0.2452 - acc: 0.8946 - val_loss: 0.4175 - val_acc: 0.8254\n",
      "Epoch 10/40\n",
      "243784/243784 [==============================] - 160s 656us/step - loss: 0.2252 - acc: 0.9054 - val_loss: 0.4255 - val_acc: 0.8296\n",
      "Epoch 11/40\n",
      "243784/243784 [==============================] - 160s 656us/step - loss: 0.2058 - acc: 0.9140 - val_loss: 0.4382 - val_acc: 0.8310\n",
      "Epoch 12/40\n",
      "243784/243784 [==============================] - 160s 656us/step - loss: 0.1907 - acc: 0.9209 - val_loss: 0.4632 - val_acc: 0.8307\n",
      "Epoch 13/40\n",
      "243784/243784 [==============================] - 159s 654us/step - loss: 0.1763 - acc: 0.9272 - val_loss: 0.4845 - val_acc: 0.8343\n",
      "Epoch 14/40\n",
      "243784/243784 [==============================] - 160s 656us/step - loss: 0.1653 - acc: 0.9325 - val_loss: 0.4871 - val_acc: 0.8342\n",
      "Epoch 15/40\n",
      "243784/243784 [==============================] - 160s 656us/step - loss: 0.1559 - acc: 0.9365 - val_loss: 0.5046 - val_acc: 0.8317\n",
      "Epoch 16/40\n",
      "243784/243784 [==============================] - 160s 655us/step - loss: 0.1472 - acc: 0.9406 - val_loss: 0.5195 - val_acc: 0.8309\n",
      "Epoch 17/40\n",
      "243784/243784 [==============================] - 159s 654us/step - loss: 0.1419 - acc: 0.9430 - val_loss: 0.5260 - val_acc: 0.8328\n",
      "Epoch 18/40\n",
      "243784/243784 [==============================] - 160s 657us/step - loss: 0.1362 - acc: 0.9454 - val_loss: 0.5304 - val_acc: 0.8321\n",
      "Epoch 19/40\n",
      "243784/243784 [==============================] - 160s 655us/step - loss: 0.1315 - acc: 0.9471 - val_loss: 0.5413 - val_acc: 0.8312\n",
      "Epoch 20/40\n",
      "243784/243784 [==============================] - 160s 655us/step - loss: 0.1274 - acc: 0.9492 - val_loss: 0.5565 - val_acc: 0.8313\n",
      "Epoch 21/40\n",
      "243784/243784 [==============================] - 160s 655us/step - loss: 0.1243 - acc: 0.9507 - val_loss: 0.5633 - val_acc: 0.8320\n",
      "Epoch 22/40\n",
      "243784/243784 [==============================] - 160s 655us/step - loss: 0.1198 - acc: 0.9521 - val_loss: 0.5684 - val_acc: 0.8328\n",
      "Epoch 23/40\n",
      "243784/243784 [==============================] - 160s 655us/step - loss: 0.1185 - acc: 0.9528 - val_loss: 0.5738 - val_acc: 0.8323\n",
      "Epoch 24/40\n",
      "243784/243784 [==============================] - 160s 657us/step - loss: 0.1168 - acc: 0.9537 - val_loss: 0.5747 - val_acc: 0.8337\n",
      "Epoch 25/40\n",
      "243784/243784 [==============================] - 160s 656us/step - loss: 0.1158 - acc: 0.9543 - val_loss: 0.5825 - val_acc: 0.8334\n",
      "Epoch 26/40\n",
      "243784/243784 [==============================] - 160s 655us/step - loss: 0.1159 - acc: 0.9539 - val_loss: 0.5797 - val_acc: 0.8334\n",
      "Epoch 27/40\n",
      "243784/243784 [==============================] - 160s 656us/step - loss: 0.1135 - acc: 0.9554 - val_loss: 0.5841 - val_acc: 0.8330\n",
      "Epoch 28/40\n",
      "243784/243784 [==============================] - 160s 658us/step - loss: 0.1127 - acc: 0.9556 - val_loss: 0.5866 - val_acc: 0.8330\n",
      "Epoch 29/40\n",
      "243784/243784 [==============================] - 160s 656us/step - loss: 0.1124 - acc: 0.9560 - val_loss: 0.5877 - val_acc: 0.8325\n",
      "Epoch 30/40\n",
      "243784/243784 [==============================] - 160s 656us/step - loss: 0.1107 - acc: 0.9567 - val_loss: 0.5892 - val_acc: 0.8322\n",
      "Epoch 31/40\n",
      "243784/243784 [==============================] - 160s 655us/step - loss: 0.1111 - acc: 0.9564 - val_loss: 0.5895 - val_acc: 0.8326\n",
      "Epoch 32/40\n",
      "243784/243784 [==============================] - 160s 655us/step - loss: 0.1098 - acc: 0.9570 - val_loss: 0.5928 - val_acc: 0.8325\n",
      "Epoch 33/40\n",
      "243784/243784 [==============================] - 160s 656us/step - loss: 0.1098 - acc: 0.9571 - val_loss: 0.5912 - val_acc: 0.8318\n",
      "Epoch 34/40\n",
      "243784/243784 [==============================] - 160s 656us/step - loss: 0.1098 - acc: 0.9573 - val_loss: 0.5913 - val_acc: 0.8328\n",
      "Epoch 35/40\n",
      "243784/243784 [==============================] - 159s 654us/step - loss: 0.1093 - acc: 0.9570 - val_loss: 0.5932 - val_acc: 0.8321\n",
      "Epoch 36/40\n",
      "243784/243784 [==============================] - 160s 655us/step - loss: 0.1092 - acc: 0.9576 - val_loss: 0.5938 - val_acc: 0.8324\n",
      "Epoch 37/40\n",
      "243784/243784 [==============================] - 160s 655us/step - loss: 0.1096 - acc: 0.9566 - val_loss: 0.5947 - val_acc: 0.8329\n",
      "Epoch 38/40\n",
      "243784/243784 [==============================] - 160s 655us/step - loss: 0.1086 - acc: 0.9571 - val_loss: 0.5956 - val_acc: 0.8324\n",
      "Epoch 39/40\n",
      "243784/243784 [==============================] - 160s 655us/step - loss: 0.1089 - acc: 0.9572 - val_loss: 0.5948 - val_acc: 0.8325\n",
      "Epoch 40/40\n",
      "243784/243784 [==============================] - 160s 655us/step - loss: 0.1079 - acc: 0.9577 - val_loss: 0.5950 - val_acc: 0.8325\n"
     ]
    }
   ],
   "source": [
    "learn_rate = LearningRateScheduler(model2_exp_decay(0.00275, 0.1875))\n",
    "result = m2.fit(x_train, y_train, batch_size= 128, epochs=40,\n",
    "  validation_split=0.1, callbacks=[learn_rate])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ha_pred'></a>\n",
    "#### Predict test data\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dl2 = m2.predict(x_test, batch_size=128)\n",
    "\n",
    "target_names = ['not duplicate', 'duplicate']\n",
    "\n",
    "convert_binary = lambda x: 1 if x[0] >= .5 else 0\n",
    "\n",
    "y_pred_dl2_classes = np.array([convert_binary(y) for y in y_pred_dl2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ha_eval'></a>\n",
    "#### Evaluate Model\n",
    "[back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for DL method 2  0.8332496345988082\n",
      "Recall score for DL method 2  0.794518596891023\n",
      "Precision score for DL method 2  0.7627204719026506\n"
     ]
    }
   ],
   "source": [
    "score_dl2 = accuracy_score(y_test.values, y_pred_dl2_classes)\n",
    "rscore_dl2 = recall_score(y_test.values, y_pred_dl2_classes)\n",
    "pscore_dl2 = precision_score(y_test.values, y_pred_dl2_classes)\n",
    "print('Accuracy score for DL method 2 ', score_dl2)\n",
    "print('Recall score for DL method 2 ', rscore_dl2)\n",
    "print('Precision score for DL method 2 ', pscore_dl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "not duplicate       0.88      0.86      0.87     84267\n",
      "    duplicate       0.76      0.79      0.78     49148\n",
      "\n",
      "    micro avg       0.83      0.83      0.83    133415\n",
      "    macro avg       0.82      0.83      0.82    133415\n",
      " weighted avg       0.84      0.83      0.83    133415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test.values, y_pred_dl2_classes, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_fpr, m2_tpr, _ = roc_curve(y_test.values, y_pred_dl2_classes)\n",
    "m2_roc_auc = auc(m2_fpr, m2_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 598.055x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc(m2_fpr, m2_tpr, m2_roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
