{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import persistence as ps\n",
    "from urllib3.response import HTTPResponse\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import partition_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_BUCKET = 'dq-data'\n",
    "HASH_BUCKET = 'dq-hashed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train_set\n",
    "data = 'train.csv'\n",
    "filestream = ps.get_file_stream(bucket=INPUT_BUCKET, filename=data)\n",
    "dtypes = {\n",
    "    'id': 'int64',\n",
    "    'qid1': 'int64',\n",
    "    'qid2': 'int64',\n",
    "    'question1': 'object',\n",
    "    'question2': 'object',\n",
    "    'is_duplicate': 'int64'\n",
    "}\n",
    "df = pd.read_csv(#urlpath=s3_in_url, \n",
    "                                     #storage_options=s3_options,\n",
    "                                     filestream,\n",
    "                                     header=0, \n",
    "                                     usecols=dtypes.keys(), \n",
    "                                     #names=dtypes.keys(),\n",
    "                                     skipinitialspace=True,\n",
    "                                     skip_blank_lines=True,\n",
    "                                     encoding='utf-8')\n",
    "df = df.set_index('id')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 404287 entries, 0 to 404289\n",
      "Data columns (total 5 columns):\n",
      "qid1            404287 non-null int64\n",
      "qid2            404287 non-null int64\n",
      "question1       404287 non-null object\n",
      "question2       404287 non-null object\n",
      "is_duplicate    404287 non-null int64\n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 18.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#shrink df to 150,000 records\n",
    "df = df.iloc[:150000]\n",
    "\n",
    "X = df.drop(columns=['is_duplicate'])\n",
    "\n",
    "y = df['is_duplicate']\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 100500 entries, 87935 to 121959\n",
      "Data columns (total 4 columns):\n",
      "qid1         100500 non-null int64\n",
      "qid2         100500 non-null int64\n",
      "question1    100500 non-null object\n",
      "question2    100500 non-null object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X,y,df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "def get_tokens(process='train'):\n",
    "    if process=='test':\n",
    "        X = X_test\n",
    "    else:\n",
    "        X = X_train\n",
    "    series = pd.Series(pd.concat([X['question1'], X['question2']]),dtype=str)\n",
    "    series.dropna()\n",
    "    for question in series:\n",
    "        yield preprocess_string(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec (fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minio.definitions.Object at 0x7f4f8d95ca20>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.get_file(bucket=INPUT_BUCKET, filename='cc.en.300.bin.gz', filepath='/tmp/cc.en.300.bin.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "with gzip.open('/tmp/cc.en.300.bin.gz', 'rb') as f_in:\n",
    "    with open('/tmp/cc.en.300.bin', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove('/tmp/cc.en.300.bin.gz')\n",
    "from gensim.models import FastText\n",
    "model = FastText.load_fasttext_format('/tmp/cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ft_vectors(model, process):\n",
    "    for tokens in get_tokens(process):\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                vector = model.wv[token]\n",
    "            except:\n",
    "                continue\n",
    "            vectors.append(vector)\n",
    "        yield np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201000,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ft = np.array([vectors for vectors in get_ft_vectors(model,'train')])\n",
    "X_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split back into two\n",
    "X1_ft = X_ft[:len(X_train)]\n",
    "X2_ft = X_ft[len(X_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = pd.concat([X_train, pd.Series(X1_ft, name='q1_ft',index=X_train.index), pd.Series(X2_ft, name='q2_ft',index=X_train.index)], axis=1)\n",
    "#X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ft_test = np.array([vectors for vectors in get_ft_vectors(model,'test')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99000,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ft_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split back into two\n",
    "X1_ft_test = X_ft_test[:len(X_test)]\n",
    "X2_ft_test = X_ft_test[len(X_test):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_ft_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test = pd.concat([X_test, pd.Series(X1_ft_test, name='q1_ft',index=X_test.index), pd.Series(X2_ft_test, name='q2_ft',index=X_test.index)], axis=1)\n",
    "#X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDS, LLE Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.manifold import MDS, LocallyLinearEmbedding\n",
    "#def reduce_dim(X, method, dimensions=3):\n",
    "#    n_jobs = -1\n",
    "#    n_neighbors = 5\n",
    "#    if X.shape[0] <= 5:\n",
    "#        n_neighbors = X.shape[0] - 1\n",
    "#    if method == 'LLE':\n",
    "#        embedding = LocallyLinearEmbedding(n_neighbors=n_neighbors, n_components=dimensions, random_state=42, n_jobs=n_jobs)\n",
    "#    elif method == 'MLLE':\n",
    "#        embedding = LocallyLinearEmbedding(n_components=dimensions, method='modified', random_state=42, n_jobs=n_jobs)\n",
    "#    elif method == 'Hessian':\n",
    "#        embedding = LocallyLinearEmbedding(n_components=dimensions, method='hessian', random_state=42, n_jobs=n_jobs)\n",
    "#    else: #method == 'MDS':\n",
    "#        embedding = MDS(n_components=dimensions, random_state=42, n_jobs=n_jobs)\n",
    "#    X_trfmd = embedding.fit_transform(X)\n",
    "#    return X_trfmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from megaman.geometry.geometry import Geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel, polynomial_kernel, sigmoid_kernel, laplacian_kernel, rbf_kernel\n",
    "from scipy.spatial.distance import cdist\n",
    "def compute_pairwise_kernel(pc1, pc2, method='linear'):\n",
    "    if method=='polynomial':\n",
    "        return polynomial_kernel(pc1, pc2, 2)\n",
    "    elif method=='rbf':\n",
    "        return rbf_kernel(pc1, pc2)\n",
    "    elif method=='sigmoid':\n",
    "        return sigmoid_kernel(pc1, pc2)\n",
    "    elif method=='laplacian':\n",
    "        return laplacian_kernel(pc1, pc2)\n",
    "    else:\n",
    "        return linear_kernel(pc1, pc2)\n",
    "    \n",
    "def compute_pairwise_dist(pc1, pc2, method='euclidean'):\n",
    "    if pc1.size == 0:\n",
    "        return []\n",
    "    if pc2.size == 0:\n",
    "        return []\n",
    "    return cdist(pc1, pc2, metric=method)\n",
    "\n",
    "#def compute_red_dim(pc1, pc2, method='MDS'):\n",
    "#    if pc1.size == 0:\n",
    "#        if pc2.size > 0:\n",
    "#            pc = pc2\n",
    "#            len_pc1 = 0\n",
    "#        else:\n",
    "#            return ()\n",
    "#    elif pc2.size == 0:\n",
    "#        pc = pc1\n",
    "#        len_pc1 = pc1.shape[0]\n",
    "#    else:\n",
    "#        pc = np.vstack((pc1,pc2))\n",
    "#        len_pc1 = pc1.shape[0]\n",
    "#    pc_embd = reduce_dim(pc, method=method, dimensions=3)\n",
    "#    pc1_embd = pc_embd[:len_pc1]\n",
    "#    pc2_embd = pc_embd[len_pc1:]\n",
    "#    return pc1_embd, pc2_embd\n",
    "        \n",
    "def assign_pwmetric(df, method='euclidean'):\n",
    "    #return compute_pairwise_kernel(pc1_embd, pc2_embd, method=method)\n",
    "    return df.apply(compute_pairwise_dist, method, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train['linear_kernel'] = X_train.apply(rowwise_pwkernel, axis=1)\n",
    "#X_train.head()\n",
    "#X_train.to_csv('/tmp/X_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dask.dataframe as dd\n",
    "#from dask import delayed, compute\n",
    "#from dask.distributed import Client\n",
    "#from utils import dask\n",
    "#client = dask.create_dask_client(num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dask.perform_dask_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_lengths(X):\n",
    "    q_meta = []\n",
    "    for q in X:\n",
    "        q_meta.append(len(q))\n",
    "    return q_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100500,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_arrays(X):\n",
    "    for y in (x for x in X if x.size>0):\n",
    "        yield np.vsplit(y,len(y))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_meta = get_q_lengths(X1_ft) + get_q_lengths(X2_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((np.concatenate([x for x in split_arrays(X1_ft)]), np.concatenate([x for x in split_arrays(X2_ft)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200781, 300)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X1_ft, X2_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "# Get a sorted list of the objects and their sizes\n",
    "y = sorted([(x, sys.getsizeof(globals().get(x))) \n",
    "        for x in dir() if not x.startswith('_') \n",
    "        and x not in sys.modules and x not in ipython_vars], \n",
    "       key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('X', 240937312),\n",
       " ('X_train', 26087192),\n",
       " ('X_test', 12830824),\n",
       " ('q_meta', 1608064),\n",
       " ('y_train', 1608024),\n",
       " ('y_test', 792024),\n",
       " ('FastText', 2000),\n",
       " ('HTTPResponse', 1464),\n",
       " ('Geometry', 1056),\n",
       " ('Dict', 888),\n",
       " ('List', 888),\n",
       " ('Tuple', 888),\n",
       " ('dtypes', 368),\n",
       " ('f_out', 176),\n",
       " ('assign_pwmetric', 136),\n",
       " ('cdist', 136),\n",
       " ('compute_pairwise_dist', 136),\n",
       " ('compute_pairwise_kernel', 136),\n",
       " ('get_ft_vectors', 136),\n",
       " ('get_q_lengths', 136),\n",
       " ('get_tokens', 136),\n",
       " ('laplacian_kernel', 136),\n",
       " ('linear_kernel', 136),\n",
       " ('partition_all', 136),\n",
       " ('polynomial_kernel', 136),\n",
       " ('preprocess_string', 136),\n",
       " ('rbf_kernel', 136),\n",
       " ('sigmoid_kernel', 136),\n",
       " ('split_arrays', 136),\n",
       " ('train_test_split', 136),\n",
       " ('X1_ft_test', 96),\n",
       " ('X2_ft_test', 96),\n",
       " ('np', 80),\n",
       " ('pd', 80),\n",
       " ('ps', 80),\n",
       " ('HASH_BUCKET', 58),\n",
       " ('data', 58),\n",
       " ('INPUT_BUCKET', 56),\n",
       " ('f_in', 56),\n",
       " ('filestream', 56)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import mmwrite, mmread\n",
    "mmwrite( 'wor2vec_300_train.mtx', X )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.copy_file(dest_bucket=INPUT_BUCKET, file='wor2vec_300_train.mtx', source='wor2vec_300_train.mtx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from megaman.embedding import SpectralEmbedding\n",
    "radius = 20\n",
    "adjacency_method = 'cyflann'\n",
    "cyflann_kwds = {'index_type':'kmeans', 'branching':64, 'iterations':20, 'cb_index':0.4}\n",
    "adjacency_kwds = {'radius':radius, 'cyflann_kwds':cyflann_kwds}\n",
    "affinity_method = 'gaussian'\n",
    "affinity_kwds = {'radius':radius}\n",
    "laplacian_method = 'geometric'\n",
    "laplacian_kwds = {'scaling_epps':radius}\n",
    "\n",
    "geom = Geometry(adjacency_method=adjacency_method, adjacency_kwds=adjacency_kwds,\n",
    "                affinity_method=affinity_method, affinity_kwds=affinity_kwds,\n",
    "                laplacian_method=laplacian_method, laplacian_kwds=laplacian_kwds)\n",
    "#Geom = Geometry(X, neighborhood_radius = radius, affinity_radius = radius,\n",
    "#                distance_method = 'cython', input_type = 'distance',\n",
    "#                laplacian_type = 'geometric')\n",
    "geom.set_data_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:462: DeprecationWarning: Passing 'None' to parameter 'accept_sparse' in methods check_array and check_X_y is deprecated in version 0.19 and will be removed in 0.21. Use 'accept_sparse=False'  instead.\n",
      "  \" instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "spec = SpectralEmbedding(n_components=2, eigen_solver='amg',geom=geom)\n",
    "X_spec = spec.fit_transform(X=X.astype(np.float))\n",
    "#adjacency_matrix = geom.compute_adjacency_matrix()\n",
    "t1 = time.time() - t0\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_rd =  reduce_dim(X, 'LLE', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild X1_rd and X2_rd\n",
    "X1_list = []\n",
    "X2_list = []\n",
    "for len_q1, len_q2 in q_meta:\n",
    "    q1 = X_rd[:len_q1]\n",
    "    q2 = X_rd[len_q1:(len_q1+len_q2)]\n",
    "    X1_list.append(q1)\n",
    "    X2_list.append(q2)\n",
    "    X_rd = X_rd[(len_q1+len_q2):]\n",
    "X1_rd = np.array(X1_list)\n",
    "X2_rd = np.array(X2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X1_list, X2_list, q_meta, X_rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard = []\n",
    "chebyshev = []\n",
    "braycurtis = []\n",
    "for q_tuple in q_rd:\n",
    "    if q_tuple:\n",
    "        q1_rd, q2_rd = q_tuple\n",
    "        jaccard.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'jaccard'))\n",
    "        chebyshev.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'chebyshev'))\n",
    "        braycurtis.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'braycurtis'))\n",
    "    else:\n",
    "        jaccard.append(delayed([]))\n",
    "        chebyshev.append(delayed([]))\n",
    "        braycurtis.append(delayed([])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard = compute(*jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chebyshev = compute(*chebyshev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "braycurtis = compute(*braycurtis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(braycurtis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_ddf = X_train_ddf.map_partitions(reduce_dim,'MDS', meta={'q1_ft':np.float32, 'q2_ft':np.float32})\n",
    "#X_train_ddf['jaccard'] = X_train_ddf.map_partitions(assign_pwmetric,'jaccard', meta=('jaccard',np.float32))\n",
    "#X_train_ddf['chebyshev'] = X_train_ddf.map_partitions(assign_pwmetric,'chebyshev', meta=('chebyshev',np.float32))\n",
    "#X_train_ddf['braycurtis'] = X_train_ddf.map_partitions(assign_pwmetric,'braycurtis', meta=('braycurtis',np.float32))\n",
    "#X_train = X_train_ddf.compute()\n",
    "#X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_ft_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask delayed\n",
    "q_rd_test = []\n",
    "for q1, q2 in zip(X1_ft_test, X2_ft_test):\n",
    "    q = delayed(compute_red_dim)(q1, q2, 'MDS')\n",
    "    q_rd_test.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_rd_test = compute(*q_rd_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(q_rd_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X1_ft_test, X2_ft_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_test = []\n",
    "chebyshev_test = []\n",
    "braycurtis_test = []\n",
    "for q_tuple in q_rd_test:\n",
    "    if q_tuple:\n",
    "        q1_rd, q2_rd = q_tuple\n",
    "        jaccard_test.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'jaccard'))\n",
    "        chebyshev_test.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'chebyshev'))\n",
    "        braycurtis_test.append(delayed(compute_pairwise_dist)(q1_rd, q2_rd, 'braycurtis'))\n",
    "    else:\n",
    "        jaccard_test.append(delayed([]))\n",
    "        chebyshev_test.append(delayed([]))\n",
    "        braycurtis_test.append(delayed([]))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_test = compute(*jaccard_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chebyshev_test = compute(*chebyshev_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "braycurtis_test = compute(*braycurtis_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Kernel estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install grakel-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grakel import GraphKernel\n",
    "from grakel import Graph\n",
    "def estimate_grakel(pc1, pc2):\n",
    "    sp_kernel = GraphKernel(kernel={\"name\": \"shortest_path\"})\n",
    "    sp_kernel.fit_transform([Graph(pc1)])\n",
    "    return sp_kernel.transform([Graph(pc2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowwise_grakel(row):\n",
    "    pc1 = row['q1_ft']\n",
    "    pc2 = row['q2_ft']\n",
    "    pc1_embd, pc2_embd = embed_pointclouds(pc1, pc2, method='LLE', dimensions=3)\n",
    "    return estimate_grakel(pc1_embd, pc2_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['grakel'] = X_train.apply(rowwise_grakel, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc1 = X_train.iloc[0]['q1_ft']\n",
    "pc2 = X_train.iloc[0]['q2_ft']\n",
    "pc1_embd, pc2_embd = embed_pointclouds(pc1, pc2, method='LLE', dimensions=3)\n",
    "print(len(pc1))\n",
    "print(pc1_embd.shape)\n",
    "print(len(pc2))\n",
    "print(pc2_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "pass_through = lambda x:x\n",
    "tfidf = TfidfVectorizer(analyzer=pass_through)\n",
    "X_trfmd = tfidf.fit_transform(get_tokens('train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trfmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension reduction using SVD\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import time\n",
    "start = time.time()\n",
    "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42)\n",
    "X_svd = svd.fit_transform(X_trfmd)\n",
    "end =  time.time()\n",
    "print('created SVD transform in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_svd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split back into two\n",
    "X1 = X_svd[:len(X_train), :]\n",
    "X2 = X_svd[len(X_train):, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_trfmd = tfidf.transform(get_tokens('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_trfmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension reduction using SVD\n",
    "start = time.time()\n",
    "X_test_svd = svd.transform(X_test_trfmd)\n",
    "end =  time.time()\n",
    "print('created SVD transform in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split back into two\n",
    "X1_test = X_test_svd[:len(X_test), :]\n",
    "X2_test = X_test_svd[len(X_test):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build complete feature dataframe\n",
    "X_test_temp = pd.concat([pd.DataFrame(X1_test, columns=['q1_'+str(i) for i in range(X1_test.shape[1])], index=X_test.index), \n",
    "                    pd.DataFrame(X2_test, columns=['q2_'+str(i) for i in range(X2_test.shape[1])], index=X_test.index)], axis=1)\n",
    "X_test_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy-wuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference in text size\n",
    "compute_size_diff = lambda row: abs(len(str(row['question1'])) - len(str(row['question2'])))\n",
    "X_train['size_diff'] = X_train.apply(compute_size_diff, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio\n",
    "compute_ratio = lambda row: fuzz.ratio(str(row['question1']), str(row['question2']))\n",
    "X_train['ratio'] = X_train.apply(compute_ratio, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial ratio\n",
    "compute_partial_ratio = lambda row: fuzz.partial_ratio(str(row['question1']), str(row['question2']))\n",
    "X_train['partial_ratio'] = X_train.apply(compute_partial_ratio, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_sort_ratio\n",
    "compute_token_sort_ratio = lambda row: fuzz.token_sort_ratio(str(row['question1']), str(row['question2']))\n",
    "X_train['token_sort_ratio'] = X_train.apply(compute_token_sort_ratio, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_set_ratio\n",
    "compute_token_set_ratio = lambda row: fuzz.token_set_ratio(str(row['question1']), str(row['question2']))\n",
    "X_train['token_set_ratio'] = X_train.apply(compute_token_set_ratio, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build complete feature dataframe\n",
    "X_train_temp = pd.concat([pd.DataFrame(X1, columns=['q1_'+str(i) for i in range(X1.shape[1])], index=X_train.index), \n",
    "                     pd.DataFrame(X2, columns=['q2_'+str(i) for i in range(X2.shape[1])], index=X_train.index)], axis=1)\n",
    "X_train_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train_temp, X_train], axis=1)\n",
    "del X_train_temp\n",
    "X_train = X_train.drop(columns=['qid1', 'qid2','question1','question2'])\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference in text size\n",
    "X_test['size_diff'] = X_test.apply(compute_size_diff, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio\n",
    "X_test['ratio'] = X_test.apply(compute_ratio, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial ratio\n",
    "X_test['partial_ratio'] = X_test.apply(compute_partial_ratio, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_sort_ratio\n",
    "X_test['token_sort_ratio'] = X_test.apply(compute_token_sort_ratio, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_set_ratio\n",
    "X_test['token_set_ratio'] = X_test.apply(compute_token_set_ratio, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.concat([X_test_temp, X_test], axis=1)\n",
    "del X_test_temp\n",
    "X_test = X_test.drop(columns=['question1','question2', 'qid1', 'qid2'])\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "logr_model = LogisticRegression(random_state=42)\n",
    "param_grid = {'C': np.logspace(-2, 7, 10),\n",
    "             #'penalty': ['l1','l2'],\n",
    "             'tol': np.logspace(-5, -1, 5),\n",
    "             #'solver': ['lbfgs']\n",
    "             #'max_iter': np.linspace(10, 1000, 10)\n",
    "             }\n",
    "logr_cv = RandomizedSearchCV(logr_model, param_distributions=param_grid, cv=5, n_jobs=-1)\n",
    "logr_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_model = LogisticRegression(#solver=logr_cv.best_params_['solver'], \n",
    "                                random_state=42, \n",
    "                                C=logr_cv.best_params_['C'], \n",
    "                                tol=logr_cv.best_params_['tol'], \n",
    "                                #max_iter=logr_cv.best_params_['max_iter'], \n",
    "                                n_jobs=-1)\n",
    "logr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_pred = logr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "logr_acc_score = accuracy_score(y_test, logr_pred)\n",
    "logr_prec_score = precision_score(y_test, logr_pred)\n",
    "logr_rec_score = recall_score(y_test, logr_pred)\n",
    "print('Logistic Regression')\n",
    "print('accuracy score : {}'.format(logr_acc_score))\n",
    "print('precision score : {}'.format(logr_prec_score))\n",
    "print('recall score : {}'.format(logr_rec_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
