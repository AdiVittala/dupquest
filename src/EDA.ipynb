{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import persistence as ps\n",
    "from urllib3.response import HTTPResponse\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import partition_all\n",
    "#from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_BUCKET: str = 'dq-data'\n",
    "HASH_BUCKET: str = 'dq-hashed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train_set\n",
    "data: str = 'train.csv'\n",
    "filestream: HTTPResponse = ps.get_file_stream(bucket=INPUT_BUCKET, filename=data)\n",
    "#s3_in_prefix: str = 's3://' + INPUT_BUCKET + '/'\n",
    "#s3_in_url: str = s3_in_prefix + train_data\n",
    "#s3_options: Dict = ps.fetch_s3_options()\n",
    "dtypes: Dict[str, str] = {\n",
    "    'id': 'int64',\n",
    "    'qid1': 'int64',\n",
    "    'qid2': 'int64',\n",
    "    'question1': 'object',\n",
    "    'question2': 'object',\n",
    "    'is_duplicate': 'int64'\n",
    "}\n",
    "df: pd.DataFrame = pd.read_csv(#urlpath=s3_in_url, \n",
    "                                     #storage_options=s3_options,\n",
    "                                     filestream,\n",
    "                                     header=0, \n",
    "                                     usecols=dtypes.keys(), \n",
    "                                     names=dtypes.keys(),\n",
    "                                     skipinitialspace=True,\n",
    "                                     skip_blank_lines=True,\n",
    "                                     encoding='utf-8')\n",
    "df = df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    qid1  qid2                                          question1  \\\n",
       "id                                                                  \n",
       "0      1     2  What is the step by step guide to invest in sh...   \n",
       "1      3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2      5     6  How can I increase the speed of my internet co...   \n",
       "3      7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4      9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                            question2  is_duplicate  \n",
       "id                                                                   \n",
       "0   What is the step by step guide to invest in sh...             0  \n",
       "1   What would happen if the Indian government sto...             0  \n",
       "2   How can Internet speed be increased by hacking...             0  \n",
       "3   Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4             Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 404290 entries, 0 to 404289\n",
      "Data columns (total 5 columns):\n",
      "qid1            404290 non-null int64\n",
      "qid2            404290 non-null int64\n",
      "question1       404289 non-null object\n",
      "question2       404288 non-null object\n",
      "is_duplicate    404290 non-null int64\n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 18.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop(columns=['is_duplicate'])\n",
    "y = df['is_duplicate']\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 270874 entries, 316451 to 121958\n",
      "Data columns (total 4 columns):\n",
      "qid1         270874 non-null int64\n",
      "qid2         270874 non-null int64\n",
      "question1    270873 non-null object\n",
      "question2    270873 non-null object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 10.3+ MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from spacy.lemmatizer import Lemmatizer\n",
    "#from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "#lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "#nlp.add_pipe(lemmatizer)\n",
    "#nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tagger = nlp.get_pipe('tagger')\n",
    "#tagger.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser = nlp.get_pipe('parser')\n",
    "#parser.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ner = nlp.get_pipe('ner')\n",
    "#ner.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize, pos-tag, parse dependencies, recognize entities (pipeline)\n",
    "#pipeline = ['tagger', 'parser', 'ner']\n",
    "#for name in pipeline:\n",
    "#    component = nlp.create_pipe(name)   # 3. create the pipeline components\n",
    "#    nlp.add_pipe(component)             # 4. add the component to the pipeline\n",
    "\n",
    "#preprocess_q1 = lambda row: nlp(row['question1'])\n",
    "#x_df1['pr_question1'] = x_df1.apply(preprocess_q1, axis=1)\n",
    "#preprocess_q2 = lambda row: nlp(row['question2'])\n",
    "#x_df1['pr_question2'] = x_df1.apply(preprocess_q2, axis=1)\n",
    "#x_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "import nltk\n",
    "def tokenize(text):\n",
    "        tokens = [word for word in nlp(text) if len(word) > 1] #if len(word) > 1 because I only want to retain words that are at least two characters before stemming, although I can't think of any such words that are not also stopwords\n",
    "        #stems = [stemmer.stem(item) for item in tokens]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "ps.create_bucket(bucket=HASH_BUCKET)\n",
    "import os\n",
    "import shutil\n",
    "tmp_train_path = '/tmp/train'\n",
    "try:\n",
    "    shutil.rmtree(tmp_train_path)\n",
    "except:\n",
    "    pass\n",
    "try:    \n",
    "    os.mkdir(tmp_train_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "def tokenize(pipeline, series, batch_id, output_dir):\n",
    "    #tfidf = TfidfVectorizer(tokenizer=tokenizer, binary=True, stop_words='english', use_idf=True, max_features=max_features)\n",
    "    #series1 = df[col1]\n",
    "    #series2 = df[col2]\n",
    "    #series = pd.concat([series1, series2])\n",
    "    #start = time.time()\n",
    "    print('processing batch {}'.format(batch_id))\n",
    "    #trnsfmd = pipeline(series)\n",
    "    #end =  time.time()\n",
    "    # save transformed batch\n",
    "    out_file = ('%d' % batch_id) \n",
    "    out_path = output_dir+'/'+out_file\n",
    "    with open(out_path, 'wb') as handle:\n",
    "        #for doc in pipeline.pipe(series):\n",
    "        docs = [doc for doc in pipeline.pipe(series)]        \n",
    "        pickle.dump(docs, handle)\n",
    "            #handle.write(' '.join(w for w in doc if not w.is_space))\n",
    "            #handle.write('\\n')\n",
    "            #file = out_file+'_'+str(i)\n",
    "            #doc.to_disk(out_path)\n",
    "    ps.copy_file(dest_bucket=HASH_BUCKET, file='train/'+out_file, source=out_path)\n",
    "    os.remove(out_path)\n",
    "    #print('created TF-IDF vectors in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all files in bucket dq-hashed at path train/ are []\n",
      "processing batch 0\n",
      "processing batch 3\n",
      "pushed file train/0 from /tmp/train/0 to minio bucket dq-hashed\n",
      "processing batch 1\n",
      "processing batch 6\n",
      "processing batch 9\n",
      "pushed file train/3 from /tmp/train/3 to minio bucket dq-hashed\n",
      "pushed file train/1 from /tmp/train/1 to minio bucket dq-hashed\n",
      "processing batch 2\n",
      "processing batch 4\n",
      "processing batch 12\n",
      "processing batch 15\n",
      "pushed file train/6 from /tmp/train/6 to minio bucket dq-hashed\n",
      "processing batch 7\n",
      "pushed file train/9 from /tmp/train/9 to minio bucket dq-hashed\n",
      "processing batch 10\n",
      "pushed file train/2 from /tmp/train/2 to minio bucket dq-hashed\n",
      "pushed file train/4 from /tmp/train/4 to minio bucket dq-hashed\n",
      "processing batch 5\n",
      "pushed file train/12 from /tmp/train/12 to minio bucket dq-hashed\n",
      "processing batch 13\n",
      "processing batch 18\n",
      "pushed file train/15 from /tmp/train/15 to minio bucket dq-hashed\n",
      "processing batch 16\n"
     ]
    }
   ],
   "source": [
    "from toolz import partition_all\n",
    "#from joblib import Parallel, delayed\n",
    "#import multiprocessing_on_dill as multiprocessing\n",
    "from multiprocessing_on_dill import Process, cpu_count, Pool\n",
    "# empty HASH_BUCKET\n",
    "ps.remove_all_files(bucket=HASH_BUCKET, path='train/')\n",
    "series = pd.Series(pd.concat([X_train['question1'], X_train['question2']]),dtype=str)\n",
    "series.dropna()\n",
    "partitions = partition_all(50000, series.tolist())\n",
    "#hashvect = HashingVectorizer(tokenizer=nlp, binary=True, stop_words='english')\n",
    "#Parallel(n_jobs=8)(delayed(tokenize)(nlp, batch, i, tmp_train_path)\n",
    "#         for i, batch in enumerate(partitions))\n",
    "#executor(tasks)\n",
    "\n",
    "# parallel joblib and spacy dont work together, trying serial\n",
    "#for i, batch in enumerate(partitions):\n",
    "#    tokenize(nlp, batch, i, tmp_train_path)\n",
    "\n",
    "#trying multiprocessing\n",
    "#processes = []\n",
    "pool = Pool(processes=6)\n",
    "args = []\n",
    "for i, batch in enumerate(partitions):\n",
    "    #p = Process(target=tokenize, name=i, kwargs={'pipeline':nlp, \n",
    "    #                                             'series':batch, \n",
    "    #                                             'batch_id':i, \n",
    "    #                                             'output_dir':tmp_train_path})\n",
    "    args.append((nlp,\n",
    "                   batch,\n",
    "                   i, \n",
    "                   tmp_train_path))\n",
    "    #p.start()\n",
    "    #print('started process {}'.format(p.name))\n",
    "    #processes.append(p)\n",
    "pool.starmap(tokenize, args)\n",
    "#for p in processes:\n",
    "#    p.join()\n",
    "#    print('finished process {}'.format(p.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import vstack\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from spacy.tokens import Doc\n",
    "data = []\n",
    "files = ps.get_all_filenames(bucket=HASH_BUCKET, path='train/')\n",
    "#files = os.listdir(tmp_train_path)\n",
    "for file in files:\n",
    "    ps.get_file(bucket=HASH_BUCKET, filename='train/'+file, filepath=tmp_train_path+file)\n",
    "    #with open(tmp_train_path+file, 'rb') as handle:\n",
    "        #data.append(pickle.load(handle))\n",
    "    data.append(Doc(nlp.vocab).from_disk(tmp_train_path+file))\n",
    "    os.remove(tmp_train_path+file)\n",
    "\n",
    "#tfidf = TfidfTransformer()\n",
    "pass_through = lambda x:x\n",
    "tfidf = TfidfVectorizer(analyzer=pass_through, binary=True)\n",
    "X_trfmd = tfidf.fit_transform(vstack(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vstack(data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trfmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension reduction using SVD\n",
    "start = time.time()\n",
    "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42)\n",
    "X_svd = svd.fit_transform(X_trfmd)\n",
    "end =  time.time()\n",
    "print('created SVD transform in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_svd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split back into two\n",
    "X1 = X_svd[:len(X_train), :]\n",
    "X2 = X_svd[len(X_train):, :]\n",
    "## find pair-wise cosine similarity\n",
    "#start = time.time()\n",
    "#X_sim = cosine_similarity(X1, X2)\n",
    "#end =  time.time()\n",
    "#print('computed cosine similarity in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svd_feature_length = X_sim.shape[1]\n",
    "#start = time.time()\n",
    "#temp_df = pd.DataFrame(X_sim)\n",
    "#x_df1 = pd.concat([x_df1,temp_df], axis=1)\n",
    "#end =  time.time()\n",
    "#print('rebuilt dataframe with new tf_svd feature columns in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_df1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_df1 = tfidf_svd_vectorize(x_df1, 'question1', 'question2', 10000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference in text size\n",
    "compute_size_diff = lambda row: abs(len(str(row['question1'])) - len(str(row['question2'])))\n",
    "X_train['size_diff'] = X_train.apply(compute_size_diff, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vector norm diff (distance)\n",
    "#compute_spacy_distance = lambda row: abs(row['question1'].vector_norm - row['question2'].vector_norm)\n",
    "#x_df1['spacy_distance'] = x_df1.apply(compute_spacy_distance, axis=1)\n",
    "#x_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return mean distance between tokens and document centroid\n",
    "def compute_mean_distance(doc):\n",
    "    mean_distance = 0.0\n",
    "    centroid = doc.vector\n",
    "    count = 0\n",
    "    for token in doc:\n",
    "        if not token.is_stop:\n",
    "            mean_distance += np.inner(token.vector,centroid)\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        count = 1\n",
    "    return mean_distance / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mean distance from centroid for question1\n",
    "#compute_q1_mean_dist = lambda row: compute_mean_distance(row['question1'])\n",
    "#x_df1['q1_mean_dist'] = x_df1.apply(compute_q1_mean_dist, axis=1)\n",
    "#x_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mean distance from centroid for question1\n",
    "#compute_q2_mean_dist = lambda row: compute_mean_distance(row['question2'])\n",
    "#x_df1['q2_mean_dist'] = x_df1.apply(compute_q2_mean_dist, axis=1)\n",
    "#x_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## difference in mean distance\n",
    "#compute_mean_dist_diff = lambda row: abs(row['q1_mean_dist'] - row['q2_mean_dist'])\n",
    "#x_df1['mean_dist_diff'] = x_df1.apply(compute_mean_dist_diff, axis=1)\n",
    "#x_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## centroid similarity\n",
    "#compute_centroid_similarity = lambda row: np.inner(row['question1'].vector, row['question2'].vector)\n",
    "#x_df1['centroid_similarity'] = x_df1.apply(compute_centroid_similarity, axis=1)\n",
    "#x_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio\n",
    "compute_ratio = lambda row: fuzz.ratio(str(row['question1']), str(row['question2']))\n",
    "X_train['ratio'] = X_train.apply(compute_ratio, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial ratio\n",
    "compute_partial_ratio = lambda row: fuzz.partial_ratio(str(row['question1']), str(row['question2']))\n",
    "X_train['partial_ratio'] = X_train.apply(compute_partial_ratio, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_sort_ratio\n",
    "compute_token_sort_ratio = lambda row: fuzz.token_sort_ratio(str(row['question1']), str(row['question2']))\n",
    "X_train['token_sort_ratio'] = X_train.apply(compute_token_sort_ratio, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_set_ratio\n",
    "compute_token_set_ratio = lambda row: fuzz.token_set_ratio(str(row['question1']), str(row['question2']))\n",
    "X_train['token_set_ratio'] = X_train.apply(compute_token_set_ratio, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build complete feature dataframe\n",
    "X_train_temp = pd.concat([pd.DataFrame(X1, columns=['q1_'+str(i) for i in range(X1.shape[1])], index=X_train.index), \n",
    "                     pd.DataFrame(X2, columns=['q2_'+str(i) for i in range(X2.shape[1])], index=X_train.index)], axis=1)\n",
    "X_train_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train_temp, X_train], axis=1)\n",
    "del X_train_temp\n",
    "X_train = X_train.drop(columns=['qid1', 'qid2','question1','question2'])\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load test_set\n",
    "#test_data: str = 'test.csv'\n",
    "#filestream: HTTPResponse = ps.get_file_stream(bucket=INPUT_BUCKET, filename=test_data)\n",
    "#dtypes: Dict[str, str] = {\n",
    "#    'id': 'int64',\n",
    "#    'question1': 'object',\n",
    "#    'question2': 'object'\n",
    "#}\n",
    "#test_df: pd.DataFrame = pd.read_csv(#urlpath=s3_in_url, \n",
    "#                                     #storage_options=s3_options,\n",
    "#                                     filestream,\n",
    "#                                     header=0, \n",
    "#                                     usecols=dtypes.keys(), \n",
    "#                                     names=dtypes.keys(),\n",
    "#                                     skipinitialspace=True,\n",
    "#                                     skip_blank_lines=True,\n",
    "#                                     encoding='utf-8')\n",
    "#test_df = test_df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ps.create_bucket(bucket=HASH_BUCKET)\n",
    "tmp_test_path = '/tmp/test/'\n",
    "if os.path.isdir(tmp_test_path):\n",
    "    shutil.rmtree(tmp_test_path)\n",
    "os.mkdir(tmp_test_path)\n",
    "def transform(transformer, series, batch_id, output_dir, max_features=10000):\n",
    "    #tfidf = TfidfVectorizer(tokenizer=tokenizer, binary=True, stop_words='english', use_idf=True, max_features=max_features)\n",
    "    #series1 = df[col1]\n",
    "    #series2 = df[col2]\n",
    "    #series = pd.concat([series1, series2])\n",
    "    #start = time.time()\n",
    "    print('processing batch {}'.format(batch_id))\n",
    "    X = transformer.transform(series)\n",
    "    #end =  time.time()\n",
    "    # save transformed batch\n",
    "    out_file = ('%d' % batch_id)\n",
    "    out_path = output_dir+'/'+out_file \n",
    "    with open(out_path, 'wb') as handle:\n",
    "        pickle.dump(X, handle)\n",
    "    #ps.copy_file(dest_bucket=HASH_BUCKET, file='test/'+out_file, source=out_path)\n",
    "    #print('created TF-IDF vectors in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty HASH_BUCKET\n",
    "#ps.remove_all_files(bucket=HASH_BUCKET, path='test/')\n",
    "series = pd.Series(pd.concat([X_test['question1'], X_test['question2']]),dtype=str)\n",
    "partitions = partition_all(10000, series.tolist())\n",
    "Parallel(n_jobs=8)(delayed(transform)(hashvect, batch, i, tmp_test_path, 10000)\n",
    "         for i, batch in enumerate(partitions))\n",
    "#executor(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "files = ps.get_all_filenames(bucket=HASH_BUCKET, path='test/')\n",
    "files = os.listdir(tmp_test_path)\n",
    "for file in files:\n",
    "    #ps.get_file(bucket=HASH_BUCKET, filename='test/'+file, filepath=tmp_test_path+file)\n",
    "    with open(tmp_test_path+file, 'rb') as handle:\n",
    "        data.append(pickle.load(handle))\n",
    "X_test_trfmd = tfidf.transform(vstack(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_trfmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension reduction using SVD\n",
    "start = time.time()\n",
    "X_test_svd = svd.transform(X_test_trfmd)\n",
    "end =  time.time()\n",
    "print('created SVD transform in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split back into two\n",
    "X1_test = X_test_svd[:len(X_test), :]\n",
    "X2_test = X_test_svd[len(X_test):, :]\n",
    "## find pair-wise cosine similarity\n",
    "#start = time.time()\n",
    "#X_sim = cosine_similarity(X1, X2)\n",
    "#end =  time.time()\n",
    "#print('computed cosine similarity in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference in text size\n",
    "X_test['size_diff'] = X_test.apply(compute_size_diff, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio\n",
    "X_test['ratio'] = X_test.apply(compute_ratio, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial ratio\n",
    "X_test['partial_ratio'] = X_test.apply(compute_partial_ratio, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_sort_ratio\n",
    "X_test['token_sort_ratio'] = X_test.apply(compute_token_sort_ratio, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_set_ratio\n",
    "X_test['token_set_ratio'] = X_test.apply(compute_token_set_ratio, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build complete feature dataframe\n",
    "X_test_temp = pd.concat([pd.DataFrame(X1_test, columns=['q1_'+str(i) for i in range(X1_test.shape[1])], index=X_test.index), \n",
    "                    pd.DataFrame(X2_test, columns=['q2_'+str(i) for i in range(X2_test.shape[1])], index=X_test.index)], axis=1)\n",
    "X_test_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.concat([X_test_temp, X_test], axis=1)\n",
    "del X_test_temp\n",
    "X_test = X_test.drop(columns=['question1','question2', 'qid1', 'qid2'])\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "logr_model = LogisticRegression(random_state=42)\n",
    "param_grid = {'C': np.logspace(-2, 7, 10),\n",
    "             #'penalty': ['l1','l2'],\n",
    "             'tol': np.logspace(-5, -1, 5),\n",
    "             'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "             #'max_iter': np.linspace(10, 1000, 10)\n",
    "             }\n",
    "logr_cv = RandomizedSearchCV(logr_model, param_distributions=param_grid, cv=5, n_jobs=-1)\n",
    "#y_train = X['is_duplicate']\n",
    "#X_train = X.drop(columns=['is_duplicate'])\n",
    "logr_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_model = LogisticRegression(solver=logr_cv.best_params_['solver'], \n",
    "                                random_state=42, \n",
    "                                C=logr_cv.best_params_['C'], \n",
    "                                tol=logr_cv.best_params_['tol'], \n",
    "                                #max_iter=logr_cv.best_params_['max_iter'], \n",
    "                                n_jobs=-1)\n",
    "logr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_pred = logr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "logr_acc_score = accuracy_score(y_test, logr_pred)\n",
    "logr_prec_score = precision_score(y_test, logr_pred)\n",
    "logr_rec_score = recall_score(y_test, logr_pred)\n",
    "print('Logistic Regression')\n",
    "print('accuracy score : {}'.format(logr_acc_score))\n",
    "print('precision score : {}'.format(logr_prec_score))\n",
    "print('recall score : {}'.format(logr_rec_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
