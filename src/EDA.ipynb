{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import persistence as ps\n",
    "from urllib3.response import HTTPResponse\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import partition_all\n",
    "#from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_BUCKET: str = 'dq-data'\n",
    "HASH_BUCKET: str = 'dq-hashed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train_set\n",
    "data: str = 'train.csv'\n",
    "filestream: HTTPResponse = ps.get_file_stream(bucket=INPUT_BUCKET, filename=data)\n",
    "#s3_in_prefix: str = 's3://' + INPUT_BUCKET + '/'\n",
    "#s3_in_url: str = s3_in_prefix + train_data\n",
    "#s3_options: Dict = ps.fetch_s3_options()\n",
    "dtypes: Dict[str, str] = {\n",
    "    'id': 'int64',\n",
    "    'qid1': 'int64',\n",
    "    'qid2': 'int64',\n",
    "    'question1': 'object',\n",
    "    'question2': 'object',\n",
    "    'is_duplicate': 'int64'\n",
    "}\n",
    "df: pd.DataFrame = pd.read_csv(#urlpath=s3_in_url, \n",
    "                                     #storage_options=s3_options,\n",
    "                                     filestream,\n",
    "                                     header=0, \n",
    "                                     usecols=dtypes.keys(), \n",
    "                                     names=dtypes.keys(),\n",
    "                                     skipinitialspace=True,\n",
    "                                     skip_blank_lines=True,\n",
    "                                     encoding='utf-8')\n",
    "df = df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop(columns=['is_duplicate'])\n",
    "y = df['is_duplicate']\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tagger = nlp.get_pipe('tagger')\n",
    "#tagger.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser = nlp.get_pipe('parser')\n",
    "#parser.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ner = nlp.get_pipe('ner')\n",
    "#ner.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize, pos-tag, parse dependencies, recognize entities (pipeline)\n",
    "#pipeline = ['tagger', 'parser', 'ner']\n",
    "#for name in pipeline:\n",
    "#    component = nlp.create_pipe(name)   # 3. create the pipeline components\n",
    "#    nlp.add_pipe(component)             # 4. add the component to the pipeline\n",
    "\n",
    "#preprocess_q1 = lambda row: nlp(row['question1'])\n",
    "#x_df1['pr_question1'] = x_df1.apply(preprocess_q1, axis=1)\n",
    "#preprocess_q2 = lambda row: nlp(row['question2'])\n",
    "#x_df1['pr_question2'] = x_df1.apply(preprocess_q2, axis=1)\n",
    "#x_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "import nltk\n",
    "def tokenize(text):\n",
    "        tokens = [word for word in nlp(text) if len(word) > 1] #if len(word) > 1 because I only want to retain words that are at least two characters before stemming, although I can't think of any such words that are not also stopwords\n",
    "        #stems = [stemmer.stem(item) for item in tokens]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#ps.create_bucket(bucket=HASH_BUCKET)\n",
    "import os\n",
    "tmp_train_path = '/tmp/train'\n",
    "try:\n",
    "    os.mkdir(tmp_train_path)\n",
    "except:\n",
    "    pass\n",
    "def fit_transform(transformer, series, batch_id, output_dir, max_features=10000):\n",
    "    #tfidf = TfidfVectorizer(tokenizer=tokenizer, binary=True, stop_words='english', use_idf=True, max_features=max_features)\n",
    "    #series1 = df[col1]\n",
    "    #series2 = df[col2]\n",
    "    #series = pd.concat([series1, series2])\n",
    "    #start = time.time()\n",
    "    print('processing batch {}'.format(batch_id))\n",
    "    trnsfmd = transformer.fit_transform(series)\n",
    "    #end =  time.time()\n",
    "    # save transformed batch\n",
    "    out_file = ('%d' % batch_id)\n",
    "    out_path = output_dir+'/'+out_file \n",
    "    with open(out_path, 'wb') as handle:\n",
    "        pickle.dump(trnsfmd, handle)\n",
    "    ps.copy_file(dest_bucket=HASH_BUCKET, file='train/'+out_file, source=out_path)\n",
    "    #print('created TF-IDF vectors in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import partition_all\n",
    "from joblib import Parallel, delayed\n",
    "# empty HASH_BUCKET\n",
    "ps.remove_all_files(bucket=HASH_BUCKET, path='train/')\n",
    "series = pd.Series(pd.concat([X_train['question1'], X_train['question2']]),dtype=str)\n",
    "partitions = partition_all(10000, series.tolist())\n",
    "hashvect = HashingVectorizer(binary=True, stop_words='english')\n",
    "Parallel(n_jobs=8)(delayed(fit_transform)(hashvect, batch, i, tmp_train_path, 10000)\n",
    "         for i, batch in enumerate(partitions))\n",
    "#executor(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import vstack\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "data = []\n",
    "files = ps.get_all_filenames(bucket=HASH_BUCKET, path='train/')\n",
    "for file in files:\n",
    "    ps.get_file(bucket=HASH_BUCKET, filename='train/'+file, filepath=tmp_train_path+file)\n",
    "    with open(tmp_train_path+file, 'rb') as handle:\n",
    "        data.append(pickle.load(handle))\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "X_trfmd = tfidf.fit_transform(vstack(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trfmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension reduction using SVD\n",
    "start = time.time()\n",
    "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42)\n",
    "X_svd = svd.fit_transform(X_trfmd)\n",
    "end =  time.time()\n",
    "print('created SVD transform in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split back into two\n",
    "X1 = X_svd[:len(X_train), :]\n",
    "X2 = X_svd[len(X_train):, :]\n",
    "## find pair-wise cosine similarity\n",
    "#start = time.time()\n",
    "#X_sim = cosine_similarity(X1, X2)\n",
    "#end =  time.time()\n",
    "#print('computed cosine similarity in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svd_feature_length = X_sim.shape[1]\n",
    "#start = time.time()\n",
    "#temp_df = pd.DataFrame(X_sim)\n",
    "#x_df1 = pd.concat([x_df1,temp_df], axis=1)\n",
    "#end =  time.time()\n",
    "#print('rebuilt dataframe with new tf_svd feature columns in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_df1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_df1 = tfidf_svd_vectorize(x_df1, 'question1', 'question2', 10000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference in text size\n",
    "compute_size_diff = lambda row: abs(len(str(row['question1'])) - len(str(row['question2'])))\n",
    "X_train['size_diff'] = X_train.apply(compute_size_diff, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vector norm diff (distance)\n",
    "#compute_spacy_distance = lambda row: abs(row['question1'].vector_norm - row['question2'].vector_norm)\n",
    "#x_df1['spacy_distance'] = x_df1.apply(compute_spacy_distance, axis=1)\n",
    "#x_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return mean distance between tokens and document centroid\n",
    "def compute_mean_distance(doc):\n",
    "    mean_distance = 0.0\n",
    "    centroid = doc.vector\n",
    "    count = 0\n",
    "    for token in doc:\n",
    "        if not token.is_stop:\n",
    "            mean_distance += np.inner(token.vector,centroid)\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        count = 1\n",
    "    return mean_distance / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mean distance from centroid for question1\n",
    "#compute_q1_mean_dist = lambda row: compute_mean_distance(row['question1'])\n",
    "#x_df1['q1_mean_dist'] = x_df1.apply(compute_q1_mean_dist, axis=1)\n",
    "#x_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mean distance from centroid for question1\n",
    "#compute_q2_mean_dist = lambda row: compute_mean_distance(row['question2'])\n",
    "#x_df1['q2_mean_dist'] = x_df1.apply(compute_q2_mean_dist, axis=1)\n",
    "#x_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## difference in mean distance\n",
    "#compute_mean_dist_diff = lambda row: abs(row['q1_mean_dist'] - row['q2_mean_dist'])\n",
    "#x_df1['mean_dist_diff'] = x_df1.apply(compute_mean_dist_diff, axis=1)\n",
    "#x_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## centroid similarity\n",
    "#compute_centroid_similarity = lambda row: np.inner(row['question1'].vector, row['question2'].vector)\n",
    "#x_df1['centroid_similarity'] = x_df1.apply(compute_centroid_similarity, axis=1)\n",
    "#x_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio\n",
    "compute_ratio = lambda row: fuzz.ratio(str(row['question1']), str(row['question2']))\n",
    "X_train['ratio'] = X_train.apply(compute_ratio, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial ratio\n",
    "compute_partial_ratio = lambda row: fuzz.partial_ratio(str(row['question1']), str(row['question2']))\n",
    "X_train['partial_ratio'] = X_train.apply(compute_partial_ratio, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_sort_ratio\n",
    "compute_token_sort_ratio = lambda row: fuzz.token_sort_ratio(str(row['question1']), str(row['question2']))\n",
    "X_train['token_sort_ratio'] = X_train.apply(compute_token_sort_ratio, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_set_ratio\n",
    "compute_token_set_ratio = lambda row: fuzz.token_set_ratio(str(row['question1']), str(row['question2']))\n",
    "X_train['token_set_ratio'] = X_train.apply(compute_token_set_ratio, axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build complete feature dataframe\n",
    "X_train_temp = pd.concat([pd.DataFrame(X1, columns=['q1_'+str(i) for i in range(X1.shape[1])], index=X_train.index), \n",
    "                     pd.DataFrame(X2, columns=['q2_'+str(i) for i in range(X2.shape[1])], index=X_train.index)], axis=1)\n",
    "X_train_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train_temp, X_train], axis=1)\n",
    "del X_train_temp\n",
    "X_train = X_train.drop(columns=['qid1', 'qid2','question1','question2'])\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[X_train['is_duplicate'] == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load test_set\n",
    "#test_data: str = 'test.csv'\n",
    "#filestream: HTTPResponse = ps.get_file_stream(bucket=INPUT_BUCKET, filename=test_data)\n",
    "#dtypes: Dict[str, str] = {\n",
    "#    'id': 'int64',\n",
    "#    'question1': 'object',\n",
    "#    'question2': 'object'\n",
    "#}\n",
    "#test_df: pd.DataFrame = pd.read_csv(#urlpath=s3_in_url, \n",
    "#                                     #storage_options=s3_options,\n",
    "#                                     filestream,\n",
    "#                                     header=0, \n",
    "#                                     usecols=dtypes.keys(), \n",
    "#                                     names=dtypes.keys(),\n",
    "#                                     skipinitialspace=True,\n",
    "#                                     skip_blank_lines=True,\n",
    "#                                     encoding='utf-8')\n",
    "#test_df = test_df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ps.create_bucket(bucket=HASH_BUCKET)\n",
    "tmp_test_path = '/tmp/test'\n",
    "try:\n",
    "    os.mkdir(tmp_test_path)\n",
    "except:\n",
    "    pass\n",
    "def transform(transformer, series, batch_id, output_dir, max_features=10000):\n",
    "    #tfidf = TfidfVectorizer(tokenizer=tokenizer, binary=True, stop_words='english', use_idf=True, max_features=max_features)\n",
    "    #series1 = df[col1]\n",
    "    #series2 = df[col2]\n",
    "    #series = pd.concat([series1, series2])\n",
    "    #start = time.time()\n",
    "    print('processing batch {}'.format(batch_id))\n",
    "    X = transformer.transform(series)\n",
    "    #end =  time.time()\n",
    "    # save transformed batch\n",
    "    out_file = ('%d' % batch_id)\n",
    "    out_path = output_dir+'/'+out_file \n",
    "    with open(out_path, 'wb') as handle:\n",
    "        pickle.dump(X, handle)\n",
    "    ps.copy_file(dest_bucket=HASH_BUCKET, file='test/'+out_file, source=out_path)\n",
    "    #print('created TF-IDF vectors in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ps' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2a4ad28580c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# empty HASH_BUCKET\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_all_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHASH_BUCKET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpartitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartition_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m Parallel(n_jobs=8)(delayed(transform)(hashvect, batch, i, tmp_test_path, 10000)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ps' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# empty HASH_BUCKET\n",
    "ps.remove_all_files(bucket=HASH_BUCKET, path='test/')\n",
    "series = pd.Series(pd.concat([X_test['question1'], X_test['question2']]),dtype=str)\n",
    "partitions = partition_all(10000, series.tolist())\n",
    "Parallel(n_jobs=8)(delayed(transform)(hashvect, batch, i, tmp_test_path, 10000)\n",
    "         for i, batch in enumerate(partitions))\n",
    "#executor(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "files = ps.get_all_filenames(bucket=HASH_BUCKET, path='test/')\n",
    "for file in files:\n",
    "    ps.get_file(bucket=HASH_BUCKET, filename='test/'+file, filepath=tmp_test_path+file)\n",
    "    with open(tmp_test_path+file, 'rb') as handle:\n",
    "        data.append(pickle.load(handle))\n",
    "X_test_trfmd = tfidf.transform(vstack(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_trfmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension reduction using SVD\n",
    "start = time.time()\n",
    "X_test_svd = svd.transform(X_test_trfmd)\n",
    "end =  time.time()\n",
    "print('created SVD transform in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split back into two\n",
    "X1_test = X_test_svd[:len(X_test), :]\n",
    "X2_test = X_test_svd[len(X_test):, :]\n",
    "## find pair-wise cosine similarity\n",
    "#start = time.time()\n",
    "#X_sim = cosine_similarity(X1, X2)\n",
    "#end =  time.time()\n",
    "#print('computed cosine similarity in time {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference in text size\n",
    "X_test['size_diff'] = X_test.apply(compute_size_diff, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio\n",
    "X_test['ratio'] = X_test.apply(compute_ratio, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial ratio\n",
    "X_test['partial_ratio'] = X_test.apply(compute_partial_ratio, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_sort_ratio\n",
    "X_test['token_sort_ratio'] = X_test.apply(compute_token_sort_ratio, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_set_ratio\n",
    "X_test['token_set_ratio'] = X_test.apply(compute_token_set_ratio, axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build complete feature dataframe\n",
    "X_test_temp = pd.concat([pd.DataFrame(X1_test, columns=['q1_'+str(i) for i in range(X1_test.shape[1])], index=X_test.index), \n",
    "                    pd.DataFrame(X2_test, columns=['q2_'+str(i) for i in range(X2_test.shape[1])], index=X_test.index)], axis=1)\n",
    "X_test_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.concat([X_test_temp, X_test], axis=1)\n",
    "del X_test_temp\n",
    "X_test = X_test.drop(columns=['question1','question2'])\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "logr_model = LogisticRegression(solver='lbfgs', random_state=42)\n",
    "param_grid = {'C': np.logspace(-2, 7, 10)}\n",
    "logr_cv = GridSearchCV(logr_model, param_grid=param_grid, cv=5, njobs=-1)\n",
    "#y_train = X['is_duplicate']\n",
    "#X_train = X.drop(columns=['is_duplicate'])\n",
    "logr_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_model = LogisticRegression(solver='lbfgs', random_state=42, C=logr_cv.best_params_['C'], njobs=-1)\n",
    "logr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_pred = logr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "logr_acc_score = accuracy_score(y_test, logr_pred)\n",
    "logr_prec_score = precision_score(y_test, logr_pred)\n",
    "logr_rec_score = recall_score(y_test, logr_pred)\n",
    "print('Logistic Regression')\n",
    "print('accuracy score : {}'.format(logr_acc_score))\n",
    "print('precision score : {}'.format(logr_prec_score))\n",
    "print('recall score : {}'.format(logr_rec_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
